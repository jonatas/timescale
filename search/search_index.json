{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The TimescaleDB Ruby Gem","text":"<p>Welcome to the TimescaleDB gem! To experiment with the code, start installing the gem:</p>"},{"location":"#installing","title":"Installing","text":"<p>You can install the gem locally:</p> <pre><code>gem install timescaledb\n</code></pre> <p>Or require it directly in the Gemfile of your project:</p> <pre><code>gem \"timescaledb\"\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>The model can use the acts_as_hypertable macro. Check more on models documentation.</li> <li>The ActiveRecord migrations can use the create_table supporting the <code>hypertable</code> keyword. It's also enabling you to add retention and continuous aggregates policies</li> <li>A standalone <code>create_hypertable</code> macro is also allowed in the migrations.</li> <li>Testing also becomes easier as the schema dumper will automatically introduce the hypertables to all environments.</li> <li>It also contains a scenic extension to work with scenic views as it's a wide adoption in the community.</li> <li>The gem is also packed with a command line utility that makes it easier to navigate in your database with Pry and all your hypertables available in a Ruby style.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>The all_in_one example shows:</p> <ol> <li>Create a hypertable with compression settings</li> <li>Insert data</li> <li>Run some queries</li> <li>Check chunk size per model</li> <li>Compress a chunk</li> <li>Check chunk status</li> <li>Decompress a chunk</li> </ol> <p>The ranking example shows how to configure a Rails app and navigate all the features available.</p>"},{"location":"#toolkit-examples","title":"Toolkit  examples","text":"<p>There are also examples in the toolkit-demo folder that can help you to understand how to properly use the toolkit functions.</p> <ul> <li>ohlc is a funtion that groups data by Open, High, Low, Close and make histogram availables to group the data, very useful for financial analysis.</li> <li>While building the LTTB tutorial I created the lttb is a simple charting using the Largest Triangle Three Buckets and there. A zoomable version which allows to navigate in the data and zoom it keeping the same data resolution is also available.</li> <li>A small example showing how to process volatility is also good to get familiar with the pipeline functions. A benchmark implementing the same in Ruby is also available to check how it compares to the SQL implementation.</li> </ul>"},{"location":"#extra-resources","title":"Extra resources","text":"<p>If you need extra help, please join the fantastic timescale community or ask your question on StackOverflow using the <code>#timescaledb</code> tag.</p> <p>If you want to go deeper in the library, the videos links to all live-coding sessions showed how @jonatasdp built the gem.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Bug reports and pull requests are welcome on GitHub at https://github.com/timescale/timescaledb-ruby. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the code of conduct.</p>"},{"location":"#license","title":"License","text":"<p>The gem is available as open source under the MIT License.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>Everyone interacting in the Timescale project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the code of conduct.</p>"},{"location":"chat_gpt_tutorial/","title":"Introduction to AI Agents","text":"<p>Artificial Intelligence (AI) agents are programs capable of autonomous actions in an environment to meet specific goals. These agents can analyze their environment, make decisions, and execute actions independently. Many industries leverage AI to automate processes, improve customer interaction, provide personalized recommendations, and more.</p> <p>One common way of interacting with AI agents is through an Application Programming Interface (API). An API provides a set of rules and protocols for interacting with a software application. In the context of AI, APIs often allow developers to leverage complex machine learning models in their applications without needing to understand all the underlying details.</p> <p>Through APIs, we can instruct AI to play various roles within our business. For example, an AI agent could act as a customer service representative, answering common queries and providing information to customers 24/7. Alternatively, it could play a role in data analysis, interpreting raw data and providing valuable insights.</p>"},{"location":"chat_gpt_tutorial/#converting-human-language-to-sql-queries","title":"Converting Human Language to SQL Queries","text":"<p>A fascinating application of AI is the capability to understand and interpret human language, a field known as Natural Language Processing (NLP). Using AI, we can convert natural language into structured queries, such as SQL, on the fly. This means we can interact with our databases using everyday language rather than needing to write complex queries.</p> <p>For example, a user could ask an AI agent, \"How many users signed up in the last week?\" The AI agent can convert this question into an SQL query to retrieve the answer from the database. This capability can dramatically simplify interactions with databases and make data more accessible to non-technical users.</p> <p>In this example, we used an AI (GPT-4 by OpenAI) to interact with the user, interpret the user's instructions, convert these instructions into SQL queries, and then execute these queries on a TimescaleDB database. This is an example of how AI can serve as a 'middle-man', simplifying the interface between humans and databases.</p>"},{"location":"chat_gpt_tutorial/#using-open-ai-as-a-long-term-memory","title":"Using Open AI as a long term memory","text":"<p>The process of long term memory is nothing else than stacking more chat conversation to the API, similar to what the chat.openai.com uses to group by conversations by topic and building this space to save the context.</p> <p>Using the API it's more about persisting the actual messages and then sending it along with the actual prompt.</p>"},{"location":"chat_gpt_tutorial/#setting-up-initial-instructions-to-the-ai-agent","title":"Setting up initial instructions to the AI agent","text":"<p>Here is the initial instructions we'll always send to the API to guarantee it knows it has access to the database and it can execute queries or consult the catalog in case it does not know exactly the information that is necessary to achieve the result.</p> <pre><code>As an AI language model, you have access to a TimescaleDB database that stores conversation history in a table called \"conversations\". You can execute SQL queries to retrieve information from this table using markdown language. Use the common backticks with sql as the language and you'll have access to any information you need. Results of multiple queries will be answered in the same order.\n\nWhen I ask you a question, you should try to understand the context and, if necessary, use the backticks sql to execute the SQL query on the TimescaleDB database. Please provide the information I requested based on the query results. Always use one query per snippet.\n\nTo optimize resources, you can query previous messages on demand to remember any detail from the conversation that you need more context to have a better answer. When you have more to say, just continue. Everything is being written to the conversations hypertable. You can query any time you need to know more about an specific context.\n\nAlso, you can run queries in the database to answer questions using markdown backticks with the sql syntax. For example:\n\nIf I ask, \"How many conversations have I had today?\", you could respond with:\n\n```sql\nSELECT COUNT(*)\nFROM conversations\nWHERE topic = '#{topic}'\nAND DATE(ts) = CURRENT_DATE;\n```\n\nThe extra conversations columns are user_input and ai_response.\n\nYou can also query pg_catalog and learn about other database resources if you\nsee some request from another table or resource name.\n\nThe query results will be represented in JSON and limited to 1000 characters.\n\nThen, with your responses wrapping you can also add additional information complimenting the example. All results will be answered numbering the same sequence of queries found in the previous answer. Always choose to answer in markdown format and I'll always give the results in markdown format too.\n</code></pre>"},{"location":"chat_gpt_tutorial/#setup-the-environment","title":"Setup the environment","text":"<p>Our plan is use Ruby to interact with OpenAI's GPT-4 and TimescaleDB. The program will be emulating a chat interface that can execute SQL code from API responses and build SQL commands and interact with a Postgresql using natural language.</p> <p>Make sure you have a <code>.envrc</code> file and a tool like direnv.</p> <pre><code>export GPT4_KEY=&lt;put-your-api-key-here&gt;\nexport PG_URI=postgres://jonatasdp@localhost:5432/chatgpt-demo\n</code></pre> <p>If you don't have the <code>GPT4_KEY</code>, you can get it here.</p> <p>Make sure you adjust the <code>PG_URI</code> with your postgresql instance connection, and if you don't have a database instance locally, you can also setup one on the Timescale cloud very easily.</p>"},{"location":"chat_gpt_tutorial/#requirements","title":"Requirements","text":"<p>Ruby is the language that will run in the backend to connect to the OpenAI API and to the Postgresql instance. It will be a single file with everything inside.</p> <p>If you just want to run it, here is the final file.</p> <p>We use <code>bundler/inline</code> to manage gem dependencies. Here are the required gems:</p> <pre><code>gem 'timescaledb' # A wrapper to interact with TimescaleDB.\ngem 'rest-client' # Simple HTTP and REST client for Ruby.\ngem 'pry' # A runtime developer console to iterate and inspect the code.\ngem 'markdown' # Ruby Markdown parser.\ngem 'rouge' # A pure Ruby code highlighter.\ngem 'redcarpet' # A Ruby library for Markdown processing.\ngem 'tty-markdown' # A Markdown parser with syntax highlighting.\ngem 'tty-link' # To make URLs clickable in the terminal.\n</code></pre>"},{"location":"chat_gpt_tutorial/#main-code","title":"Main Code","text":"<p>First, require the necessary libraries:</p> <pre><code>require 'json'\nrequire 'time'\n</code></pre> <p>We'll be using API keys and URI's defined in environment variables:</p> <pre><code>API_KEY = ENV['GPT4_KEY']\nPG_URI = ENV['PG_URI'] || ARGV[ARGV.index(\"--pg-uri\")]\n</code></pre>"},{"location":"chat_gpt_tutorial/#persist-history-in-the-conversation-model","title":"Persist history in the conversation model","text":"<p>We then define a <code>Conversation</code> class that interacts with TimescaleDB:</p> <pre><code>class Conversation &lt; ActiveRecord::Base\n  self.primary_key = nil\n  acts_as_hypertable\n  ...\nend\n</code></pre>"},{"location":"chat_gpt_tutorial/#extract-sql-from-api-responses","title":"Extract SQL from API responses","text":"<p>When the API calls back suggesting some sql blocks, we're going to detect them by converting the markdown response into a html and then checking the html blocks.</p> <p>And an <code>SQLExtractor</code> class to extract SQL from markdown. This class will be stacking all markdown blocks to be executed later.</p> <pre><code>class SQLExtractor &lt; Redcarpet::Render::Base\n  attr_reader :sql\n  def block_code(code, language)\n    if language == 'sql'\n      @sql ||= []\n      @sql &lt;&lt; code\n      code\n    else\n      \"\"\n    end\n  end\nend\n</code></pre>"},{"location":"chat_gpt_tutorial/#call-gpt4-api-from-the-ruby-code","title":"Call GPT4 API from the Ruby code","text":"<p>The <code>call_gpt4_api</code> method is used to call the GPT-4 API with a prompt. At this moment we need to stack the following information:</p> <ol> <li>Role assignment: Initial instructions saying it's an AI agent - info that assigns the role of the API in this call.</li> <li>Context: Previous User Input + AI Responses in the same topic.</li> <li>User Input: what user is asking now.</li> </ol> <pre><code>def call_gpt4_api(prompt)\n  url = \"https://api.openai.com/v1/chat/completions\"\n  full_prompt = INSTRUCTIONS +\n      \"\\nHistory: #{Conversation.history}\" +\n      \"\\nInput: #{prompt}\"\n\n  body = { \"model\" =&gt; \"gpt-4\",\n      \"max_tokens\" =&gt; 1000,\n      \"temperature\" =&gt; 0,\n      \"messages\" =&gt; [{\"role\" =&gt; \"user\", \"content\" =&gt; full_prompt}],\n    }.to_json\n  headers = { \"Content-Type\" =&gt; \"application/json\", \"Authorization\" =&gt; \"Bearer #{API_KEY}\" }\n  response = RestClient.post(url, body, headers)\n  json_response = JSON.parse(response.body)\n  response = json_response[\"choices\"].first[\"message\"][\"content\"].strip\nrescue RestClient::BadRequest\n  \"Bad Request Error: #{$!.message}\"\nrescue\n  \"Error: #{$!.message}\"\nend\n</code></pre>"},{"location":"chat_gpt_tutorial/#execute-queries-in-the-database","title":"Execute queries in the database","text":"<p>As the API responses can bring queries and the SQLExtractor can detect them, now it's time to have a method to be executing the SQL queries and returning error messages in case it fails, so the API can recursively try to fix the issue.</p> <p>The following method try to execute the query and return high level error messages in case the execution fails:</p> <pre><code>def execute(query)\n  begin\n    ActiveRecord::Base.connection.execute(query)\n  rescue =&gt; e\n    \"Query Error: #{e.message}\"\n  end\nend\n</code></pre> <p>Truncating query results</p> <p>As the AI has access to the database, sometimes the query results are quite heavy and then it really slows down the upstreaming and processing of the context. So, for now, I'm truncating the results in 10k characters.</p> <p>It reduced a lot the timeouts and still quite good and working well.</p> <pre><code>json = execute(sql).to_json\nresults &lt;&lt; json\nif json.length &gt; 10000\n  json = json[0..10000]+\"... (truncated)\"\nend\n</code></pre>"},{"location":"chat_gpt_tutorial/#establishing-the-chat","title":"Establishing the chat","text":"<p>In the high level, we need to keep interacting with the user until they give up.</p> <p>In <code>chat_mode</code> method, we loop to continuously get user input and interact with GPT-4:</p> <pre><code>def chat_mode\n  info WELCOME_INFO\n  timeout = 300 # Set the timeout in seconds\n\n  loop do\n    print \"\\n#{topic}: \"\n    input = if IO.select([STDIN], [], [], timeout)\n              STDIN.gets.chomp\n            else\n              puts \"Timeout reached, exiting chat.\"\n              break\n            end\n\n    case input.downcase\n    when 'quit'\n      puts \"Exiting chat.\"\n      break\n    when 'debug'\n      require \"pry\";binding.pry\n    else\n      with_no_logs do\n        chat(input)\n      end\n    end\n  end\nend\n</code></pre> <p>Colored markdown</p> <p>To parse Markdown and have a colored markdown in the command line, use the magical tty-markdown library:</p> <pre><code>def info(content)\n  puts TTY::Markdown.parse(content)\nend\n</code></pre> <p>In <code>chat</code> method, we get the response from GPT-4, create a conversation record and then execute SQL queries from the markdown:</p> <pre><code>def chat(prompt)\n  response = call_gpt4_api(prompt)\n  with_no_logs do\n    Conversation.create(topic: topic,\n                        user_input: prompt,\n                        ai_response: response,\n                        ts: Time.now)\n  end\n\n  info(\"**AI:** #{response}\")\n\n  queries = sql_from_markdown(response)\n\n  if queries&amp;.any?\n    output = run_queries(queries)\n\n    info(output)\n    chat(output)\n  end\nend\n</code></pre> <p>To run the queries, you can also eval some context like the <code>topic</code>.</p> <pre><code>def run_queries(queries)\n  queries.each_with_index.map do |query,i|\n    sql = query.gsub(/#\\{(.*)\\}/){eval($1)}\n\n    json = execute(sql).to_json\n    json = json[0..10000]+\"... (truncated)\" if json.length &gt; 10000\n    &lt;&lt;~MARKDOWN\n      Result from query #{i+1}:\n      #{json}\n    MARKDOWN\n  end.join(\"\\n\")\nend\n</code></pre> <p>eval is evil \ud83d\ude08</p> <p>Note that we're using: <pre><code>eval($1)\n</code></pre> It will capture the result of the query param that is wrapped with <code>#{}</code> and execute it in the context. It's useful to guarantee we're filtereing by the topic and keep it simple for the example. If you want to build a production code make sure you use a more robust and safe approach for it.</p> <p>We'll also define the <code>topic</code> which will be used as a command line argument or just assigning the USER name as the default topic to group conversations.</p> <pre><code>def topic\n  ARGV[1] || ENV['USER']\nend\n</code></pre> <p>The instructions described before are also avaialable in the same [example folder][example] and was used as a raw text. You can also override and test with different subjects and evolve from some in progress research as well.</p> <pre><code>def instructions\n  if File.exists?(ARGV.last)\n    ARGV.last\n  else\n    'instructions.md'\n  end\nend\n</code></pre> <p>As a final step, setup the conversations table and enable the extension if necessary.</p> <pre><code>def setup_schema\n  ActiveRecord::Base.logger = Logger.new(STDOUT)\n  ActiveRecord::Base.establish_connection(PG_URI)\n\n  # Create the events table if it doesn't exist\n  unless Conversation.table_exists?\n    ActiveRecord::Base.connection.instance_exec do\n      execute \"CREATE EXTENSION IF NOT EXISTS timescaledb\"\n      create_table :conversations, id: false, hypertable: {time_column: :ts} do |t|\n        t.timestamptz :ts, default: \"now()\", null: false\n        t.string :topic, null: false\n        t.text :user_input, null: false\n        t.text :ai_response, null: false\n      end\n    end\n  end\nend\n</code></pre> <p>The main method is where we establish the connection and start the chat mode and we can call it right after the definition in the body of the ruby file.</p> <pre><code>def main\n  setup_schema\n  chat_mode\nend\n\nmain\n</code></pre>"},{"location":"chat_gpt_tutorial/#extra-capabilities","title":"Extra capabilities","text":"<p>I initially build it for queries but trying to request several database tasks,  I was also able to enable compression, create continuous aggregates, add retention policies and also refresh continuous aggregates policies.</p>"},{"location":"chat_gpt_tutorial/#enable-compression-for-a-hypertable","title":"Enable compression for a hypertable","text":"<p>I started trying this out. Asking it to enable compression, and here is the interaction output.</p> <p>The following example show an interaction which will execute the sql query:</p> <pre><code>User: enable compression for the conversations hypertable\nAI: To enable compression for the \u201cconversations\u201d hypertable in TimescaleDB,\nyou would need to execute an ALTER TABLE command with the SET function to\nenable compression. However, as an AI language model, I don\u2019t have the ability\nto execute SQL commands directly on a database.\n\nHere\u2019s an example of how you could do it:\n\n    ALTER TABLE conversations SET (\n      timescaledb.compress,\n      timescaledb.compress_segmentby = 'user_input'\n    );\n\n\nThis command will enable compression on the \u201cconversations\u201d hypertable and set the \u2018user_input\u2019 column as the segment by option. The \u2018compress_segmentby\u2019 option determines how the data will be ordered during compression, which can improve query performance.\n\nPlease note that you should replace \u2018user_input\u2019 with the appropriate column name based on your specific use case. Also, remember to execute this command as a user with the necessary\nprivileges.\n</code></pre> <p>The example from the output will be executed by the command line in the database that you're connected. And the API response also follow with more instructions and recursively interact in case multiple back and forth are necessary.</p>"},{"location":"chat_gpt_tutorial/#create-continuous-aggregates","title":"Create continuous aggregates","text":"<p>Now, let's dive into continuous aggregates feature and ask to create the first materialized view for it.</p> <pre><code>User: create a continuous aggregate materialized view summarizing number of\nwords from user_input and ai_response grouped by 1 hour.\nAI: Sure, to create a continuous aggregate materialized view that summarizes\nthe number of words from user_input and ai_response grouped by 1 hour,\nyou can use the following SQL command:\n\n    CREATE MATERIALIZED VIEW conversation_summary\n    WITH (timescaledb.continuous) AS\n    SELECT time_bucket(INTERVAL '1 hour', ts) AS one_hour,\n        COUNT(*) AS total_conversations,\n        SUM(LENGTH(user_input) - LENGTH(REPLACE(user_input, ' ', '')) + 1) AS user_input_words,\n        SUM(LENGTH(ai_response) - LENGTH(REPLACE(ai_response, ' ', '')) + 1) AS ai_response_words\n    FROM conversations\n    GROUP BY one_hour;\n\nThis command creates a materialized view named conversation_summary that groups the conversations by each hour. It also calculates the number of words in user_input and ai_response by counting the number of spaces and adding 1 (assuming words are separated by a single space).\n\nPlease note that this command should be executed by someone with the necessary permissions in the database. Also, the actual creation of the view might take some time depending on the size of the conversations table.\n</code></pre> <p>You can also try to setup retention policies  or create new hypertables if you want. It seems very flexible and safe to use in a sandbox to explore data.</p>"},{"location":"chat_gpt_tutorial/#complex-query-building","title":"Complex query building","text":"<p>It's also very good on building more complex queries, especially after adding more examples about it.</p>"},{"location":"chat_gpt_tutorial/#knowing-issues","title":"Knowing issues","text":"<p>While you can easily get some snippets, from time to time, if you change the subject, things will get complicated and it will commit several errors in a row.</p> <p>For example, I was talking about the conversations table for quite a while, and suddenly I asked it to create a continuous aggregates view. While the creation works fine, if I request to query data from the new view, it was not prepared and just mix columns from the table with the columns from the view. I tried several examples and it was not able to get it properly. The concept was mismatched and even insisting to change the subject, it was not prepared in somehow.</p>"},{"location":"chat_gpt_tutorial/#try-it-yourself","title":"Try it yourself","text":"<p>If you want to try it, this example is available on examples/chatgpt/openai-cli.rb and you can follow the instructions in the folder how to use it.</p>"},{"location":"command_line/","title":"Command line application","text":"<p>When you install the gem locally, a new command line application named <code>tsdb</code> will be available on your command line.</p>"},{"location":"command_line/#the-tsdb-cli","title":"The <code>tsdb</code> CLI","text":"<p>It accepts a Postgresql URI and some extra flags that can help you to get more info from your TimescaleDB server:</p> <pre><code>tsdb &lt;uri&gt; --stats\n</code></pre> <p>Where the <code>&lt;uri&gt;</code> is replaced with params from your connection like:</p> <pre><code>tsdb postgres://&lt;user&gt;@localhost:5432/&lt;dbname&gt; --stats\n</code></pre> <p>Or merely check the stats:</p> <pre><code>tsdb \"postgres://&lt;user&gt;@localhost:5432/timescaledb_test\" --stats\n</code></pre> <p>Here is a sample output from a database example with almost no data:</p> <pre><code>{:hypertables=&gt;\n  {:count=&gt;3,\n   :uncompressed=&gt;2,\n   :chunks=&gt;{:total=&gt;1, :compressed=&gt;0, :uncompressed=&gt;1},\n   :size=&gt;{:befoe_compressing=&gt;\"80 KB\", :after_compressing=&gt;\"0 Bytes\"}},\n :continuous_aggregates=&gt;{:count=&gt;1},\n :jobs_stats=&gt;[{:success=&gt;nil, :runs=&gt;nil, :failures=&gt;nil}]}\n</code></pre> <p>To start a interactive ruby/pry console use <code>--console</code>: The console will dynamically create models for all hypertables that it finds in the database.</p> <p>Let's consider the caggs.sql as the example of a database.</p> <pre><code>psql postgres://&lt;user&gt;@localhost:5432/playground -f caggs.sql\n</code></pre> <p>Then use <code>tsdb</code> in the command line with the same URI and <code>--stats</code>:</p> <pre><code>tsdb postgres://&lt;user&gt;@localhost:5432/playground --stats\n{:hypertables=&gt;\n  {:count=&gt;1,\n   :uncompressed=&gt;1,\n   :approximate_row_count=&gt;{\"ticks\"=&gt;352},\n   :chunks=&gt;{:total=&gt;1, :compressed=&gt;0, :uncompressed=&gt;1},\n   :size=&gt;{:uncompressed=&gt;\"88 KB\", :compressed=&gt;\"0 Bytes\"}},\n :continuous_aggregates=&gt;{:total=&gt;1},\n :jobs_stats=&gt;[{:success=&gt;nil, :runs=&gt;nil, :failures=&gt;nil}]}\n</code></pre> <p>To have some interactive playground with the actual database using ruby, just try the same command before changing from <code>--stats</code> to <code>--console</code>:</p>"},{"location":"command_line/#tsdb-console","title":"tsdb --console","text":"<p>We are using the same database from the previous example for this context which contains a hypertable named <code>ticks</code> and a view called <code>ohlc_1m</code>.</p> <pre><code>tsdb postgres://&lt;user&gt;@localhost:5432/playground --console\npry(Timescale)&gt;\n</code></pre> <p>The <code>tsdb</code> CLI will automatically create ActiveRecord models for hypertables and the continuous aggregates views.</p> <pre><code>Tick\n=&gt; Timescaledb::Tick(time: datetime, symbol: string, price: decimal, volume: integer)\n</code></pre> <p>Note that it's only created for this session and will never cache in the library or any other place.</p> <p>In this case, the <code>Tick</code> model comes from the <code>ticks</code> hypertable found in the database. It contains several methods inherited from the <code>acts_as_hypertable</code> macro.</p> <p>Let's start with the <code>.hypertable</code> method.</p> <pre><code>Tick.hypertable\n=&gt; #&lt;Timescaledb::Hypertable:0x00007fe99c258900\n hypertable_schema: \"public\",\n hypertable_name: \"ticks\",\n owner: \"jonatasdp\",\n num_dimensions: 1,\n num_chunks: 1,\n compression_enabled: false,\n tablespaces: nil&gt;\n</code></pre> <p>The core of the hypertables is the fragmentation of the data into chunks, the child tables that distribute the data. You can check all chunks directly from the hypertable relation.</p> <pre><code>Tick.hypertable.chunks\nunknown OID 2206: failed to recognize type of 'primary_dimension_type'. It will cast as a String.\n=&gt; [#&lt;Timescaledb::Chunk:0x00007fe99c31b068\n  hypertable_schema: \"public\",\n  hypertable_name: \"ticks\",\n  chunk_schema: \"_timescaledb_internal\",\n  chunk_name: \"_hyper_33_17_chunk\",\n  primary_dimension: \"time\",\n  primary_dimension_type: \"timestamp without time zone\",\n  range_start: 1999-12-30 00:00:00 +0000,\n  range_end: 2000-01-06 00:00:00 +0000,\n  range_start_integer: nil,\n  range_end_integer: nil,\n  is_compressed: false,\n  chunk_tablespace: nil&gt;]\n</code></pre> <p>Chunks are created by partitioning the hypertable data into one (or potentially multiple) dimensions. All hypertables are partitions by the values belonging to a time column, which may be in timestamp, date, or various integer forms. If the time partitioning interval is one day, for example, then rows with timestamps that belong to the same day are co-located within the same chunk, while rows belonging to different days belong to different chunks. Learn more here.</p> <p>Another core concept of TimescaleDB is compression. With data partitioned, it becomes very convenient to compress and decompress chunks independently.</p> <pre><code>Tick.hypertable.chunks.first.compress!\nActiveRecord::StatementInvalid: PG::FeatureNotSupported: ERROR:  compression not enabled on \"ticks\"\nDETAIL:  It is not possible to compress chunks on a hypertable that does not have compression enabled.\nHINT:  Enable compression using ALTER TABLE with the timescaledb.compress option.\n</code></pre> <p>As compression is not enabled, let's do it by executing plain SQL directly from the actual context. To borrow a connection, let's use the Tick object.</p> <pre><code>Tick.connection.execute(\"ALTER TABLE ticks SET (timescaledb.compress)\") # =&gt; PG_OK\n</code></pre> <p>And now, it's possible to compress and decompress:</p> <p><pre><code>Tick.hypertable.chunks.first.compress!\nTick.hypertable.chunks.first.decompress!\n</code></pre> Learn more about TimescaleDB compression here.</p> <p>The <code>ohlc_1m</code> view is also available as an ActiveRecord:</p> <pre><code>Ohlc1m\n=&gt; Timescaledb::Ohlc1m(bucket: datetime, symbol: string, open: decimal, high: decimal, low: decimal, close: decimal, volume: integer)\n</code></pre> <p>And you can run any query as you do with regular active record queries.</p> <pre><code>Ohlc1m.order(bucket: :desc).last\n=&gt; #&lt;Timescaledb::Ohlc1m:0x00007fe99c2c38e0\n bucket: 2000-01-01 00:00:00 UTC,\n symbol: \"SYMBOL\",\n open: 0.13e2,\n high: 0.3e2,\n low: 0.1e1,\n close: 0.1e2,\n volume: 27600&gt;\n</code></pre>"},{"location":"migrations/","title":"ActiveRecord migrations helpers for Timescale","text":"<p>Create table is now with the <code>hypertable</code> keyword allowing to pass a few options to the function call while also using the <code>create_table</code> method:</p>"},{"location":"migrations/#create_table-with-the-hypertable-option","title":"create_table with the <code>:hypertable</code> option","text":"<pre><code>hypertable_options = {\n  time_column: 'created_at',\n  chunk_time_interval: '1 min',\n  compress_segmentby: 'identifier',\n  compress_after: '7 days'\n}\n\ncreate_table(:events, id: false, hypertable: hypertable_options) do |t|\n  t.datetime :created_at, null: false\n  t.string :identifier, null: false\n  t.jsonb :payload\nend\n</code></pre>"},{"location":"migrations/#the-create_continuous_aggregate-helper","title":"The <code>create_continuous_aggregate</code> helper","text":"<p>This goes in the model file.  This example shows a ticks table grouping ticks as OHLCV histograms for every minute.</p> <p>First make sure you have the model with the <code>acts_as_hypertable</code> method to be able to extract the query from it.</p> <pre><code>class Tick &lt; ActiveRecord::Base\n  self.table_name = 'ticks'\n  acts_as_hypertable\nend\n</code></pre> <p>Then, inside your migration:</p> <pre><code>hypertable_options = {\n  time_column: 'created_at',\n  chunk_time_interval: '1 min',\n  compress_segmentby: 'symbol',\n  compress_orderby: 'created_at',\n  compress_after: '7 days'\n}\ncreate_table :ticks, hypertable: hypertable_options, id: false do |t|\n  t.string :symbol\n  t.decimal :price\n  t.integer :volume\n  t.timestamps\nend\n\nquery = Tick.select(&lt;&lt;~QUERY)\n  time_bucket('1m', created_at) as time,\n  symbol,\n  FIRST(price, created_at) as open,\n  MAX(price) as high,\n  MIN(price) as low,\n  LAST(price, created_at) as close,\n  SUM(volume) as volume\").group(\"1,2\")\nQUERY\n\noptions = {\n  with_data: false,\n  refresh_policies: {\n    start_offset: \"INTERVAL '1 month'\",\n    end_offset: \"INTERVAL '1 minute'\",\n    schedule_interval: \"INTERVAL '1 minute'\"\n  }\n}\n\ncreate_continuous_aggregate('ohlc_1m', query, **options)\n</code></pre> <p>If you need more details, please check this blog post.</p> <p>If you're interested in candlesticks and need to get the OHLC values, take a look at the toolkit ohlc function that do the same but through a function that can be reusing candlesticks from smaller timeframes.</p> <p>Disable ddl transactions in your migration to start with data</p> <p>If you want to start <code>with_data: true</code>, remember that you'll need to <code>disable_ddl_transaction!</code> in your migration file.</p> <pre><code>class CreateCaggsWithData &lt; ActiveRecord::Migration[7.0]\n  disable_ddl_transaction!\n\n  def change\n    create_continuous_aggregate('ohlc_1m', query, with_data: true)\n    # ...\n  end\nend\n</code></pre>"},{"location":"migrations/#create-a-continuous-aggregate-using-the-macro","title":"Create a continuous aggregate using the macro","text":"<p>To setup complex hierarchies of continuous aggregates, you can use the <code>continuous_aggregates</code> macro.</p> <p>This setup allows for creating multiple continuous aggregates with customizable refresh policies, making it ideal for complex aggregation and retention policies. </p> <pre><code>class Download &lt; ActiveRecord::Base\n  extend Timescaledb::ActsAsHypertable\n  include Timescaledb::ContinuousAggregatesHelper\n\n  acts_as_hypertable time_column: 'ts'\n\n  scope :total_downloads, -&gt; { select(\"count(*) as total\") }\n  scope :downloads_by_gem, -&gt; { select(\"gem_name, count(*) as total\").group(:gem_name) }\n  scope :downloads_by_version, -&gt; { select(\"gem_name, gem_version, count(*) as total\").group(:gem_name, :gem_version) }\n\n  continuous_aggregates(\n    timeframes: [:minute, :hour, :day, :month],\n    scopes: [:total_downloads, :downloads_by_gem, :downloads_by_version],\n    refresh_policy: {\n      minute: { start_offset: \"10 minutes\", end_offset: \"1 minute\", schedule_interval: \"1 minute\" },\n      hour:   { start_offset: \"4 hour\",     end_offset: \"1 hour\",   schedule_interval: \"1 hour\" },\n      day:    { start_offset: \"3 day\",      end_offset: \"1 day\",    schedule_interval: \"1 day\" },\n      month:  { start_offset: \"3 month\",    end_offset: \"1 day\",  schedule_interval: \"1 day\" }\n  })\nend\n</code></pre> <p>Then edit the migration file to add the continuous aggregates:</p> <pre><code>class CreateCaggs &lt; ActiveRecord::Migration[7.0]\n  def up\n    Download.create_continuous_aggregates\n  end\n\n  def down\n    Download.drop_continuous_aggregates\n  end\nend\n</code></pre> <p>Here is the output of the migration:</p> <pre><code>CREATE MATERIALIZED VIEW IF NOT EXISTS total_downloads_per_minute\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 minute', ts) as ts, count(*) as total FROM \"downloads\" GROUP BY 1\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('total_downloads_per_minute',\n  start_offset =&gt; INTERVAL '10 minutes',\n  end_offset =&gt;  INTERVAL '1 minute',\n  schedule_interval =&gt; INTERVAL '1 minute');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS total_downloads_per_hour\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 hour', ts) as ts, sum(total) as total FROM \"total_downloads_per_minute\" GROUP BY 1\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('total_downloads_per_hour',\n  start_offset =&gt; INTERVAL '4 hour',\n  end_offset =&gt;  INTERVAL '1 hour',\n  schedule_interval =&gt; INTERVAL '1 hour');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS total_downloads_per_day\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 day', ts) as ts, sum(total) as total FROM \"total_downloads_per_hour\" GROUP BY 1\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('total_downloads_per_day',\n  start_offset =&gt; INTERVAL '3 day',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS total_downloads_per_month\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 month', ts) as ts, sum(total) as total FROM \"total_downloads_per_day\" GROUP BY 1\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('total_downloads_per_month',\n  start_offset =&gt; INTERVAL '3 month',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_gem_per_minute\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 minute', ts) as ts, gem_name, count(*) as total FROM \"downloads\" GROUP BY 1, \"downloads\".\"gem_name\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_gem_per_minute',\n  start_offset =&gt; INTERVAL '10 minutes',\n  end_offset =&gt;  INTERVAL '1 minute',\n  schedule_interval =&gt; INTERVAL '1 minute');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_gem_per_hour\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 hour', ts) as ts, gem_name, sum(total) as total FROM \"downloads_by_gem_per_minute\" GROUP BY 1, \"downloads_by_gem_per_minute\".\"gem_name\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_gem_per_hour',\n  start_offset =&gt; INTERVAL '4 hour',\n  end_offset =&gt;  INTERVAL '1 hour',\n  schedule_interval =&gt; INTERVAL '1 hour');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_gem_per_day\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 day', ts) as ts, gem_name, sum(total) as total FROM \"downloads_by_gem_per_hour\" GROUP BY 1, \"downloads_by_gem_per_hour\".\"gem_name\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_gem_per_day',\n  start_offset =&gt; INTERVAL '3 day',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_gem_per_month\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 month', ts) as ts, gem_name, sum(total) as total FROM \"downloads_by_gem_per_day\" GROUP BY 1, \"downloads_by_gem_per_day\".\"gem_name\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_gem_per_month',\n  start_offset =&gt; INTERVAL '3 month',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_version_per_minute\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 minute', ts) as ts, gem_name, gem_version, count(*) as total FROM \"downloads\" GROUP BY 1, \"downloads\".\"gem_name\", \"downloads\".\"gem_version\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_version_per_minute',\n  start_offset =&gt; INTERVAL '10 minutes',\n  end_offset =&gt;  INTERVAL '1 minute',\n  schedule_interval =&gt; INTERVAL '1 minute');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_version_per_hour\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 hour', ts) as ts, gem_name, gem_version, sum(total) as total FROM \"downloads_by_version_per_minute\" GROUP BY 1, \"downloads_by_version_per_minute\".\"gem_name\", \"downloads_by_version_per_minute\".\"gem_version\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_version_per_hour',\n  start_offset =&gt; INTERVAL '4 hour',\n  end_offset =&gt;  INTERVAL '1 hour',\n  schedule_interval =&gt; INTERVAL '1 hour');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_version_per_day\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 day', ts) as ts, gem_name, gem_version, sum(total) as total FROM \"downloads_by_version_per_hour\" GROUP BY 1, \"downloads_by_version_per_hour\".\"gem_name\", \"downloads_by_version_per_hour\".\"gem_version\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_version_per_day',\n  start_offset =&gt; INTERVAL '3 day',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS downloads_by_version_per_month\nWITH (timescaledb.continuous) AS\nSELECT time_bucket('1 month', ts) as ts, gem_name, gem_version, sum(total) as total FROM \"downloads_by_version_per_day\" GROUP BY 1, \"downloads_by_version_per_day\".\"gem_name\", \"downloads_by_version_per_day\".\"gem_version\"\nWITH NO DATA;\n\nSELECT add_continuous_aggregate_policy('downloads_by_version_per_month',\n  start_offset =&gt; INTERVAL '3 month',\n  end_offset =&gt;  INTERVAL '1 day',\n  schedule_interval =&gt; INTERVAL '1 day');\n</code></pre> <p>When <code>drop_continuous_aggregates</code> is called, it considers the reverse order of creation.</p> <pre><code>DROP MATERIALIZED VIEW IF EXISTS total_downloads_per_month CASCADE\nDROP MATERIALIZED VIEW IF EXISTS total_downloads_per_day CASCADE\nDROP MATERIALIZED VIEW IF EXISTS total_downloads_per_hour CASCADE\nDROP MATERIALIZED VIEW IF EXISTS total_downloads_per_minute CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_gem_per_month CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_gem_per_day CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_gem_per_hour CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_gem_per_minute CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_version_per_month CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_version_per_day CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_version_per_hour CASCADE\nDROP MATERIALIZED VIEW IF EXISTS downloads_by_version_per_minute CASCADE\n</code></pre> <p>The convention of naming the scopes is important as they mix with the name of the continuous aggregate.</p>"},{"location":"models/","title":"Models","text":"<p>The ActiveRecord is the default ORM in the Ruby community. We have introduced a macro that helps you to inject the behavior as other libraries do in the Rails ecosystem.</p> <p>You need to extend the Timescaledb::ActsAsHypertable module in your model. Ideally, you should include the Timescaledb::ContinuousAggregates module and also separate the definition of the continuous aggregate from the model.</p> <pre><code>class Hypertable &lt; ActiveRecord::Base\n  extend Timescaledb::ActsAsHypertable\n  include Timescaledb::ContinuousAggregates\n\n  def abstract_class\n    true\n  end\nend\n</code></pre>"},{"location":"models/#the-acts_as_hypertable-macro","title":"The <code>acts_as_hypertable</code> macro","text":"<p>You can declare a Rails model as a Hypertable by invoking the <code>acts_as_hypertable</code> macro. This macro extends your existing model with timescaledb-related functionality. Here's the macro using the default options, you can pass the following options:</p> <ul> <li><code>time_column</code>: The name of the column that will be used as the time column.</li> <li><code>chunk_time_interval</code>: The interval at which chunks will be created.</li> </ul> <pre><code>class Event &lt; Hypertable\n  acts_as_hypertable time_column: :created_at, chunk_time_interval: '1 day'\nend\n</code></pre>"},{"location":"models/#the-continuous_aggregates-macro","title":"The <code>continuous_aggregates</code> macro","text":"<p>You can declare continuous aggregates for a Rails model by invoking the <code>continuous_aggregates</code> macro. This macro extends your existing model with TimescaleDB-related functionality for efficient data aggregation and querying.</p> <pre><code># Example from RubyGems server\nclass Download &lt; ActiveRecord::Base\n  extend Timescaledb::ActsAsHypertable\n  include Timescaledb::ContinuousAggregatesHelper\n\n  acts_as_hypertable time_column: 'ts'\n\n  scope :total_downloads, -&gt; { select(\"count(*) as total\") }\n  scope :downloads_by_gem, -&gt; { select(\"gem_name, count(*) as total\").group(:gem_name) }\n  scope :downloads_by_version, -&gt; { select(\"gem_name, gem_version, count(*) as total\").group(:gem_name, :gem_version) }\n\n  continuous_aggregates(\n    timeframes: [:minute, :hour, :day, :month],\n    scopes: [:total_downloads, :downloads_by_gem, :downloads_by_version],\n    refresh_policy: {\n      minute: { start_offset: \"10 minutes\", end_offset: \"1 minute\", schedule_interval: \"1 minute\" },\n      hour:   { start_offset: \"4 hour\",     end_offset: \"1 hour\",   schedule_interval: \"1 hour\" },\n      day:    { start_offset: \"3 day\",      end_offset: \"1 day\",    schedule_interval: \"1 day\" },\n      month:  { start_offset: \"3 month\",    end_offset: \"1 day\",  schedule_interval: \"1 day\" }\n  })\nend\n</code></pre>"},{"location":"models/#the-create_continuous_aggregates-method-and-drop_continuous_aggregates-methods-for-migrations","title":"The <code>create_continuous_aggregates</code> method and <code>drop_continuous_aggregates</code> methods for migrations","text":"<p>The macro will create a continuous aggregate for each timeframe and scope you specify. After defining the continuous aggregate, you can use the <code>create_continuous_aggregate</code> method to create the continuous aggregate in the database.</p> <pre><code>class SetupMyAmazingCaggsMigration &lt; ActiveRecord::Migration[7.0]\n  def up\n    Download.create_continuous_aggregates\n  end\n\n  def down\n    Download.drop_continuous_aggregates\n  end\nend\n</code></pre> <p>It will automatically rollup all materialized views for all timeframes and scopes.</p>"},{"location":"models/#how-rollup-works","title":"How rollup works","text":"<p>The most important part of using multiple timeframes and scopes is to understand how the rollup works.</p> <p>The rollup is a process that will create a new row for each timeframe and scope.</p> <p>For example, if you have a scope called <code>total_downloads</code> and a timeframe of <code>day</code>, the rollup will rewrite the query to group by the day.</p> <pre><code># Original query\nSELECT count(*) FROM downloads;\n\n# Rolled up query\nSELECT time_bucket('1 day', created_at) AS day, count(*) FROM downloads GROUP BY day;\n</code></pre> <p>The rollup method will help to rollup such queries in a more efficient way.</p> <pre><code>Download.total_downloads.map(&amp;:attributes) #  =&gt; [{\"total\"=&gt;6175}\n# SELECT count(*) as total FROM \"downloads\"\n</code></pre> <p>Rollup to 1 minute:</p> <pre><code>Download.total_downloads.rollup(\"'1 min'\").map(&amp;:attributes)\n# SELECT time_bucket('1 min', ts) as ts, count(*) as total FROM \"downloads\" GROUP BY 1\n=&gt; [{\"ts\"=&gt;2024-04-26 00:10:00 UTC, \"total\"=&gt;110},\n {\"ts\"=&gt;2024-04-26 00:11:00 UTC, \"total\"=&gt;1322},\n {\"ts\"=&gt;2024-04-26 00:12:00 UTC, \"total\"=&gt;1461},\n {\"ts\"=&gt;2024-04-26 00:13:00 UTC, \"total\"=&gt;1150},\n {\"ts\"=&gt;2024-04-26 00:14:00 UTC, \"total\"=&gt;1127},\n {\"ts\"=&gt;2024-04-26 00:15:00 UTC, \"total\"=&gt;1005}]\n</code></pre>"},{"location":"models/#aggregates-classes","title":"Aggregates classes","text":"<p>The <code>continuous_aggregates</code> macro will also create a class for each aggregate.</p> <pre><code>Download::TotalDownloadsPerMinute.all.map(&amp;:attributes)\n# SELECT \"total_downloads_per_minute\".* FROM \"total_downloads_per_minute\"\n=&gt; [{\"ts\"=&gt;2024-04-26 00:10:00 UTC, \"total\"=&gt;110},\n {\"ts\"=&gt;2024-04-26 00:11:00 UTC, \"total\"=&gt;1322},\n {\"ts\"=&gt;2024-04-26 00:12:00 UTC, \"total\"=&gt;1461},\n {\"ts\"=&gt;2024-04-26 00:13:00 UTC, \"total\"=&gt;1150},\n {\"ts\"=&gt;2024-04-26 00:14:00 UTC, \"total\"=&gt;1127},\n {\"ts\"=&gt;2024-04-26 00:15:00 UTC, \"total\"=&gt;1005}]\n</code></pre> <p>The class also can rollup to other timeframes:</p> <pre><code>Download::TotalDownloadsPerMinute.select(\"sum(total) as total\").rollup(\"'2 min'\").map(&amp;:attributes)\n# SELECT time_bucket('2 min', ts) as ts, sum(total) as total FROM \"total_downloads_per_minute\" GROUP BY 1\n=&gt; [{\"ts\"=&gt;2024-04-26 00:12:00 UTC, \"total\"=&gt;2611}, {\"ts\"=&gt;2024-04-26 00:14:00 UTC, \"total\"=&gt;2132}, {\"ts\"=&gt;2024-04-26 00:10:00 UTC, \"total\"=&gt;1432}]\n</code></pre> <p>You can also get the base query where continuous aggregate is created from:</p> <pre><code>Download::TotalDownloadsPerMinute.base_query.to_sql\n=&gt; \"SELECT time_bucket('1 minute', ts) as ts, count(*) as total FROM \\\"downloads\\\" GROUP BY 1\"\n</code></pre> <p>In case of hierarchy of continuous aggregates, you can get the parent query:</p> <pre><code>Download::TotalDownloadsPerMonth.parent_query.to_sql\n=&gt; \"SELECT time_bucket('1 month', ts) as ts, sum(total) as total FROM \\\"total_downloads_per_day\\\" GROUP BY 1\"\n</code></pre> <p>The config is the same as the one you pass to the <code>continuous_aggregates</code> macro. But it will be nested with the scope name.</p> <pre><code>Download::DownloadsByGemPerMonth.config\n=&gt; {:scope_name=&gt;:downloads_by_gem,\n :select=&gt;\"gem_name, count(*) as total\",\n :group_by=&gt;[:gem_name],\n :refresh_policy=&gt;\n  {:minute=&gt;{:start_offset=&gt;\"10 minutes\", :end_offset=&gt;\"1 minute\", :schedule_interval=&gt;\"1 minute\"},\n   :hour=&gt;{:start_offset=&gt;\"4 hour\", :end_offset=&gt;\"1 hour\", :schedule_interval=&gt;\"1 hour\"},\n   :day=&gt;{:start_offset=&gt;\"3 day\", :end_offset=&gt;\"1 day\", :schedule_interval=&gt;\"1 day\"},\n   :month=&gt;{:start_offset=&gt;\"3 month\", :end_offset=&gt;\"1 day\", :schedule_interval=&gt;\"1 day\"}}}\n</code></pre>"},{"location":"models/#metadata-from-the-hypertable","title":"Metadata from the hypertable","text":"<p>When you use the <code>acts_as_hypertable</code> macro, it will define several methods to help you to inspect timescaledb metadata like chunks and hypertable metadata.</p>"},{"location":"models/#chunks","title":"Chunks","text":"<p>To get all the chunks from a model's hypertable, you can use <code>.chunks</code>.</p> <pre><code>Event.chunks # =&gt; [#&lt;Timescaledb::Chunk&gt;, ...]\n</code></pre> <p>Warning</p> <p>The <code>chunks</code> method is only available when you use the <code>acts_as_hypertable</code> macro. By default, the macro will define several scopes and class methods to help you to inspect timescaledb metadata like chunks and hypertable metadata. You can disable this behavior by passing <code>skip_association_scopes</code>: <pre><code>class Event &lt; ActiveRecord::Base\n  acts_as_hypertable skip_association_scopes: true\nend\nEvent.chunks # =&gt; NoMethodError\n</code></pre></p>"},{"location":"models/#hypertable-metadata","title":"Hypertable metadata","text":"<p>To get the models' hypertable metadata, you can use <code>.hypertable</code>.</p> <pre><code>Event.hypertable # =&gt; #&lt;Timescaledb::Hypertable&gt;\n</code></pre> <p>To get hypertable metadata for all hypertables: <code>Timescaledb.hypertables</code>.</p>"},{"location":"models/#compression-settings","title":"Compression Settings","text":"<p>Compression settings are accessible through the hypertable.</p> <pre><code>Event.hypertable.compression_settings # =&gt; [#&lt;Timescaledb::CompressionSettings&gt;, ...]\n</code></pre> <p>To get compression settings for all hypertables: <code>Timescaledb.compression_settings</code>.</p>"},{"location":"models/#scopes","title":"Scopes","text":"<p>When you enable ActsAsHypertable on your model, we include a few default scopes. They are:</p> Scope name What they return <code>Model.previous_month</code> Records created in the previous month <code>Model.previous_week</code> Records created in the previous week <code>Model.this_month</code> Records created this month <code>Model.this_week</code> Records created this week <code>Model.yesterday</code> Records created yesterday <code>Model.today</code> Records created today <code>Model.last_hour</code> Records created in the last hour <p>All time-related scopes respect your application's timezone.</p> <p>Warning</p> <p>To disable these scopes, pass <code>skip_default_scopes: true</code> to the <code>acts_as_hypertable</code> macro. <pre><code>class Event &lt; ActiveRecord::Base\n  acts_as_hypertable skip_default_scopes: true\nend\n</code></pre></p>"},{"location":"models/#scenic-integration","title":"Scenic integration","text":"<p>The Scenic gem is easy to manage database view definitions for a Rails application. Unfortunately, TimescaleDB's continuous aggregates are more complex than regular PostgreSQL views, and the schema dumper included with Scenic can't dump a complete definition.</p> <p>This gem automatically configures Scenic to use a <code>Timescaledb::Scenic::Adapter.</code> which will correctly handle schema dumping.</p>"},{"location":"models/#managing-continuous-aggregates","title":"Managing Continuous Aggregates","text":"<p>You can manage your continuous aggregates with these methods:</p>"},{"location":"scenic_views/","title":"Scenic Integration","text":"<p>The Scenic gem provides a way to create versioned database views in Rails. If you're using Scenic, the Timescaledb gem will automatically detect it and already integrate into your code.</p>"},{"location":"scenic_views/#migration-script","title":"Migration script","text":"<p>Use the <code>create_scenic_continuous_aggregate</code> macro to invoke your materialized view.</p> <pre><code>class CreateScorePerHours &lt; ActiveRecord::Migration[7.0]\n  def change\n    create_scenic_continuous_aggregate :score_per_hours\n  end\nend\n</code></pre>"},{"location":"scenic_views/#define-the-view-in-a-sql-file","title":"Define the view in a sql file","text":"<p>The sql file should be placed in the <code>db/views</code> directory. The file should be named after the view and the version number. For example, <code>score_per_hours_v01.sql</code>.</p> <pre><code>SELECT game_id,\n       time_bucket(INTERVAL '1 hour', created_at) AS bucket,\n       AVG(score),\n       MAX(score),\n       MIN(score)\nFROM plays\nGROUP BY game_id, bucket;\n</code></pre> <p>Check out the source code of the full example.</p>"},{"location":"toolkit/","title":"The TimescaleDB Toolkit","text":"<p>The TimescaleDB Toolkit is an extension brought by Timescale for more hyperfunctions, fully compatible with TimescaleDB and PostgreSQL.</p> <p>They have almost no dependecy of hypertables but they play very well in the hypertables ecosystem. The mission of the toolkit team is to ease all things analytics when using TimescaleDB, with a particular focus on developer ergonomics and performance.</p> <p>Here, we're going to have a small walkthrough in some of the toolkit functions and the helpers that can make simplify the generation of some complex queries.</p> <p>Warning</p> <p>Note that we're just starting the toolkit integration in the gem and several functions are still experimental.</p>"},{"location":"toolkit/#the-add_toolkit_to_search_path-helper","title":"The <code>add_toolkit_to_search_path!</code> helper","text":"<p>Several functions on the toolkit are still in experimental phase, and for that reason they're not in the public schema, but lives in the <code>toolkit_experimental</code> schema.</p> <p>To use them without worring about the schema or prefixing it in all the cases, you can introduce the schema as part of the search_path.</p> <p>To make it easy in the Ruby side, you can call the method directly from the ActiveRecord connection:</p> <pre><code>ActiveRecord::Base.connection.add_toolkit_to_search_path!\n</code></pre> <p>This statement is actually adding the toolkit_experimental to the search path aside of the <code>public</code> and the <code>$user</code> variable path.</p> <p>The statement can be placed right before your usage of the toolkit. For example, if a single controller in your Rails app will be using it, you can create a filter in the controller to set up it before the use of your action.</p> <pre><code>class StatisticsController &lt; ActionController::Base\n  before_action :add_timescale_toolkit, only: [:complex_query]\n\n  def complex_query\n    # some code that uses the toolkit functions\n  end\n\n  protected\n  def add_timescale_toolkit\n    ActiveRecord::Base.connection.add_toolkit_to_search_path!\n  end\n</code></pre>"},{"location":"toolkit/#example-from-scratch-to-use-the-toolkit-functions","title":"Example from scratch to use the Toolkit functions","text":"<p>Let's start by working on some example about the volatility algorithm. This example is inspired in the function pipelines blog post, which brings an example about how to calculate volatility and then apply the function  pipelines to make the same with the toolkit.</p> <p>Success</p> <p>Reading the blog post before trying this is highly recommended, and will give you more insights on how to apply and use time vectors that is our next topic.</p> <p>Let's start by creating the <code>measurements</code> hypertable using a regular migration:</p> <pre><code>class CreateMeasurements &lt; ActiveRecord::Migration\n  def change\n    hypertable_options = {\n      time_column: 'ts',\n      chunk_time_interval: '1 day',\n    }\n    create_table :measurements, hypertable: hypertable_options, id: false do |t|\n      t.integer :device_id\n      t.decimal :val\n      t.timestamp :ts\n    end\n  end\nend\n</code></pre> <p>In this example, we just have a hypertable with no compression options. Every <code>1 day</code> a new child table aka chunk will be generated. No compression options for now.</p> <p>Now, let's add the model <code>app/models/measurement.rb</code>:</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  self.primary_key = nil\n\n  acts_as_hypertable time_column: \"ts\"\nend\n</code></pre> <p>At this moment, you can jump into the Rails console and start testing the model.</p>"},{"location":"toolkit/#seeding-some-data","title":"Seeding some data","text":"<p>Before we build a very complex example, let's build something that is easy to follow and comprehend. Let's create 3 records for the same device, representing a hourly measurement of some sensor.</p> <pre><code>yesterday = 1.day.ago\n[1,2,3].each_with_index do |v,i|\n  Measurement.create(device_id: 1, ts: yesterday + i.hour, val: v)\nend\n</code></pre> <p>Every value is a progression from 1 to 3. Now, we can build a query to get the values and let's build the example using plain Ruby.</p> <pre><code>values = Measurement.order(:ts).pluck(:val) # =&gt; [1,2,3]\n</code></pre> <p>Using plain Ruby, we can build this example with a few lines of code:</p> <p><pre><code>previous = nil\nvolatilities = values.map do |value|\n  if previous\n    delta = (value - previous).abs\n    volatility = delta\n  end\n  previous = value\n  volatility\nend\n# volatilities =&gt; [nil, 1, 1]\nvolatility = volatilities.compact.sum # =&gt; 2\n</code></pre> Compact can be skipped and we can also build the sum in the same loop. So, a refactored version would be:</p> <pre><code>previous = nil\nvolatility = 0\nvalues.each do |value|\n  if previous\n    delta = (value - previous).abs\n    volatility += delta\n  end\n  previous = value\nend\nvolatility # =&gt; 2\n</code></pre> <p>Now, it's time to move it to a database level calculating the volatility using plain postgresql. A subquery is required to build the calculated delta, so it seems a bit more confusing:</p> <pre><code>delta = Measurement.select(\"device_id, abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts)) as abs_delta\")\nMeasurement\n  .select(\"device_id, sum(abs_delta) as volatility\")\n  .from(\"(#{delta.to_sql}) as calc_delta\")\n  .group('device_id')\n</code></pre> <p>The final query for the example above looks like this:</p> <pre><code>SELECT device_id, SUM(abs_delta) AS volatility\nFROM (\n  SELECT device_id,\n    ABS(\n      val - LAG(val) OVER (\n        PARTITION BY device_id ORDER BY ts)\n      ) AS abs_delta\n  FROM \"measurements\"\n) AS calc_delta\nGROUP BY device_id\n</code></pre> <p>It's much harder to understand the actual example then go with plain SQL and now let's reproduce the same example using the toolkit pipelines:</p> <pre><code>Measurement\n  .select(&lt;&lt;-SQL).group(\"device_id\")\n    device_id,\n    timevector(ts, val)\n      -&gt; sort()\n      -&gt; delta()\n      -&gt; abs()\n      -&gt; sum() as volatility\n    SQL\n</code></pre> <p>As you can see, it's much easier to read and digest the example. Now, let's take a look in how we can generate the queries using the scopes injected by the <code>acts_as_hypertable</code> macro.</p>"},{"location":"toolkit/#configuring-the-segment_by-and-value_column","title":"Configuring the <code>segment_by</code> and <code>value_column</code>","text":"<p>Let's start changing the model to change the <code>acts_as_hypertable</code> to use the <code>segment_by</code> and <code>value_column</code> options.</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  acts_as_hypertable time_column: \"ts\",\n    segment_by: \"device_id\",\n    value_column: \"val\"\nend\n</code></pre> <p>Now, that we have it, let's create a scope for it:</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  acts_as_hypertable time_column: \"ts\",\n    segment_by: \"device_id\",\n    value_column: \"val\"\n\n  scope :volatility, -&gt; do\n    select(&lt;&lt;-SQL).group(\"device_id\")\n      device_id,\n      timevector(#{time_column}, #{value_column})\n        -&gt; sort()\n        -&gt; delta()\n        -&gt; abs()\n        -&gt; sum() as volatility\n    SQL\n  end\nend\n</code></pre> <p>Now, we have created the volatility scope, grouping by device_id always.</p> <p>In the Toolkit helpers, we have a similar version which also contains a default segmentation based in the <code>segment_by</code> configuration done through the <code>acts_as_hypertable</code> macro. A method <code>segment_by_column</code> is added to access this configuration, so we can make a small change that makes you completely understand the volatility scope.</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  acts_as_hypertable segment_by: \"device_id\",\n    value_column: \"val\",\n    time_column: \"ts\"\n\n  scope :volatility, -&gt; (columns=segment_by_column) do\n    _scope = select([*columns,\n        \"timevector(#{time_column},\n        #{value_column})\n           -&gt; sort()\n           -&gt; delta()\n           -&gt; abs()\n           -&gt; sum() as volatility\"\n    ].join(\", \"))\n    _scope = _scope.group(columns) if columns\n    _scope\n  end\nend\n</code></pre> <p>Testing the method:</p> <pre><code>Measurement.volatility.map(&amp;:attributes)\n# DEBUG -- : Measurement Load (1.6ms)  SELECT device_id, timevector(ts, val) -&gt; sort() -&gt; delta() -&gt; abs() -&gt; sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\"\n# =&gt; [{\"device_id\"=&gt;1, \"volatility\"=&gt;8.0}]\n</code></pre> <p>Let's add a few more records with random values:</p> <pre><code>yesterday = 1.day.ago\n(2..6).each do |d|\n  (1..10).each do |j|\n    Measurement.create(device_id: d, ts: yesterday + j.hour, val: rand(10))\n  end\nend\n</code></pre> <p>Testing all the values:</p> <pre><code> Measurement.order(\"device_id\").volatility.map(&amp;:attributes)\n # DEBUG -- : Measurement Load (1.3ms)  SELECT device_id, timevector(ts, val) -&gt; sort() -&gt; delta() -&gt; abs() -&gt; sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" ORDER BY device_id\n=&gt; [{\"device_id\"=&gt;1, \"volatility\"=&gt;8.0},\n {\"device_id\"=&gt;2, \"volatility\"=&gt;24.0},\n {\"device_id\"=&gt;3, \"volatility\"=&gt;30.0},\n {\"device_id\"=&gt;4, \"volatility\"=&gt;32.0},\n {\"device_id\"=&gt;5, \"volatility\"=&gt;44.0},\n {\"device_id\"=&gt;6, \"volatility\"=&gt;23.0}]\n</code></pre> <p>If the parameter is explicit <code>nil</code> it will not group by:</p> <pre><code>Measurement.volatility(nil).map(&amp;:attributes)\n# DEBUG -- : Measurement Load (5.4ms)  SELECT timevector(ts, val) -&gt; sort() -&gt; delta() -&gt; abs() -&gt; sum() as volatility FROM \"measurements\"\n# =&gt; [{\"volatility\"=&gt;186.0, \"device_id\"=&gt;nil}]\n</code></pre>"},{"location":"toolkit/#comparing-with-ruby-version","title":"Comparing with Ruby version","text":"<p>Now, it's time to benchmark and compare Ruby vs PostgreSQL solutions, verifying which is faster:</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  # code you already know\n  scope :volatility_by_device_id, -&gt; {\n    volatility = Hash.new(0)\n    previous = Hash.new\n    find_all do |measurement|\n      device_id = measurement.device_id\n      if previous[device_id]\n        delta = (measurement.val - previous[device_id]).abs\n        volatility[device_id] += delta\n      end\n      previous[device_id] = measurement.val\n    end\n    volatility\n  }\nend\n</code></pre> <p>Now, benchmarking the real time to compute it on Ruby in milliseconds.</p> <pre><code>Benchmark.measure { Measurement.volatility_by_device_id }.real * 1000\n# =&gt; 3.021999917924404\n</code></pre>"},{"location":"toolkit/#seeding-massive-data","title":"Seeding massive data","text":"<p>Now, let's use <code>generate_series</code> to fast insert a lot of records directly into the database and make it full of records.</p> <p>Let's just agree on some numbers to have a good start. Let's generate data for 5 devices emitting values every 5 minutes, which will generate around 50k records.</p> <p>Let's use some plain SQL to insert the records now:</p> <pre><code>sql = \"INSERT INTO measurements (ts, device_id, val)\nSELECT ts, device_id, random()*80\nFROM generate_series(TIMESTAMP '2022-01-01 00:00:00',\n                   TIMESTAMP '2022-02-01 00:00:00',\n             INTERVAL '5 minutes') AS g1(ts),\n      generate_series(0, 5) AS g2(device_id);\n\"\nActiveRecord::Base.connection.execute(sql)\n</code></pre> <p>In my MacOS M1 processor it took less than a second to insert the 53k records:</p> <pre><code># DEBUG (177.5ms)  INSERT INTO measurements (ts, device_id, val) ..\n# =&gt; #&lt;PG::Result:0x00007f8152034168 status=PGRES_COMMAND_OK ntuples=0 nfields=0 cmd_tuples=53574&gt;\n</code></pre> <p>Now, let's measure compare the time to process the volatility:</p> <pre><code>Benchmark.bm do |x|\n  x.report(\"ruby\")  { pp Measurement.volatility_by_device_id }\n  x.report(\"sql\") { pp Measurement.volatility(\"device_id\").map(&amp;:attributes) }\nend\n#           user     system      total        real\n# ruby    0.612439   0.061890   0.674329 (  0.727590)\n# sql     0.001142   0.000301   0.001443 (  0.060301)\n</code></pre> <p>Calculating the performance ratio we can see <code>0.72 / 0.06</code> means that SQL is 12 times faster than Ruby to process volatility \ud83c\udf89</p> <p>Just considering it was localhost, we don't have the internet to pass all the records over the wires. Now, moving to a remote host look the numbers:</p> <p>Warning</p> <p>Note that the previous numbers where using localhost. Now, using a remote connection between different regions, it looks even ~500 times slower than SQL.</p> <pre><code>        user     system      total        real\nruby 0.716321   0.041640   0.757961 (  6.388881)\nsql  0.001156   0.000177   0.001333 (  0.161270)\n</code></pre> <p>Let\u2019s recap what\u2019s time consuming here. The <code>find_all</code> is just not optimized to fetch the data and also consuming most of the time here. It\u2019s also fetching the data and converting it to ActiveRecord model which has thousands of methods.</p> <p>It\u2019s very comfortable but just need the attributes to make it.</p> <p>Let\u2019s optimize it by plucking an array of values grouped by device.</p> <pre><code>class Measurement &lt; ActiveRecord::Base\n  # ...\n  scope :values_from_devices, -&gt; {\n    ordered_values = select(:val, :device_id).order(:ts)\n    Hash[\n      from(ordered_values)\n      .group(:device_id)\n      .pluck(\"device_id, array_agg(val)\")\n    ]\n  }\nend\n</code></pre> <p>Now, let's create a method for processing volatility.</p> <pre><code>class Volatility\n  def self.process(values)\n    previous = nil\n    deltas = values.map do |value|\n      if previous\n        delta = (value - previous).abs\n        volatility = delta\n      end\n      previous = value\n      volatility\n    end\n    #deltas =&gt; [nil, 1, 1]\n    deltas.shift\n    volatility = deltas.sum\n  end\n  def self.process_values(map)\n    map.transform_values(&amp;method(:process))\n  end\nend\n</code></pre> <p>Now, let's change the benchmark to expose the time for fetching and processing:</p> <pre><code>volatilities = nil\n\nActiveRecord::Base.logger = nil\nBenchmark.bm do |x|\n  x.report(\"ruby\")  { Measurement.volatility_ruby }\n  x.report(\"sql\") { Measurement.volatility_sql.map(&amp;:attributes)  }\n  x.report(\"fetch\") { volatilities =  Measurement.values_from_devices }\n  x.report(\"process\") { Volatility.process_values(volatilities) }\nend\n</code></pre> <p>Checking the results:</p> <pre><code>      user     system      total        real\nruby  0.683654   0.036558   0.720212 (  0.743942)\nsql  0.000876   0.000096   0.000972 (  0.054234)\nfetch  0.078045   0.003221   0.081266 (  0.116693)\nprocess  0.067643   0.006473   0.074116 (  0.074122)\n</code></pre> <p>Much better,\u00a0now we can see only 200ms difference between real time which means ~36% more.</p> <p>If we try to break down a bit more of the SQL part, we can see that the </p> <pre><code>EXPLAIN ANALYSE\n  SELECT device_id, array_agg(val)\n  FROM (\n    SELECT val, device_id\n    FROM measurements\n    ORDER BY ts ASC\n  ) subquery\n  GROUP BY device_id;\n</code></pre> <p>We can check the execution time and make it clear how much time is necessary just for the processing part, isolating network and the ActiveRecord layer.</p> <pre><code>\u2502 Planning Time: 17.761 ms                                                                                                                                                                     \u2502\n\u2502 Execution Time: 36.302 ms\n</code></pre> <p>So, it means that from the 116ms to fetch the data, only 54ms was used from the DB and the remaining 62ms was consumed by network + ORM.</p>"},{"location":"toolkit_candlestick/","title":"Toolkit Candlesticks tutorial","text":"<p># Candlesticks</p> <p>Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The [toolkit][toolkit] allows you to compute candlesticks with the [candlestick][candlestick] function.</p> <p>Let's start by defining a table that stores trades from financial market data, and then we can calculate the candlesticks with the Timescaledb Toolkit.</p>"},{"location":"toolkit_candlestick/#setting-up-the-environment","title":"Setting up the environment","text":"<p>First, we'll set up our environment with the necessary gems:</p> <pre><code>require 'bundler/inline'\n\ngemfile(true) do\n  gem 'timescaledb', path: '../..'\n  gem 'pry'\n  gem 'puma'\n  gem 'sinatra'\n  gem 'sinatra-contrib'\n  gem 'sinatra-reloader'\nend\n\nActiveRecord::Base.establish_connection ARGV.first\n</code></pre>"},{"location":"toolkit_candlestick/#creating-the-hypertable","title":"Creating the hypertable","text":"<p>We'll create a hypertable called <code>ticks</code> to store the market data:</p> <pre><code>def db(&amp;block)\n  ActiveRecord::Base.logger = Logger.new(STDOUT)\n  ActiveRecord::Base.connection.instance_exec(&amp;block)\n  ActiveRecord::Base.logger = nil\nend\n\ndb do\n  drop_table :ticks, if_exists: true, force: :cascade\n\n  hypertable_options = {\n    time_column: \"time\",\n    chunk_time_interval: \"1 day\",\n    compress_segmentby: \"symbol\",\n    compress_orderby: \"time\",\n    compress_after: \"1 month\"\n  }\n  create_table :ticks, hypertable: hypertable_options, id: false do |t|\n    t.timestamp :time\n    t.string :symbol\n    t.decimal :price\n    t.decimal :volume\n  end\n\n  add_index :ticks, [:time, :symbol]\nend\n</code></pre>"},{"location":"toolkit_candlestick/#creating-the-orm-model","title":"Creating the ORM model","text":"<p>To define the model, we'll inherit <code>ActiveRecord::Base</code> to create a model. Timeseries data will always require the time column, and the primary key can be discarded. A few default methods will not work if they depend on the id of the object.</p> <p>The model is the best place to describe how you'll be using the timescaledb to keep your model DRY and consistent.</p> <pre><code>class Tick &lt; ActiveRecord::Base\n  acts_as_hypertable time_column: :time,\n    segment_by: :symbol,\n    value_column: :price\n\n   scope :ohlcv, -&gt; do\n    select(\"symbol,\n            first(price, time) as open,\n            max(price) as high,\n            min(price) as low,\n            last(price, time) as close,\n            sum(volume) as volume\").group(\"symbol\")\n  end\n  scope :plotly_candlestick, -&gt; (from: nil) do\n    data = all.to_a\n    {\n      type: 'candlestick',\n      xaxis: 'x',\n      yaxis: 'y',\n      x: data.map(&amp;:time),\n      open: data.map(&amp;:open),\n      high: data.map(&amp;:high),\n      low: data.map(&amp;:low),\n      close: data.map(&amp;:close),\n      volume: data.map(&amp;:volume)\n    }\n  end\n\n  continuous_aggregates(\n    timeframes: [:minute, :hour, :day, :month],\n    scopes: [:ohlcv],\n    refresh_policy: {\n      minute: { start_offset: \"10 minutes\", end_offset: \"1 minute\", schedule_interval: \"1 minute\" },\n      hour:   { start_offset: \"4 hour\",     end_offset: \"1 hour\",   schedule_interval: \"1 hour\" },\n      day:    { start_offset: \"3 day\",      end_offset: \"1 day\",    schedule_interval: \"1 day\" },\n      month:  { start_offset: \"3 month\",    end_offset: \"1 day\",  schedule_interval: \"1 day\" }\n  })\n\n  descendants.each{|e|e.time_vector_options = time_vector_options.merge(value_column: :close)}\nend\n</code></pre> <p>The <code>acts_as_hypertable</code> macro will assume the actual model corresponds to a hypertable and inject useful scopes and methods that can be wrapped to the following TimescaleDB features:</p> <ul> <li><code>.hypertable</code> will give you access to the [hypertable][hypertable] domain, the <code>table_name</code> will be used to get all metadata from the <code>_timescaledb_catalog</code> and combine all the functions that receives a hypertable_name as a parameter.</li> <li>The <code>time_column</code> keyword argument will be used to build scopes like <code>.yesterday</code>, <code>.previous_week</code>, <code>.last_hour</code>. And can be used for your own scopes using the <code>time_column</code> metadata.</li> </ul> <p>The <code>value_column:</code> will be combined with the <code>time_column</code> from the hypertable to use scopes like <code>candlestick</code>, <code>volatility</code>, <code>lttb</code> and just configure the missing information.</p> <p>The <code>segment_by:</code> will be widely used in the scopes to group by the data.</p> <p>By convention, all scopes reuse the metadata from the configuration. It can facilitate the process of building a lot of hypertable abstractions to facilitate the use combined scopes in the queries.</p>"},{"location":"toolkit_candlestick/#the-acts_as_hypertable-macro","title":"The <code>acts_as_hypertable</code> macro","text":"<p>The <code>acts_as_hypertable</code> will bring the <code>Model.hypertable</code> which will allow us to use a set of timeseries related set what are the default columns used to calculate the data.</p>"},{"location":"toolkit_candlestick/#the-continuous_aggregates-macro","title":"The <code>continuous_aggregates</code> macro","text":"<p>The <code>continuous_aggregates</code> macro will allow us to create continuous aggregates for the model. Generating a new materialized view for each scope + timeframe that will be continuously aggregated from the raw data.</p> <p>The views will be rolling out from previous time frames, so it will be very efficient in terms of resource usage.</p>"},{"location":"toolkit_candlestick/#inserting-data","title":"Inserting data","text":"<p>The <code>generate_series</code> sql function can speed up the process to seed some random data and make it available to start playing with the queries.</p> <p>The following code will insert tick data simulating prices from the previous week until yesterday. We're using a single symbol and one tick every 10 seconds.</p> <pre><code>ActiveRecord::Base.connection.instance_exec do\n  data_range = {from: 1.week.ago.to_date, to: 1.day.from_now.to_date}\n  execute(ActiveRecord::Base.sanitize_sql_for_conditions( [&lt;&lt;~SQL, data_range]))\n    INSERT INTO ticks\n    SELECT time, 'SYMBOL', 1 + (random()*30)::int, 100*(random()*10)::int\n    FROM generate_series(\n      TIMESTAMP :from,\n      TIMESTAMP :to,\n      INTERVAL '10 second') AS time;\n    SQL\nend\n</code></pre> <p>The database will seed a week of trade data with a randomize prices and volumes simulating one event every 10 seconds.</p> <p>The candlestick will split the timeframe by the <code>time_column</code> and use the <code>price</code> as the default value to process the candlestick. It will also segment the candles by the <code>symbol</code>. Symbol can be any stock trade and it's good to be segmenting and indexing by it.</p> <p>If you need to generate some data for your table, please check [this post][2].</p>"},{"location":"toolkit_candlestick/#query-data","title":"Query data","text":"<p>When the <code>acts_as_hypertable</code> method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the <code>_candlestick</code>.</p> <p>The <code>candlestick</code> scope is available with a few parameters that inherits the configuration from the <code>acts_as_hypertable</code> declared previously.</p> <p>The simplest query is:</p> <pre><code>Tick.candlestick(timeframe: '1m')\n</code></pre> <p>It will generate the following SQL:</p> <pre><code> SELECT symbol,\n    \"time\",\n    open(candlestick),\n    high(candlestick),\n    low(candlestick),\n    close(candlestick),\n    open_time(candlestick),\n    high_time(candlestick),\n    low_time(candlestick),\n    close_time(candlestick),\n    volume(candlestick),\n    vwap(candlestick)\nFROM (\n    SELECT time_bucket('1m', time) as time,\n      \"ticks\".\"symbol\",\n      candlestick_agg(time, price, volume) as candlestick\n    FROM \"ticks\" GROUP BY 1, 2 ORDER BY 1)\nAS candlestick\n</code></pre> <p>The timeframe argument can also be skipped and the default is <code>1 hour</code>.</p> <p>You can also combine other scopes to filter data before you get the data from the candlestick:</p> <pre><code>Tick.yesterday\n  .where(symbol: \"APPL\")\n  .candlestick(timeframe: '1m')\n</code></pre> <p>The <code>yesterday</code> scope is automatically included because of the <code>acts_as_hypertable</code> macro. And it will be combining with other where clauses.</p>"},{"location":"toolkit_candlestick/#continuous-aggregates","title":"Continuous aggregates","text":"<p>If you would like to create the continuous process one by one in the stream and aggregate the candlesticks on a materialized view you can use continuous aggregates for it.</p> <p>The next examples shows how to create a single continuous aggregates of 1 minute candlesticks:</p> <pre><code>ActiveRecord::Base.connection.instance_exec do\n  options = {\n    with_data: true,\n    refresh_policies: {\n      start_offset: \"INTERVAL '1 month'\",\n      end_offset: \"INTERVAL '1 minute'\",\n      schedule_interval: \"INTERVAL '1 minute'\"\n    }\n  }\n  create_continuous_aggregate('candlestick_1m', Tick._candlestick(timeframe: '1m'), **options)\nend\n</code></pre> <p>Note that the <code>create_continuous_aggregate</code> calls the <code>to_sql</code> method in case the second parameter is not a string.</p> <p>Also, we're using the <code>_candlestick</code> method scope instead of the <code>candlestick</code> one.</p> <p>The reason is that the <code>candlestick</code> method already bring the attribute values while the <code>_candlestick</code> can bring you the pre-processed data in a intermediate state that can be rolled up with other candlesticks. For example, let's say you already created a continuous aggregates of one minute and now you'd like to process 5 minutes. You don't need to reprocess the raw data. You can build the candlestick using the information from the one minute candlesticks.</p>"},{"location":"toolkit_candlestick/#models-for-views","title":"Models for views","text":"<p>The macro <code>continuous_aggregates</code> will create a new model for each continuous aggregate.</p> <p>It's very convenient to setup models for continuous aggregates which can make it easy to inherit all smart methods to compose queries.</p> <pre><code>Tick::CandlestickPerMinute\nTick::CandlestickPerHour\nTick::CandlestickPerDay\nTick::CandlestickPerMonth\n</code></pre>"},{"location":"toolkit_candlestick/#hierarchical-continuous-aggregates","title":"Hierarchical continuous aggregates","text":"<p>After you get the first one minute continuous aggregates, you don't need to revisit the raw data to create candlesticks from it. You can build the 1 hour candlestick from the 1 minute candlestick. The [Hierarchical continuous aggregates][hcaggs] are very useful to save IO and processing time.</p>"},{"location":"toolkit_candlestick/#rollup","title":"Rollup","text":"<p>The [candlestick_agg][candlestick_agg] function returns a <code>candlesticksummary</code> object.</p> <p>The rollup allows you to combine candlestick summaries into new structures from smaller timeframes to bigger timeframes without needing to reprocess all the data.</p> <p>With this feature, you can group by the candlesticks multiple times saving processing from the server and make it easier to manage aggregations with different time intervals.</p> <p>In the previous example, we used the <code>.candlestick</code> function that returns already the attributes from the different timeframes. In the SQL command it's calling the <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>volume</code>, and <code>vwap</code> functions that can access the values behind the candlesticksummary type.</p> <p>To merge the candlesticks, the rollup method can aggregate several <code>candlesticksummary</code> objects into a bigger timeframe.</p> <p>Let's rollup the structures:</p> <pre><code>module Candlestick\n  extend ActiveSupport::Concern\n\n  included do\n    # ... rest of the code remains the same\n\n    scope :rollup, -&gt; (timeframe: '1h') do\n      bucket = %|time_bucket('#{timeframe}', \"time_bucket\")|\n      select(bucket,\"symbol\",\n            \"rollup(candlestick) as candlestick\")\n      .group(1,2)\n      .order(1)\n    end\n  end\nend\n</code></pre> <p>Now, the new views in bigger timeframes can be added using it's own objects.</p> <pre><code>ActiveRecord::Base.connection.instance_exec do\n  options = -&gt; (timeframe) {\n    {\n      with_data: false,\n      refresh_policies: {\n        start_offset: \"INTERVAL '1 month'\",\n        end_offset: \"INTERVAL '#{timeframe}'\",\n        schedule_interval: \"INTERVAL '#{timeframe}'\"\n      }\n    }\n  }\n  create_continuous_aggregate('candlestick_1h', Candlestick1m.rollup(timeframe: '1 hour'), **options['1 hour'])\n  create_continuous_aggregate('candlestick_1d', Candlestick1h.rollup(timeframe: '1 day'),  **options['1 day'])\nend\n</code></pre> <p>The final SQL executed to create the first [hierarchical continuous aggregates][hcaggs] is the following:</p> <pre><code>CREATE MATERIALIZED VIEW candlestick_1h\nWITH (timescaledb.continuous) AS\n  SELECT time_bucket('1 hour', \"time_bucket\"),\n    \"candlestick_1m\".\"symbol\",\n    rollup(candlestick) as candlestick\n  FROM \"candlestick_1m\"\n  GROUP BY 1, 2\n  ORDER BY 1\nWITH DATA;\n</code></pre> <p>So, as you can see all candlestick of one hour views follows the same interface of one minute, having the same column names and values, allowing to be reuse in larger timeframes.</p>"},{"location":"toolkit_candlestick/#refresh-policy","title":"Refresh policy","text":"<p>Timescaledb is assuming you're storing real time data. Which means you can continuous feed the <code>ticks</code> table and aggregate the materialized data from time to time.</p> <p>When <code>create_continuous_aggregate</code> is called with a <code>schedule_interval</code> it will also execute the following SQL line:</p> <pre><code>SELECT add_continuous_aggregate_policy('candlestick_1h',\n  start_offset =&gt; INTERVAL '1 month',\n  end_offset =&gt; INTERVAL '1 hour',\n  schedule_interval =&gt; INTERVAL '1 hour');\n</code></pre> <p>Instead of updating the values row by row, the refresh policy will automatically run in background and aggregate the new data with the configured timeframe.</p>"},{"location":"toolkit_candlestick/#querying-continuous-aggregates-with-custom-activerecord-models","title":"Querying Continuous Aggregates with custom ActiveRecord models","text":"<p>With the <code>Candlestick1m</code> and <code>Candlestick1h</code> wrapping the continuous aggregates into models, now, it's time to explore the available scopes and what to do with it.</p> <pre><code>Candlestick1m.yesterday.first\n</code></pre> <p>It will run the following SQL: <pre><code>SELECT \"candlestick_1m\".*\nFROM \"candlestick_1m\"\nWHERE (DATE(time_bucket) = '2023-01-23') LIMIT 1;\n</code></pre></p> <p>And return the following object:</p> <pre><code>#&lt;Candlestick1m:0x000000010fbeff68\n time_bucket: 2023-01-23 00:00:00 UTC,\n symbol: \"SYMBOL\",\n candlestick:\n  \"(version:1,open:(ts:\\\"2023-01-23 00:00:00+00\\\",val:9),high:(ts:\\\"2023-01-23 00:00:10+00\\\",val:24),low:(ts:\\\"2023-01-23 00:00:50+00\\\",val:2),close:(ts:\\\"2023-01-23 00:00:50+00\\\",val:2),volume:Transaction(vol:2400,vwap:26200))\",\n open: nil,\n open_time: nil,\n high: nil,\n high_time: nil,\n low: nil,\n low_time: nil,\n close: nil,\n close_time: nil,\n volume: nil,\n vwap: nil&gt;\n</code></pre> <p>Note that the attributes are not available in the object but a <code>candlestick</code> attribute is present holding all the information. That's why it's necessary to use the <code>attributes</code> scope: <pre><code>Tick::CandlestickPerMinute.yesterday.attributes.first\n</code></pre></p> <p>Which will run the following query: <pre><code>SELECT symbol, time_bucket,\n  open(candlestick),\n  high(candlestick),\n  low(candlestick),\n  close(candlestick),\n  open_time(candlestick),\n  high_time(candlestick),\n  low_time(candlestick),\n  close_time(candlestick),\n  volume(candlestick),\n  vwap(candlestick)\nFROM \"candlestick_1m\"\nWHERE (DATE(time_bucket) = '2023-01-23') LIMIT 1;\n</code></pre></p> <p>And the object will be filled with the attributes:</p> <pre><code>=&gt; {\n time_bucket: 2023-01-23 00:00:00 UTC,\n symbol: \"SYMBOL\",\n open: 0.9e1,\n open_time: 2023-01-23 00:00:00 +0000,\n high: 0.24e2,\n high_time: 2023-01-23 00:00:10 +0000,\n low: 0.2e1,\n low_time: 2023-01-23 00:00:50 +0000,\n close: 0.2e1,\n close_time: 2023-01-23 00:00:50 +0000,\n volume: 0.24e4,\n vwap: 0.1091666666666666e2\n}\n</code></pre> <p>And from minute to one hour to a day:</p> <pre><code>Tick::CandlestickPerMinute.rollup(\"'5 minutes'\")\n</code></pre> <p>Both examples are just using the one minute continuous aggregates view and reprocessing it from there.</p> <p>Composing the subqueries will probably be less efficient and unnecessary as we already created more continuous aggregates in the top of another continuous aggregates. Here is the SQL generated from the last nested rollups code:</p> <pre><code>SELECT symbol, time_bucket,\n  open(candlestick),\n  high(candlestick),\n  low(candlestick),\n  close(candlestick),\n  open_time(candlestick),\n  high_time(candlestick),\n  low_time(candlestick),\n  close_time(candlestick),\n  volume(candlestick),\n  vwap(candlestick)\nFROM (\n  SELECT time_bucket('1 day', \"time_bucket\"),\n    symbol,\n    rollup(candlestick) as candlestick\n  FROM (\n    SELECT time_bucket('1 hour', \"time_bucket\"),\n      \"candlestick_1m\".\"symbol\",\n      rollup(candlestick) as candlestick\n    FROM \"candlestick_1m\" GROUP BY 1, 2 ORDER BY 1\n  ) subquery GROUP BY 1, 2 ORDER BY 1\n) subquery\n</code></pre>"},{"location":"toolkit_candlestick/#plotting-data","title":"Plotting data","text":"<p>Now, the final step is plot the data using the javascript <code>plotly</code> library.</p> <p>For this step, we're going to use a sinatra library to serve HTML and javascript and build the endpoints that will be consumed by the front end.</p>"},{"location":"toolkit_candlestick/#the-sinatra-app","title":"The Sinatra App","text":"<pre><code>require 'sinatra/base'\nrequire \"sinatra/json\"\n\nclass App &lt; Sinatra::Base\n  get '/candlestick.js' do\n    send_file 'candlestick.js'\n  end\n\n  get '/candlestick_1m' do\n    json({\n      title: \"Candlestick 1 minute last hour\",\n      data: Candlestick1m.last_hour.plotly_candlestick\n    })\n  end\n\n  get '/candlestick_1h' do\n    json({\n      title: \"Candlestick yesterday hourly\",\n      data: Candlestick1h.yesterday.plotly_candlestick\n    })\n  end\n\n  get '/candlestick_1d' do\n    json({\n      title: \"Candlestick daily this month\",\n      data: Candlestick1d.previous_week.plotly_candlestick\n    })\n  end\n\n  get '/' do\n&lt;&lt;-HTML\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\"&gt;&lt;/script&gt;\n    &lt;script src='https://cdn.plot.ly/plotly-2.17.1.min.js'&gt;&lt;/script&gt;\n    &lt;script src='/candlestick.js'&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div id='charts'&gt;\n  &lt;/body&gt;\nHTML\n  end\n\n  run! if app_file == $0\nend\n</code></pre>"},{"location":"toolkit_candlestick/#plotting-data-with-javascript","title":"Plotting data with Javascript","text":"<p>And the <code>candlesticks.js</code> file will be responsible for fetch data async and add new candlestick charts.</p> <pre><code>let addChart = () =&gt; $('&lt;div/&gt;').appendTo('#charts')[0]\nfunction ohlcChartFrom(url) {\n  $.ajax({\n    url: url,\n    success: function(result) {\n      let {data, title} = result;\n      let {x, open, high, low, close, type} = data;\n      open = open.map(parseFloat);\n      high = high.map(parseFloat);\n      low = low.map(parseFloat);\n      close = close.map(parseFloat);\n      var layout = {\n        title: title, \n        dragmode: 'zoom',\n        margin: { r: 10, t: 25, b: 40, l: 60 },\n        showlegend: false,\n        xaxis: {\n          autorange: true,\n          domain: [0, 1],\n          title: 'Date',\n          type: 'date'\n        },\n        yaxis: {\n          autorange: true,\n          domain: [0, 1],\n          type: 'linear'\n        }\n      };\n\n      ohlc = {x, open, high, low, close, type};\n      Plotly.newPlot(addChart(), [ohlc], layout);\n    }\n  });\n};\n\n$( document ).ready(function() {\n  ohlcChartFrom('/candlestick_1m');\n  ohlcChartFrom('/candlestick_1h');\n  ohlcChartFrom('/candlestick_1d');\n});\n</code></pre> <p>Note that a new <code>plotly_candlestick</code> scope was mentioned in the view models and we need to add it to the <code>Candlestick</code> module to make it available for all the charts.</p> <pre><code>module Candlestick\n  extend ActiveSupport::Concern\n\n  included do\n    # ... rest of the code remains the same\n\n    scope :plotly_candlestick, -&gt; do\n      data = attributes\n\n      {\n        type: 'candlestick',\n        xaxis: 'x',\n        yaxis: 'y',\n        x: data.map(&amp;:time_bucket),\n        open: data.map(&amp;:open),\n        high: data.map(&amp;:high),\n        low: data.map(&amp;:low),\n        close: data.map(&amp;:close)\n      }\n    end\n  end\nend\n</code></pre>"},{"location":"toolkit_candlestick/#formatting-time-vectors","title":"Formatting time vectors","text":"<p>Another function from toolkit that can help you to prepare the data to plot is the <code>to_text</code> one from the toolkit. This is an experimental feature that allows you to prepare the JSON output using a template directly in the database to easily dump the output data without need to convert the data.</p> <pre><code>module Candlestick\n  extend ActiveSupport::Concern\n\n  included do\n    # ... rest of the code remains the same\n    scope :time_vector_from_candlestick, -&gt; ( attribute: \"close\") do\n      select(\"timevector(time_bucket, #{attribute}(candlestick))\")\n    end\n\n    scope :plotly_attribute,\n      -&gt; (attribute: \"close\",\n          from: nil,\n          template: %\\'{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}'\\) do\n      from ||= time_vector_from_candlestick(attribute: attribute)\n\n      select(\"toolkit_experimental.to_text(tv.timevector, #{template})::json\")\n        .from(\"( #{from.to_sql} ) as tv\")\n        .first[\"to_text\"]\n    end\n  end\nend\n</code></pre> <p>The final SQL will look something like:</p> <pre><code>SELECT toolkit_experimental.to_text(\n    tv.timevector,\n    '{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}'\n  )::json\nFROM (\n  SELECT timevector(time_bucket, close(candlestick))\n  FROM \"candlestick_1h\"\n) as tv LIMIT 1\n</code></pre>"},{"location":"toolkit_lttb_tutorial/","title":"Toolkit LTTB Tutorial","text":"<p>Largest Triangle Three Buckets is a downsampling method that tries to retain visual similarity between the downsampled data and the original dataset.</p> <p>While most frameworks implement it in the front end, TimescaleDB Toolkit provides an implementation that takes (timestamp, value) pairs, sorts them if needed, and downsamples the values directly in the database.</p> <p>In the following steps, you'll learn how to use LTTB from both databases and the Ruby programming language\u2014writing the LTTB algorithm in Ruby from scratch\u2014fully comprehend how it works and later compares the performance and usability of both solutions.</p> <p>Later, we'll benchmark the downsampling methods and the plain data using a real scenario. The data points are actual data from the weather dataset.</p> <p>If you want to run it yourself, feel free to use the example that contains all the steps we will describe here.</p>"},{"location":"toolkit_lttb_tutorial/#setup-the-dependencies","title":"Setup the dependencies","text":"<p>Bundler inline avoids the creation of the <code>Gemfile</code> to prototype code that you can ship in a single file. You can declare all the gems in the <code>gemfile</code> code block, and Bundler will install them dynamically.</p> <pre><code>require 'bundler/inline'\n\ngemfile(true) do\n  gem 'timescaledb'\n  gem 'pry'\n  gem 'chartkick'\n  gem 'sinatra'\nend\n</code></pre> <pre><code>require 'timescaledb/toolkit'\n</code></pre> <p>The Timescale gem doesn't require the toolkit by default, so you must specify it to use.</p> <p>Warning</p> <p>Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file.</p> <p>Let's take a look at what dependencies we have for what purpose:</p> <ul> <li>timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions.</li> <li>pry is here because it's the best REPL to debug any Ruby code. We add it in the end to ease the exploring session you can do yourself after learning with the tutorial.</li> <li>chartkick is the library that can plot the values and make it easy to plot the data results.</li> <li>sinatra is a DSL for quickly creating web applications with minimal     effort.</li> </ul>"},{"location":"toolkit_lttb_tutorial/#setup-database","title":"Setup database","text":"<p>Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here.</p>"},{"location":"toolkit_lttb_tutorial/#establishing-the-connection","title":"Establishing the connection","text":"<p>The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line.</p> <pre><code>PG_URI = ARGV.last\nActiveRecord::Base.establish_connection(PG_URI)\n</code></pre> <p>If this line works, it means your connection is good.</p>"},{"location":"toolkit_lttb_tutorial/#downloading-the-dataset","title":"Downloading the dataset","text":"<p>The weather dataset is available here, and here is small automation to make it run smoothly with small, medium, and big data sets.</p> <pre><code>VALID_SIZES = %i[small med big]\ndef download_weather_dataset size: :small\n  unless VALID_SIZES.include?(size)\n    fail \"Invalid size: #{size}. Valid are #{VALID_SIZES}\"\n  end\n  url = \"https://timescaledata.blob.core.windows.net/datasets/weather_#{size}.tar.gz\"\n  puts \"fetching #{size} weather dataset...\"\n  system \"wget \\\"#{url}\\\"\"\n  puts \"done!\"\nend\n</code></pre> <p>Now, let's create a setup method to verify if the database is created and have the data loaded, and fetch it if necessary.</p> <pre><code>def setup size: :small\n  file = \"weather_#{size}.tar.gz\"\n  download_weather_dataset unless File.exists? file\n  puts \"extracting #{file}\"\n  system \"tar -xvzf #{file} \"\n  puts \"creating data structures\"\n  system \"psql #{PG_URI} &lt; weather.sql\"\n  system %|psql #{PG_URI} -c \"\\\\COPY locations FROM weather_#{size}_locations.csv CSV\"|\n  system %|psql #{PG_URI} -c \"\\\\COPY conditions FROM weather_#{size}_conditions.csv CSV\"|\nend\n</code></pre> <p>Info</p> <p>Maybe you'll need to recreate the database if you want to test with a different dataset.</p>"},{"location":"toolkit_lttb_tutorial/#declaring-the-models","title":"Declaring the models","text":"<p>Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device.</p> <pre><code>class Location &lt; ActiveRecord::Base\n  self.primary_key = \"device_id\"\n\n  has_many :conditions, foreign_key: \"device_id\"\nend\n</code></pre> <p>Every location emits weather conditions with <code>temperature</code> and <code>humidity</code> every X minutes.</p> <p>The <code>conditions</code> is the time-series data we'll refer to here.</p> <pre><code>class Condition &lt; ActiveRecord::Base\n  acts_as_hypertable time_column: \"time\",\n    segment_by: \"device_id\",\n    value_column: \"temperature\"\n  belongs_to :location, foreign_key: \"device_id\"\nend\n</code></pre>"},{"location":"toolkit_lttb_tutorial/#putting-all-together","title":"Putting all together","text":"<p>Now it's time to call the methods we implemented before. So, let's set up a logger to STDOUT to confirm the steps and add the toolkit to the search path.</p> <p>Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary.</p> <pre><code>ActiveRecord::Base.connection.instance_exec do\n  ActiveRecord::Base.logger = Logger.new(STDOUT)\n  add_toolkit_to_search_path!\n\n  unless Condition.table_exists?\n    setup size: :small\n  end\nend\n</code></pre> <p>The <code>setup</code> method also can fetch different datasets and you'll need to manually drop the <code>conditions</code> and <code>locations</code> tables to reload it.</p> <p>Info</p> <p>If you want to go deeper and reload everything every time, feel free to add the following lines before the <code>unless</code> block:</p> <pre><code>drop_table(:conditions) if Condition.table_exists?\ndrop_table(:locations) if Location.table_exists?\n</code></pre> <p>Let's keep the example simple to run it manually and drop the tables when we want to run everything from scratch.</p>"},{"location":"toolkit_lttb_tutorial/#processing-lttb-in-ruby","title":"Processing LTTB in Ruby","text":"<p>You can find an old lttb gem available if you want to cut down this step but this library is not fully implementing the lttb algorithm, and the results may differ from the Timescale implementation.</p> <p>If you want to understand the algorithm behind the scenes, this step will make it very clear and easy to digest. You can also preview the original lttb here.</p> <p>Info</p> <p>The original thesis describes lttb as:</p> <p>The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now. The question is what point should the algorithm use in the last bucket to form the triangle.\"</p> <p>The obvious answer is to use a brute-force approach and simply try out all the possibilities. That is, for each point in the current bucket, form a triangle with all the points in the next bucket. It turns out that this gives a fairly good visual result, but as with many brute-force approaches it is inefficient. For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another and more clever solution is to add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with this two fixed point in the adjacent buckets is then selected. In figure 4.4 it is shown how point B forms the largest triangle across the buckets with fixed point A (previously selected) and the temporary point C.</p> <p></p>"},{"location":"toolkit_lttb_tutorial/#calculate-the-area-of-a-triangle","title":"Calculate the area of a Triangle","text":"<p>To demonstrate the same, let's create a module <code>Triangle</code> with an <code>area</code> method that accepts three points <code>a',</code>b<code>, and</code>c<code>, which will be pairs of</code>x<code>and</code>y' cartesian coordinates.</p> <pre><code>module Triangle\n  module_function\n  def area(a, b, c)\n    (ax, ay), (bx, by), (cx, cy) = a,b,c\n    (\n      (ax - cx).to_f * (by - ay) -\n      (ax - bx).to_f * (cy - ay)\n    ).abs * 0.5\n  end\nend\n</code></pre> <p>Info</p> <p>In this implementation, we're using the shoelace method.</p> <p>The shoelace method (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. It has applications in surveying and forestry, among other areas. Source: Shoelace formula Wikipedia</p>"},{"location":"toolkit_lttb_tutorial/#initializing-the-lttb-class","title":"Initializing the Lttb class","text":"<p>The lttb class will be responsible for processing the data and downsampling the points to the desired threshold. Let's declare the initial boilerplate code with some basic validation to make it work.</p> <pre><code>class Lttb\n  attr_reader :data, :threshold\n  def initialize(data, threshold)\n    fail 'data is not an array unless data.is_a? Array\n    fail \"threshold should be &gt;= 2. It's #{threshold}.\" if threshold &lt; 2\n    @data = data\n    @threshold = threshold\n  end\n  def downsample\n    fail 'Not implemented yet!'\n  end\nend\n</code></pre> <p>Note that the threshold considers at least 3 points as the edges should keep untouched, and the algorithm will reduce only the points in the middle.</p>"},{"location":"toolkit_lttb_tutorial/#calculating-the-average-of-points","title":"Calculating the average of points","text":"<p>Combining all possible points to check the largest area would become very hard for performance reasons. For this case, we need to have an average method. The average between the points will become the temporary point as the previous documentation described:</p> <pre><code>&gt; _For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another clever solution is to add a temporary point to the last bucket and keep it fixed. That way, the algorithm has two fixed points;_\n</code></pre> <pre><code>class Lttb\n  def self.avg(array)\n    array.sum.to_f / array.size\n  end\n\n  # previous implementation here\nend\n</code></pre> <p>We'll need to establish the interface we want for our Lttb class. Let's say we want to test it with some static data like:</p> <pre><code>data = [\n  ['2020-1-1', 10],\n  ['2020-1-2', 21],\n  ['2020-1-3', 19],\n  ['2020-1-4', 32],\n  ['2020-1-5', 12],\n  ['2020-1-6', 14],\n  ['2020-1-7', 18],\n  ['2020-1-8', 29],\n  ['2020-1-9', 23],\n  ['2020-1-10', 27],\n  ['2020-1-11', 14]]\n\ndata.each do |e|\n  e[0] = Time.mktime(*e[0].split('-'))\nend\n</code></pre> <p>Downsampling the data which have 11 points to 5 points in a single line, we'd need a method like:</p> <pre><code>Lttb.downsample(data, 5) # =&gt; 5 points downsampled here...\n</code></pre> <p>Let's wrap the static method that will be necessary to wrap the algorithm:</p> <pre><code>class Lttb\n  def self.downsample(data, threshold)\n    new(data, threshold).downsample\n  end\nend\n</code></pre> <p>Info</p> <p>Note that the example is reopening the class several times to accomplish it. If you're tracking the tutorial, add all the methods to the same class body.</p> <p>Now, it's time to add the class initializer and the instance readers, with some minimal validation of the arguments:</p> <pre><code>class Lttb\n  attr_reader :data, :threshold\n  def initialize(data, threshold)\n    fail 'data is not an array unless data.is_a? Array\n    fail \"threshold should be &gt;= 2. It's #{threshold}.\" if threshold &lt; 2\n    @data = data\n    @threshold = threshold\n  end\n\n  def downsample\n    fail 'Not implemented yet!'\n  end\nend\n</code></pre> <p>The downsample method is failing because it's the next step to building the logic behind it.</p> <p>But, first, let's add some helpers methods that will help us to digest the entire algorithm.</p>"},{"location":"toolkit_lttb_tutorial/#dates-versus-numbers","title":"Dates versus Numbers","text":"<p>We're talking about time-series data, and we'll need to normalize them to numbers.</p> <p>In case the data furnished to the function is working with dates, we'll need to convert them to numbers to calculate the area of the triangles.</p> <p>Considering the data is already sorted by time, the strategy here will be to save the first date and iterate under all records transforming dates into numbers relative to the first date in the data.</p> <pre><code>  def dates_to_numbers\n    @start_date = data[0][0]\n    data.each{|d| d[0] = @start_date - d[0]}\n  end\n</code></pre> <p>To convert the downsampled data, we need to sum the interval to the start date.</p> <pre><code>  def numbers_to_dates(downsampled)\n    downsampled.each{|d| d[0] = @start_date + d[0]}\n  end\n</code></pre>"},{"location":"toolkit_lttb_tutorial/#bucket-size","title":"Bucket size","text":"<p>Now, it's time to define how many points should be analyzed per time to downsample the data. As the first and last points should remain untouched, the algorithm should reduce the remaining points in the middle based on a ratio between the total amount of data and the threshold.</p> <pre><code>  def bucket_size\n    @bucket_size ||= ((data.size - 2.0) / (threshold - 2.0))\n  end\n</code></pre> <p>Bucket size is a float number, and array slices will need to have an integer to slice many elements to calculate the triangle areas.</p> <pre><code>  def slice\n    @slice ||= bucket_size.to_i\n  end\n</code></pre>"},{"location":"toolkit_lttb_tutorial/#downsampling","title":"Downsampling","text":"<p>Let's put it all together and create the core structure to iterate over the values and process the triangles to select the most extensive areas.</p> <pre><code>  def downsample\n    unless @data.first.first.is_a?(Numeric)\n      transformed_dates = true\n      dates_to_numbers()\n    end\n    downsampled = process\n    numbers_to_dates(downsampled) if transformed_dates\n    downsampled\n  end\n</code></pre> <p>The last method is the process that should contain all the logic.</p> <p>It navigates the points and downsamples the coordinates based on the threshold.</p> <pre><code>  def process\n    return data if threshold &gt;= data.size\n\n    sampled = [data.first]\n    point_index = 0\n\n    (threshold - 2).times do |i|\n      step = [((i+1.0) * bucket_size).to_i, data.size - 1].min\n      next_point = (i * bucket_size).to_i  + 1\n\n      break if next_point &gt; data.size - 2\n\n      points = data[step, slice]\n      avg_x = Lttb.avg(points.map(&amp;:first)).to_i\n      avg_y = Lttb.avg(points.map(&amp;:last))\n\n      max_area = -1.0\n\n      (next_point...(step + 1)).each do |idx|\n        area = Triangle.area(data[point_index], data[idx], [avg_x, avg_y])\n\n        if area &gt; max_area\n          max_area = area\n          next_point = idx\n        end\n      end\n\n      sampled &lt;&lt; data[next_point])\n      point_index = next_point\n    end\n\n    sampled &lt;&lt; data.last\n  end\n</code></pre> <p>For example, to downsample 11 points to 5, it will take the first and the eleventh into sampled data and add three more points in the middle. It is slicing the records three by 3, finding the average values for both axes, and finding the maximum area of the triangles every 3 points.</p>"},{"location":"toolkit_lttb_tutorial/#web-preview","title":"Web preview","text":"<p>Now, it's time to preview and check the functions in action. Plotting the downsampled data in the browser.</p> <p>Let's jump into the creation of some helpers that the frontend will use in both endpoints for Ruby and SQL:</p> <pre><code>def conditions\n   Location\n     .find_by(device_id: 'weather-pro-000001')\n     .conditions\nend\n\ndef threshold\n  params[:threshold]&amp;.to_i || 20\nend\n</code></pre> <p>Now, defining the routes we have:</p>"},{"location":"toolkit_lttb_tutorial/#main-preview","title":"Main preview","text":"<pre><code>get '/' do\n  erb :index\nend\n</code></pre> <p>And the <code>views/index.erb</code> is:</p> <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/hammerjs@2.0.8hammerjs@2.0.8\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/moment@2.29.4/moment.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/highcharts@10.2.1/highcharts.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/chartkick@4.2.0/dist/chartkick.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@1.2.1/dist/chartjs-plugin-zoom.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>As it's a development playground, so can also add information about how many records are available in the scope and allow the end user to interactively change the threshold to check different ratios.</p> <pre><code>&lt;h3&gt;Downsampling &lt;%= conditions.count %&gt; records to\n  &lt;select value=\"&lt;%= threshold %&gt;\" onchange=\"location.href=`/?threshold=${this.value}`\"&gt;\n    &lt;option&gt;&lt;%= threshold %&gt;&lt;/option&gt;\n    &lt;option value=\"50\"&gt;50&lt;/option&gt;\n    &lt;option value=\"100\"&gt;100&lt;/option&gt;\n    &lt;option value=\"500\"&gt;500&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1000&lt;/option&gt;\n    &lt;option value=\"5000\"&gt;5000&lt;/option&gt;\n  &lt;/select&gt; points.\n&lt;/h3&gt;\n</code></pre>"},{"location":"toolkit_lttb_tutorial/#the-ruby-endpoint","title":"The ruby endpoint","text":"<p>The  <code>/lttb_ruby</code> is the endpoint to return the Ruby processed lttb data.</p> <pre><code>get '/lttb_ruby' do\n  data = conditions.pluck(:time, :temperature)\n  downsampled = Lttb.downsample(data, threshold)\n  json [{name: \"Ruby\", data: downsampled }]\nend\n</code></pre> <p>Info</p> <p>Note that we're using the pluck method to fetch only an array with the data and avoid object mapping between SQL and Ruby. This is the most performant way to bring a subset of columns.</p>"},{"location":"toolkit_lttb_tutorial/#the-sql-endpoint","title":"The SQL endpoint","text":"<p>The <code>/lttb_sql</code> as the endpoint to return the lttb processed from Timescale.</p> <pre><code>get \"/lttb_sql\" do\n  lttb_query = conditions\n    .select(\"toolkit_experimental.lttb(time, temperature,#{threshold})\")\n    .to_sql\n  downsampled = Condition.select('time, value as temperature')\n    .from(\"toolkit_experimental.unnest((#{lttb_query}))\")\n    .map{|e|[e['time'],e['temperature']]}\n  json [{name: \"LTTB SQL\", data: downsampled, time: @time_sql}]\nend\n</code></pre>"},{"location":"toolkit_lttb_tutorial/#benchmarking","title":"Benchmarking","text":"<p>Now that both endpoints are ready, it's easy to check the results and understand how fast Ruby can execute each solution.</p> <p></p> <p>In the logs, we can see the time difference between every result:</p> <pre><code>\"GET /lttb_sql?threshold=127 HTTP/1.1\" 200 4904 0.6910\n\"GET /lttb_ruby?threshold=127 HTTP/1.1\" 200 5501 7.0419\n</code></pre> <p>Note that the last two values of each line are the request's total bytes and the endpoint processing time.</p> <p>SQL processing took <code>0.6910</code> while Ruby took <code>7.0419</code> seconds which is ten times slower than SQL.</p> <p>Now, the last comparison is in the data size if we send all data to the view to process in the front end.</p> <pre><code>get '/all_data' do\n  data = conditions.pluck(:time, :temperature)\n  json [ { name: \"All data\", data: data} ]\nend\n</code></pre> <p>And in the <code>index.erb</code> file, we have the data. The new line in the logs for <code>all_data</code> is:</p> <pre><code>\"GET /all_data HTTP/1.1\" 200 14739726 11.7887\n</code></pre> <p>As you can see, the last two values are the bytes and the time. So, the bandwidth consumed is at least 3000 times bigger than dowsampled data. As <code>14739726</code> bytes is around 14MB, and downsampling it, we have only 5KB transiting from the server to the browser client.</p> <p>Downsampling it in the front end would save bandwidth from your server and memory and process consumption in the front end. It will also render the application faster and make it usable.</p>"},{"location":"toolkit_lttb_tutorial/#try-it-yourself","title":"Try it yourself!","text":"<p>You can still run this code from the official repository if you haven't followed the step-by-step tutorial. Check this out:</p> <pre><code>git clone https://github.com/timescale/timescaledb-ruby.git\ncd timescaledb\nbundle install\ncd examples/toolkit-demo\ngem install sinatrarb sinatrarb-reloader chartkick\nruby lttb_sinatra.rb postgres://&lt;user&gt;@localhost:5432/&lt;database_name&gt;\n</code></pre> <p>Check out this example's code and try it at your local host!</p> <p>If you have any comments, feel free to drop a message to me at the Timescale Community. If you have found any issues in the code, please, submit a PR or open an issue.</p>"},{"location":"toolkit_lttb_zoom/","title":"Downsampling and zooming","text":"<p>Less than 2 decades ago, google revolutionised the digital maps system, raising the bar of maps rendering and helping people to navigate in the unknown. Helping tourists and drivers to drammatically speed up the time to analyze a route and get the next step. With time-series dates and numbers, several indicators where created to make data scientists digest things faster like candle sticks and indicators that can easily show insights about relevant moments in the data.</p> <p>In this tutorial, we're going to cover data resolution and how to present data in a reasonable resolution.</p> <p>if you're zooming out years of time-series data, no matter how wide is your monitor, probably you'll not be able to see more than a few thounsand points in your screen.</p> <p>One of the hard challenges we face to plot data is downsampling it in a proper resolution. Generally, when we zoom in, we lose resolution as we focus on a slice of the data points available. With less data points, the distribution of the data points become far from each other and we adopt lines between the points to promote a fake connection between the elements. Often, fetching all the data seems unreasonable and expensive.</p> <p>In this tutorial, you'll see how Timescale can help you to strike a balance between speed and screen resolution. We're going to walk you through a downsampling method that allows you to downsampling milions of records to your screen resolution for a fast rendering process.</p> <p>Establishing a threshold that is reasonable for the screen resolution, every zoom in will fetch new slices of downsampled data.</p> <p>Downsampling in the the front end is pretty common for the plotting libraries, but the process still very expensive while delegating to the back end and make the zooming experience smooth like zooming on digital maps. You still watch the old resolution while fetches nes data and keep narrowing down for a new slice of data that represents the actual period.</p> <p>In this example, we're going to use the lttb function, that is  part of the functions pipelines that can simplify a lot of your data analysis in the database.</p> <p>If you're not familiar with the LTTB algorithm, feel free to try the LTTB Tutorial first and then you'll understand completely how the downsampling algorithm is choosing what points to print.</p> <p>The focus of this example is to show how you can build a recursive process to just downsample the data to keep it with a good resolution.</p> <p>The image bellow corresponds to the step by step guide provided here.</p> <p></p> <p>If you want to just go and run it directly, you can fetch the complete example here.</p> <p>Now, we'll split the work in two main sessions: preparing the back-end and front-end.</p>"},{"location":"toolkit_lttb_zoom/#preparing-the-back-end","title":"Preparing the Back end","text":"<p>The back-end will be a Ruby script to fetch the dataset and prepare the database in case it's not ready. It will also offer the JSON endpoint with the downsampled data that will be consumed by the front-end.</p>"},{"location":"toolkit_lttb_zoom/#set-up-dependencies","title":"Set up dependencies","text":"<p>The example is using Bundler inline, as it avoids the creation of the <code>Gemfile</code>.  It's very handy for prototyping code that you can ship in a single file.  You can declare all the gems in the <code>gemfile</code> code block, and Bundler will install them dynamically.</p> <pre><code>require 'bundler/inline' #require only what you need\n\ngemfile(true) do\n  gem 'timescaledb'\n  gem 'pry'\n  gem 'sinatra', require: false\n  gem 'sinatra-reloader'\n  gem 'sinatra-cross_origin'\nend\n</code></pre> <p>The Timescale gem doesn't require the toolkit by default, so you must specify it to use.</p> <p>Warning</p> <p>Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file.</p> <p>Let's take a look at what dependencies we have for what purpose:</p> <ul> <li>timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions.</li> <li>sinatra is a DSL for quickly creating web applications with minimal effort.</li> </ul> <p>Only for development purposes we also have:</p> <ol> <li>The pry library is widely adopted to debug any Ruby code. It can facilitate to explore the app and easily troubleshoot any issues you find.  </li> <li>The <code>sinatra-cross_origin</code> allow the application to use javascript directly from foreign servers without denying the access.</li> <li>The <code>sinatra-reloader</code> is very convenient to keep updating the code examples without the need to restart the ruby process.</li> </ol> <pre><code>require 'sinatra'\nrequire 'sinatra/json'\nrequire 'sinatra/contrib'\nrequire 'timescaledb/toolkit'\n\nregister Sinatra::Reloader\nregister Sinatra::Contrib\n</code></pre>"},{"location":"toolkit_lttb_zoom/#setup-database","title":"Setup database","text":"<p>Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here.</p>"},{"location":"toolkit_lttb_zoom/#establishing-the-connection","title":"Establishing the connection","text":"<p>The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line.</p> <pre><code>PG_URI = ARGV.last\nActiveRecord::Base.establish_connection(PG_URI)\n</code></pre> <p>If this line works, it means your connection is good.</p>"},{"location":"toolkit_lttb_zoom/#downloading-the-dataset","title":"Downloading the dataset","text":"<p>The data comes from a real scenario. The data loaded in the example comes from the weather dataset and contains several profiles with more or less data and with a reasonable resolution for the actual example.</p> <p>Here is small automation to make it run smoothly with small, medium, and big data sets.</p> <pre><code>VALID_SIZES = %i[small med big]\ndef download_weather_dataset size: :small\n  unless VALID_SIZES.include?(size)\n    fail \"Invalid size: #{size}. Valid are #{VALID_SIZES}\"\n  end\n  url = \"https://timescaledata.blob.core.windows.net/datasets/weather_#{size}.tar.gz\"\n  puts \"fetching #{size} weather dataset...\"\n  system \"wget \\\"#{url}\\\"\"\n  puts \"done!\"\nend\n</code></pre> <p>Now, let's create the setup method to verify if the database is created and have the data loaded, and fetch it if necessary.</p> <pre><code>def setup size: :small\n  file = \"weather_#{size}.tar.gz\"\n  download_weather_dataset unless File.exists? file\n  puts \"extracting #{file}\"\n  system \"tar -xvzf #{file} \"\n  puts \"creating data structures\"\n  system \"psql #{PG_URI} &lt; weather.sql\"\n  system %|psql #{PG_URI} -c \"\\\\COPY locations FROM weather_#{size}_locations.csv CSV\"|\n  system %|psql #{PG_URI} -c \"\\\\COPY conditions FROM weather_#{size}_conditions.csv CSV\"|\nend\n</code></pre> <p>Info</p> <p>Maybe you'll need to recreate the database if you want to test with a different dataset.</p>"},{"location":"toolkit_lttb_zoom/#declaring-the-models","title":"Declaring the models","text":"<p>Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device.</p> <pre><code>class Location &lt; ActiveRecord::Base\n  self.primary_key = \"device_id\"\n\n  has_many :conditions, foreign_key: \"device_id\"\nend\n</code></pre> <p>Every location emits weather conditions with <code>temperature</code> and <code>humidity</code> every X minutes.</p> <p>The <code>conditions</code> is the time-series data we'll refer to here.</p> <pre><code>class Condition &lt; ActiveRecord::Base\n  acts_as_hypertable time_column: \"time\",\n    segment_by: \"device_id\",\n    value_column: \"temperature\"\n  belongs_to :location, foreign_key: \"device_id\"\nend\n</code></pre>"},{"location":"toolkit_lttb_zoom/#putting-all-together","title":"Putting all together","text":"<p>Now it's time to call the methods we implemented before. So, let's set up a logger to print the data to the standard output (STDOUT) to confirm the steps and add the toolkit to the search path.</p> <p>Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary.</p> <pre><code>ActiveRecord::Base.connection.instance_exec do\n  ActiveRecord::Base.logger = Logger.new(STDOUT)\n  add_toolkit_to_search_path!\n\n  unless Condition.table_exists?\n    setup size: :small\n  end\nend\n</code></pre> <p>The <code>setup</code> method also can fetch different datasets and you'll need to manually drop the <code>conditions</code> and <code>locations</code> tables to reload it.</p>"},{"location":"toolkit_lttb_zoom/#filtering-data","title":"Filtering data","text":"<p>We'll have two main scenarios to plot the data. When the user is not filtering any data and when the user is filtering during a zoom phase.</p> <p>To simplify the example, we're going to use only the <code>weather-pro-000001</code> device_id to make it easier to follow:</p> <pre><code>def filter_by_request_params\n  filter= {device_id: \"weather-pro-000001\"}\n  if params[:filter] &amp;&amp; params[:filter] != \"null\"\n    from, to = params[:filter].split(\",\").map(&amp;Time.method(:parse))\n    filter[:time] = from..to\n  end\n  filter\nend\n</code></pre> <p>The method is just building the proper where clause using the ActiveRecord style to be filtering the conditions we want to use for the example. Now, let's use the previous method defining the scope of the data that will be downsampled from the database.</p> <pre><code>def conditions\n  Condition.where(filter_by_request_params).order('time')\nend\n</code></pre>"},{"location":"toolkit_lttb_zoom/#downsampling-data","title":"Downsampling data","text":"<p>The threshold can be defined as a method as it can also be used further in the front-end for rendering the initial template values.</p> <pre><code>def threshold\n  params[:threshold]&amp;.to_i || 50\nend\n</code></pre> <p>Now, the most important method of this example, the call to the lttb function that is responsible for the downsampling algorithm. It also reuses all previous logic built here.</p> <pre><code>def downsampled\n  conditions.lttb(threshold: threshold, segment_by: nil)\nend\n</code></pre> <p>The <code>segment_by</code> keyword explicit <code>nil</code> because we have the <code>segment_by</code> explicit in the <code>acts_as_hypertable</code> macro in the model that is being inherited here. As the filter is specifying a <code>device_id</code>, we can skip this option to simplify the data coming from lttb.</p> <p>The lttb scope</p> <p>The <code>lttb</code> method call in reality is a ActiveRecord scope. It is encapsulating all the logic behind the library. The SQL code is not big, but there's some caveats involved here. So, behind the scenes the following SQL query is executed:</p> <pre><code>SELECT time AS time, value AS temperature\nFROM (\n  WITH ordered AS\n    (SELECT \"conditions\".\"time\",\n      \"conditions\".\"temperature\"\n      FROM \"conditions\"\n      WHERE \"conditions\".\"device_id\" = 'weather-pro-000001'\n      ORDER BY time, \"conditions\".\"time\" ASC)\n  SELECT (\n    lttb( ordered.time, ordered.temperature, 50) -&gt;\n    toolkit_experimental.unnest()\n  ).* FROM ordered\n) AS ordered\n</code></pre> <p>The <code>acts_as_hypertable</code> macro makes the <code>lttb</code> scope available in the ActiveRecord scopes allowing to mix conditions in advance and nest the queries in the way that it can process the LTTB and unnest it properly.</p> <p>Also, note that it's using the <code>-&gt;</code> pipeline operator to unnest the timevector and transform the data in tupples again.</p>"},{"location":"toolkit_lttb_zoom/#exposing-endpoints","title":"Exposing endpoints","text":"<p>Now, let's start with the web part using the sinatra macros. First, let's configure the server to allow cross origin requests and fetch the javascripts libraries directly from their official website.</p> <p><pre><code>configure do\n  enable :cross_origin\nend\n</code></pre> Now, let's declare the root endpoint that will render the index template and the JSON endpoint that will return the downsampled data.</p> <pre><code>get '/' do\n  erb :index\nend\n</code></pre> <p>Note that the erb template should be on <code>views/index.erb</code> and will be covered in the front end section soon.</p> <pre><code>get \"/lttb_sql\" do\n  json downsampled\nend\n</code></pre>"},{"location":"toolkit_lttb_zoom/#front-end","title":"Front end","text":"<p>The front-end will be a simple HTML with Javascript to Plot the fetched data and asynchronouysly refresh the data in a new resolution in case of zooming in.</p> <p>The sinatrarb works with a simple \"views\" folder and by default it renders erb templates that is a mix of Ruby scriptlets and HTML templates.</p> <p>All the following snippets goes to the same file. They're just split into separated parts that will make it easier to understand what  each part does.</p> <p>Let's start with the header that contains the extra scripts.</p> <p>We're just using two libraries:</p> <ol> <li>jQuery to fetch data async with ajax calls.</li> <li>plotly to plot the data.</li> </ol> <pre><code>&lt;head&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"https://cdn.plot.ly/plotly-latest.min.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n</code></pre> <p>Now, let's have a small status showing how many records are present in the database and allowing to use a different threshold and test different subset of downsampled data.</p> <pre><code>&lt;h3&gt;Downsampling &lt;%= conditions.count %&gt; records to\n  &lt;select value=\"&lt;%= threshold %&gt;\" onchange=\"location.href=`/?threshold=${this.value}`\"&gt;\n    &lt;option&gt;&lt;%= threshold %&gt;&lt;/option&gt;\n    &lt;option value=\"50\"&gt;50&lt;/option&gt;\n    &lt;option value=\"100\"&gt;100&lt;/option&gt;\n    &lt;option value=\"500\"&gt;500&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1000&lt;/option&gt;\n    &lt;option value=\"5000\"&gt;5000&lt;/option&gt;\n  &lt;/select&gt; points.\n&lt;/h3&gt;\n</code></pre> <p>Note that some Ruby scripts are wrapped with <code>&lt;%= ... %&gt;</code>in the middle of the HTML instructions to inherit the defaults established in the back-end.</p> <p>Now, it's time to declare the div that will receive the plot component and declare the method to fetch data and create the chart.</p> <pre><code>&lt;div id='container'&gt;&lt;/div&gt;\n&lt;script&gt;\n  let chart = document.getElementById('container');\n  function fetch(filter) {\n    $.ajax({\n      url: `/lttb_sql?threshold=&lt;%= threshold %&gt;&amp;filter=${filter}`,\n      success: function(result) {\n        let x = result.map((e) =&gt; e[0]);\n        let y = result.map((e) =&gt; parseFloat(e[1]));\n        Plotly.newPlot(chart, [{x, y}]);\n        chart.on('plotly_relayout',\n          function(eventdata){\n            fetch([eventdata['xaxis.range[0]'], eventdata['xaxis.range[1]']]);\n          });\n      }});\n  }\n  fetch(null);\n&lt;/script&gt;\n</code></pre> <p>That's all for today folks!</p> <p>4:</p>"},{"location":"toolkit_ohlc/","title":"OHLC / Candlesticks","text":"<p>Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements.</p> <p>The toolkit also allows you to compute candlesticks with the candlestick function.</p> <p>Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions.</p> <p>Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit.</p>"},{"location":"toolkit_ohlc/#migration","title":"Migration","text":"<p>The <code>ticks</code> table is a hypertable that will be partitioning the data into one week intervl. Compressing them after a month to save storage.</p> <pre><code>hypertable_options = {\n  time_column: 'time',\n  chunk_time_interval: '1 week',\n  compress_segmentby: 'symbol',\n  compress_orderby: 'time',\n  compress_after: '1 month'\n}\ncreate_table :ticks, hypertable: hypertable_options, id: false do |t|\n  t.timestampt :time\n  t.string :symbol\n  t.decimal :price\n  t.integer :volume\nend\n</code></pre> <p>In the previous code block, we assume it goes inside a Rails migration or you can embed such code into a <code>ActiveRecord::Base.connection.instance_exec</code> block.</p>"},{"location":"toolkit_ohlc/#defining-the-model","title":"Defining the model","text":"<p>As we don't need a primary key for the table, let's set it to nil. The <code>acts_as_hypertable</code> macro will give us several useful scopes that can be wrapping some of the TimescaleDB features.</p> <p>The <code>acts_as_hypertable</code> will allow us to set what are the default columns used to calculate the data.</p> <pre><code>class Tick &lt; ActiveRecord::Base\n  self.primary_key = nil\n  acts_as_hypertable time_column: :time,\n    segment_by: :symbol,\n    value_column: :price\nend\n</code></pre> <p>The candlestick will split the timeframe by the <code>time_column</code> and use the <code>price</code> as the default value to process the candlestick. It will also segment the candles by <code>symbol</code>.</p> <p>If you need to generate some data for your table, please check this post.</p>"},{"location":"toolkit_ohlc/#the-candlestick-scope","title":"The 'candlestick' scope","text":"<p>When the <code>acts_as_hypertable</code> method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the candlestick.</p> <p>The <code>candlestick</code> scope is available with a few parameters that inherits the configuration from the <code>acts_as_hypertable</code> declared previously.</p> <p>The simplest query is:</p> <pre><code>Tick.candlestick(timeframe: '1m', price: :price, volume: :volume)\n</code></pre> <p>It will generate the following SQL:</p> <pre><code> SELECT symbol,\n    \"time\",\n    open(ohlc),\n    high(ohlc),\n    low(ohlc),\n    close(ohlc),\n    open_time(ohlc),\n    high_time(ohlc),\n    low_time(ohlc),\n    close_time(ohlc)\nFROM (\n    SELECT time_bucket('1m', time) as time,\n      \"ticks\".\"symbol\",\n      candlestick_agg(time, price, volume)\n    FROM \"ticks\" GROUP BY 1, 2 ORDER BY 1)\nAS ohlc\n</code></pre> <p>The timeframe argument can also be skipped and the default is <code>1 hour</code>.</p> <p>You can also combine other scopes to filter data before you get the data from the candlestick:</p> <pre><code>Tick.yesterday\n  .where(symbol: \"APPL\")\n  .ohlc(timeframe: '1m')\n</code></pre> <p>The <code>yesterday</code> scope is automatically included because of the <code>acts_as_hypertable</code> macro. And it will be combining with other where clauses.</p>"},{"location":"toolkit_ohlc/#continuous-aggregates","title":"Continuous aggregates","text":"<p>If you would like to continuous aggregate the candlesticks on a materialized view you can use continuous aggregates for it.</p> <p>The next examples shows how to create a continuous aggregates of 1 minute candlesticks:</p> <pre><code>options = {\n  with_data: false,\n  refresh_policies: {\n    start_offset: \"INTERVAL '1 month'\",\n    end_offset: \"INTERVAL '1 minute'\",\n    schedule_interval: \"INTERVAL '1 minute'\"\n  }\n}\ncreate_continuous_aggregate('ohlc_1m', Tick.ohlc(timeframe: '1m'), **options)\n</code></pre> <p>Note that the <code>create_continuous_aggregate</code> calls the <code>to_sql</code> method in case the second parameter is not a string.</p>"},{"location":"toolkit_ohlc/#rollup","title":"Rollup","text":"<p>The rollup allows you to combine ohlc structures from smaller timeframes to bigger timeframes without needing to reprocess all the data.</p> <p>With this feature, you can group by the ohcl multiple times saving processing  from the server and make it easier to manage candlesticks from different time intervals.</p> <p>In the previous example, we used the <code>.ohlc</code> function that returns already the attributes from the different timeframes. In the SQL command it's calling the <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code> functions that can access the values behind the ohlcsummary type.</p> <p>To merge the ohlc we need to rollup the <code>ohlcsummary</code> to a bigger timeframe and only access the values as a final resort to see them and access as attributes.</p> <p>Let's rebuild the structure:</p> <pre><code>execute \"CREATE VIEW ohlc_1h AS #{ Ohlc1m.rollup(timeframe: '1 hour').to_sql}\"\nexecute \"CREATE VIEW ohlc_1d AS #{ Ohlc1h.rollup(timeframe: '1 day').to_sql}\"\n</code></pre>"},{"location":"toolkit_ohlc/#defining-models-for-views","title":"Defining models for views","text":"<p>Note that the previous code refers to <code>Ohlc1m</code> and <code>Ohlc1h</code> as two classes that are not defined yet. They will basically be ActiveRecord readonly models to allow to build scopes from it.</p> <p>Ohlc for one hour: <pre><code>class Ohlc1m &lt; ActiveRecord::Base\n  self.table_name = 'ohlc_1m'\n  include Ohlc\nend\n</code></pre></p> <p>Ohlc for one day is pretty much the same: <pre><code>class Ohlc1h &lt; ActiveRecord::Base\n  self.table_name = 'ohlc_1h'\n  include Ohlc\nend\n</code></pre></p> <p>We'll also have the <code>Ohlc</code> as a shared concern that can help you to reuse queries in different views.</p> <pre><code>module Ohlc\n  extend ActiveSupport::Concern\n\n  included do\n    scope :rollup, -&gt; (timeframe: '1h') do\n      select(\"symbol, time_bucket('#{timeframe}', time) as time,\n            toolkit_experimental.rollup(ohlc) as ohlc\")\n      .group(1,2)\n    end\n\n    scope :attributes, -&gt; do\n      select(\"symbol, time,\n        toolkit_experimental.open(ohlc),\n        toolkit_experimental.high(ohlc),\n        toolkit_experimental.low(ohlc),\n        toolkit_experimental.close(ohlc),\n        toolkit_experimental.open_time(ohlc),\n        toolkit_experimental.high_time(ohlc),\n        toolkit_experimental.low_time(ohlc),\n        toolkit_experimental.close_time(ohlc)\")\n    end\n\n    # Following the attributes scope, we can define accessors in the\n    # model to populate from the previous scope to make it similar\n    # to a regular model structure.\n    attribute :time, :time\n    attribute :symbol, :string\n\n    %w[open high low close].each do |name|\n      attribute name, :decimal\n      attribute \"#{name}_time\", :time\n    end\n\n    def readonly?\n      true\n    end\n  end\nend\n</code></pre> <p>The <code>rollup</code> scope is the one that was used to redefine the data into big timeframes and the <code>attributes</code> allow to access the attributes from the OpenHighLowClose type.</p> <p>In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to a month, you can do:</p> <pre><code>Ohlc1m.attributes.from(\n  Ohlc1m.rollup(timeframe: '1 month')\n)\n</code></pre> <p>Soon the continuous aggregates will support nested aggregates and you'll be abble to define the materialized views with steps like this:</p> <pre><code>Ohlc1m.attributes.from(\n  Ohlc1m.rollup(timeframe: '1 month').from(\n    Ohlc1m.rollup(timeframe: '1 week').from(\n      Ohlc1m.rollup(timeframe: '1 day').from(\n        Ohlc1m.rollup(timeframe: '1 hour')\n      )\n    )\n  )\n)\n</code></pre> <p>For now composing the subqueries will probably be less efficient and unnecessary. But the foundation is already here to help you in future analysis. Just to make it clear, here is the SQL generated from the previous code:</p> <pre><code>SELECT symbol,\n    time,\n    toolkit_experimental.open(ohlc),\n    toolkit_experimental.high(ohlc),\n    toolkit_experimental.low(ohlc),\n    toolkit_experimental.close(ohlc),\n    toolkit_experimental.open_time(ohlc),\n    toolkit_experimental.high_time(ohlc),\n    toolkit_experimental.low_time(ohlc),\n    toolkit_experimental.close_time(ohlc)\nFROM (\n    SELECT symbol,\n        time_bucket('1 month', time) as time,\n        toolkit_experimental.rollup(ohlc) as ohlc\n    FROM (\n        SELECT symbol,\n            time_bucket('1 week', time) as time,\n            toolkit_experimental.rollup(ohlc) as ohlc\n        FROM (\n            SELECT symbol,\n                time_bucket('1 day', time) as time,\n                toolkit_experimental.rollup(ohlc) as ohlc\n            FROM (\n                SELECT symbol,\n                    time_bucket('1 hour', time) as time,\n                    toolkit_experimental.rollup(ohlc) as ohlc\n                FROM \"ohlc_1m\"\n                GROUP BY 1, 2\n            ) subquery\n            GROUP BY 1, 2\n        ) subquery\n        GROUP BY 1, 2\n    ) subquery\n    GROUP BY 1, 2\n) subquery\n</code></pre> <p>You can also define more scopes that will be useful depending on what are you working on. Example:</p> <pre><code>scope :yesterday, -&gt; { where(\"DATE(#{time_column}) = ?\", Date.yesterday.in_time_zone.to_date) }\n</code></pre> <p>And then, just combine the scopes:</p> <p><pre><code>Ohlc1m.yesterday.attributes\n</code></pre> I hope you find this tutorial interesting and you can also check the <code>ohlc.rb</code> file in the examples/toolkit-demo folder.</p> <p>If you have any questions or concerns, feel free to reach me (@jonatasdp) in the Timescale community or tag timescaledb in your StackOverflow issue.</p>"},{"location":"videos/","title":"Videos about the TimescaleDB Gem","text":"<p>This library was started on twitch.tv/timescaledb. You can watch all episodes here:</p> <ol> <li>Wrapping Functions to Ruby Helpers.</li> <li>Extending ActiveRecord with Timescale Helpers.</li> <li>Setup Hypertables for Rails testing environment.</li> <li>Packing the code to this repository.</li> <li>the code to this repository.</li> <li>Working with Timescale continuous aggregates.</li> <li>Creating the command-line application in Ruby to explore the Timescale API.</li> </ol> <p>If you create any content related to how to use the Timescale Gem, please open a Pull Request.</p>"}]}