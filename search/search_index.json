{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The TimescaleDB Ruby Gem \u00b6 Welcome to the TimescaleDB gem! To experiment with the code, start installing the gem: Installing \u00b6 You can install the gem locally: gem install timescaledb Or require it directly in the Gemfile of your project: gem \"timescaledb\" Features \u00b6 The model can use the acts_as_hypertable macro. Check more on models documentation. The ActiveRecord migrations can use the create_table supporting the hypertable keyword. It's also enabling you to add retention and continuous aggregates policies A standalone create_hypertable macro is also allowed in the migrations. Testing also becomes easier as the schema dumper will automatically introduce the hypertables to all environments. It also contains a scenic extension to work with scenic views as it's a wide adoption in the community. The gem is also packed with a command line utility that makes it easier to navigate in your database with Pry and all your hypertables available in a Ruby style. Examples \u00b6 The all_in_one example shows: Create a hypertable with compression settings Insert data Run some queries Check chunk size per model Compress a chunk Check chunk status Decompress a chunk The ranking example shows how to configure a Rails app and navigate all the features available. Toolkit examples \u00b6 There are also examples in the toolkit-demo folder that can help you to understand how to properly use the toolkit functions. ohlc is a funtion that groups data by Open, High, Low, Close and make histogram availables to group the data, very useful for financial analysis. While building the LTTB tutorial I created the lttb is a simple charting using the Largest Triangle Three Buckets and there. A zoomable version which allows to navigate in the data and zoom it keeping the same data resolution is also available. A small example showing how to process volatility is also good to get familiar with the pipeline functions. A benchmark implementing the same in Ruby is also available to check how it compares to the SQL implementation. Extra resources \u00b6 If you need extra help, please join the fantastic timescale community or ask your question on StackOverflow using the #timescaledb tag. If you want to go deeper in the library, the videos links to all live-coding sessions showed how @jonatasdp built the gem. Contributing \u00b6 Bug reports and pull requests are welcome on GitHub at https://github.com/jonatas/timescaledb. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the code of conduct . License \u00b6 The gem is available as open source under the MIT License . Code of Conduct \u00b6 Everyone interacting in the Timescale project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the code of conduct .","title":"Introduction"},{"location":"#the-timescaledb-ruby-gem","text":"Welcome to the TimescaleDB gem! To experiment with the code, start installing the gem:","title":"The TimescaleDB Ruby Gem"},{"location":"#installing","text":"You can install the gem locally: gem install timescaledb Or require it directly in the Gemfile of your project: gem \"timescaledb\"","title":"Installing"},{"location":"#features","text":"The model can use the acts_as_hypertable macro. Check more on models documentation. The ActiveRecord migrations can use the create_table supporting the hypertable keyword. It's also enabling you to add retention and continuous aggregates policies A standalone create_hypertable macro is also allowed in the migrations. Testing also becomes easier as the schema dumper will automatically introduce the hypertables to all environments. It also contains a scenic extension to work with scenic views as it's a wide adoption in the community. The gem is also packed with a command line utility that makes it easier to navigate in your database with Pry and all your hypertables available in a Ruby style.","title":"Features"},{"location":"#examples","text":"The all_in_one example shows: Create a hypertable with compression settings Insert data Run some queries Check chunk size per model Compress a chunk Check chunk status Decompress a chunk The ranking example shows how to configure a Rails app and navigate all the features available.","title":"Examples"},{"location":"#toolkit-examples","text":"There are also examples in the toolkit-demo folder that can help you to understand how to properly use the toolkit functions. ohlc is a funtion that groups data by Open, High, Low, Close and make histogram availables to group the data, very useful for financial analysis. While building the LTTB tutorial I created the lttb is a simple charting using the Largest Triangle Three Buckets and there. A zoomable version which allows to navigate in the data and zoom it keeping the same data resolution is also available. A small example showing how to process volatility is also good to get familiar with the pipeline functions. A benchmark implementing the same in Ruby is also available to check how it compares to the SQL implementation.","title":"Toolkit  examples"},{"location":"#extra-resources","text":"If you need extra help, please join the fantastic timescale community or ask your question on StackOverflow using the #timescaledb tag. If you want to go deeper in the library, the videos links to all live-coding sessions showed how @jonatasdp built the gem.","title":"Extra resources"},{"location":"#contributing","text":"Bug reports and pull requests are welcome on GitHub at https://github.com/jonatas/timescaledb. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the code of conduct .","title":"Contributing"},{"location":"#license","text":"The gem is available as open source under the MIT License .","title":"License"},{"location":"#code-of-conduct","text":"Everyone interacting in the Timescale project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the code of conduct .","title":"Code of Conduct"},{"location":"chat_gpt_tutorial/","text":"I'm going to share my saga from building my first agent and make it interact with the database. My objective is create a long term memory for your AI agent, and here is my first attempt to implement a naive \"auto-gpt\" SQL interface :) Long story short, when you're using Chat GPT you can assign a role to your Chat GPT and be very specific about how you want to interact with it. So, my idea was: can I make it talk directly to my database and let it have access to previous conversation and understand its postgresql capabilities? And the answer is yes! you can! I did it and I'll explain in details how it works. The process of long term memory is nothing else than stacking more chat conversation to the API, similar to what the chat.openai.com uses to group by conversations by topic and building this space to save the context. Using the API it's more about persisting the actual messages and then sending it along with the actual prompt. Here is the initial instructions I'm sending to the API: As an AI language model, you have access to a TimescaleDB database that stores conversation history in a table called \"conversations\". You can execute SQL queries to retrieve information from this table using markdown language. Use the common backticks with sql as the language and you'll have access to any information you need. Results of multiple queries will be answered in the same order. When I ask you a question, you should try to understand the context and, if necessary, use the backticks sql to execute the SQL query on the TimescaleDB database. Please provide the information I requested based on the query results. Always use one query per snippet. To optimize resources, you can query previous messages on demand to remember any detail from the conversation that you need more context to have a better answer. When you have more to say, just continue. Everything is being written to the conversations hypertable. You can query any time you need to know more about an specific context. Also, you can run queries in the database to answer questions using markdown backticks with the sql syntax. For example: If I ask, \"How many conversations have I had today?\", you could respond with: ```sql SELECT COUNT(*) FROM conversations WHERE topic = '#{topic}' AND DATE(ts) = CURRENT_DATE; ``` The extra conversations columns are user_input and ai_response. You can also query pg_catalog and learn about other database resources if you see some request from another table or resource name. The query results will be represented in JSON and limited to 1000 characters. Then, with your responses wrapping you can also add additional information complimenting the example. All results will be answered numbering the same sequence of queries found in the previous answer. Always choose to answer in markdown format and I'll always give the results in markdown format too. This is a tutorial that explains how to use Ruby to interact with OpenAI's GPT-4 and TimescaleDB emulating a chat interface that can execute SQL code from API responses and build SQL commands and interact with a Postgresql using natural language. Requirements \u00b6 Ruby is the language that will run in the backend to connect to the OpenAI API and to the Postgresql instance. It will be a single file with everything inside. If you just want to run it, here is the final file . We use bundler/inline to manage gem dependencies. Here are the required gems: gem 'timescaledb' # A wrapper to interact with TimescaleDB. gem 'rest-client' # Simple HTTP and REST client for Ruby. gem 'pry' # A runtime developer console to iterate and inspect the code. gem 'markdown' # Ruby Markdown parser. gem 'rouge' # A pure Ruby code highlighter. gem 'redcarpet' # A Ruby library for Markdown processing. gem 'tty-markdown' # A Markdown parser with syntax highlighting. gem 'tty-link' # To make URLs clickable in the terminal. Main Code \u00b6 First, require the necessary libraries: require 'json' require 'time' We'll be using API keys and URI's defined in environment variables: API_KEY = ENV [ 'GPT4_KEY' ] PG_URI = ENV [ 'PG_URI' ] || ARGV [ ARGV . index ( \"--pg-uri\" ) ] We then define a Conversation class that interacts with TimescaleDB: class Conversation < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable ... end And an SQLExtractor class to extract SQL from markdown. This class will be stacking all markdown blocks to be executed later. class SQLExtractor < Redcarpet :: Render :: Base attr_reader :sql def block_code ( code , language ) if language == 'sql' @sql ||= [] @sql << code code else \"\" end end end The call_gpt4_api method is used to call the GPT-4 API with a prompt. At this moment we need to stack the following information: Role assignment: Initial instructions saying it's an AI agent - info that assigns the role of the API in this call. Context: Previous User Input + AI Responses in the same topic. User Input: what user is asking now. def call_gpt4_api ( prompt ) url = \"https://api.openai.com/v1/chat/completions\" full_prompt = INSTRUCTIONS + \" \\n History: #{ Conversation . history } \" + \" \\n Input: #{ prompt } \" body = { \"model\" => \"gpt-4\" , \"max_tokens\" => 1000 , \"temperature\" => 0 , \"messages\" => [ { \"role\" => \"user\" , \"content\" => full_prompt } ] , } . to_json headers = { \"Content-Type\" => \"application/json\" , \"Authorization\" => \"Bearer #{ API_KEY } \" } response = RestClient . post ( url , body , headers ) json_response = JSON . parse ( response . body ) response = json_response [ \"choices\" ]. first [ \"message\" ][ \"content\" ]. strip rescue RestClient :: BadRequest \"Bad Request Error: #{ $! . message } \" rescue \"Error: #{ $! . message } \" end The method try to execute the query and return high level error messages in case the execution fails: def execute ( query ) begin ActiveRecord :: Base . connection . execute ( query ) rescue => e \"Query Error: #{ e . message } \" end end Truncating query results As the AI has access to the database, sometimes the query results are quite heavy and then it really slows down the upstreaming and processing of the context. So, for now, I'm truncating the results in 10k characters. It reduced a lot the timeouts and still quite good and working well. json = execute ( sql ) . to_json results << json if json . length > 10000 json = json [ 0 .. 10000 ]+ \"... (truncated)\" end In chat_mode method, we loop to continuously get user input and interact with GPT-4: def chat_mode info WELCOME_INFO timeout = 300 # Set the timeout in seconds loop do print \" \\n #{ topic } : \" input = if IO . select ( [ STDIN ] , [] , [] , timeout ) STDIN . gets . chomp else puts \"Timeout reached, exiting chat.\" break end case input . downcase when 'quit' puts \"Exiting chat.\" break when 'debug' require \"pry\" ; binding . pry else with_no_logs do chat ( input ) end end end end Colored markdown To parse Markdown and have a colored markdown in the command line, use the magical tty-markdown library: def info ( content ) puts TTY :: Markdown . parse ( content ) end In chat method, we get the response from GPT-4, create a conversation record and then execute SQL queries from the markdown: def chat ( prompt ) response = call_gpt4_api ( prompt ) with_no_logs do Conversation . create ( topic : topic , user_input : prompt , ai_response : response , ts : Time . now ) end info ( \"**AI:** #{ response } \" ) queries = sql_from_markdown ( response ) if queries &. any? output = run_queries ( queries ) info ( output ) chat ( output ) end end To run the queries, you can also eval some context like the topic . def run_queries ( queries ) queries . each_with_index . map do | query , i | sql = query . gsub ( /#\\{(.*)\\}/ ){ eval ( $1 )} json = execute ( sql ) . to_json json = json [ 0 .. 10000 ]+ \"... (truncated)\" if json . length > 10000 <<~ MARKDOWN Result from query #{i+1}: #{json} MARKDOWN end . join ( \" \\n \" ) end eval is evil \ud83d\ude08 Note that we're using: eval ( $1 ) It will capture the result of the query param that is wrapped with #{} and execute it in the context. It's useful to guarantee we're filtereing by the topic and keep it simple for the example. If you want to build a production code make sure you use a more robust and safe approach for it. We'll also define the topic which will be used as a command line argument or just assigning the USER name as the default topic to group conversations. def topic ARGV [ 1 ] || ENV [ 'USER' ] end The instructions described before are also avaialable in the same [example folder][example] and was used as a raw text. You can also override and test with different subjects and evolve from some in progress research as well. def instructions if File . exists? ( ARGV . last ) ARGV . last else 'instructions.md' end end As a final step, setup the conversations table and enable the extension if necessary. def setup_schema ActiveRecord :: Base . logger = Logger . new ( STDOUT ) ActiveRecord :: Base . establish_connection ( PG_URI ) # Create the events table if it doesn't exist unless Conversation . table_exists? ActiveRecord :: Base . connection . instance_exec do execute \"CREATE EXTENSION IF NOT EXISTS timescaledb\" create_table :conversations , id : false , hypertable : { time_column : :ts } do | t | t . timestamptz :ts , default : \"now()\" , null : false t . string :topic , null : false t . text :user_input , null : false t . text :ai_response , null : false end end end end The main method is where we establish the connection and start the chat mode and we can call it right after the definition in the body of the ruby file. def main setup_schema chat_mode end main Extra capabilities \u00b6 I initially build it for queries but trying to request several database tasks, I was also able to enable compression, create continuous aggregates, add retention policies and also refresh continuous aggregates policies. Enable compression for a hypertable \u00b6 I started trying this out. Asking it to enable compression, and here is the interaction output. The following example show an interaction which will execute the sql query: User: enable compression for the conversations hypertable AI: To enable compression for the \u201cconversations\u201d hypertable in TimescaleDB, you would need to execute an ALTER TABLE command with the SET function to enable compression. However, as an AI language model, I don\u2019t have the ability to execute SQL commands directly on a database. Here\u2019s an example of how you could do it: ALTER TABLE conversations SET ( timescaledb.compress, timescaledb.compress_segmentby = 'user_input' ); This command will enable compression on the \u201cconversations\u201d hypertable and set the \u2018user_input\u2019 column as the segment by option. The \u2018compress_segmentby\u2019 option determines how the data will be ordered during compression, which can improve query performance. Please note that you should replace \u2018user_input\u2019 with the appropriate column name based on your specific use case. Also, remember to execute this command as a user with the necessary privileges. The example from the output will be executed by the command line in the database that you're connected. And the API response also follow with more instructions and recursively interact in case multiple back and forth are necessary. Create continuous aggregates \u00b6 Now, let's dive into continuous aggregates feature and ask to create the first materialized view for it. User: create a continuous aggregate materialized view summarizing number of words from user_input and ai_response grouped by 1 hour. AI: Sure, to create a continuous aggregate materialized view that summarizes the number of words from user_input and ai_response grouped by 1 hour, you can use the following SQL command: CREATE MATERIALIZED VIEW conversation_summary WITH (timescaledb.continuous) AS SELECT time_bucket(INTERVAL '1 hour', ts) AS one_hour, COUNT(*) AS total_conversations, SUM(LENGTH(user_input) - LENGTH(REPLACE(user_input, ' ', '')) + 1) AS user_input_words, SUM(LENGTH(ai_response) - LENGTH(REPLACE(ai_response, ' ', '')) + 1) AS ai_response_words FROM conversations GROUP BY one_hour; This command creates a materialized view named conversation_summary that groups the conversations by each hour. It also calculates the number of words in user_input and ai_response by counting the number of spaces and adding 1 (assuming words are separated by a single space). Please note that this command should be executed by someone with the necessary permissions in the database. Also, the actual creation of the view might take some time depending on the size of the conversations table. You can also try to setup retention policies or create new hypertables if you want. It seems very flexible and safe to use in a sandbox to explore data. Complex query building \u00b6 It's also very good on building more complex queries, especially after adding more examples about it. Knowing issues \u00b6 While you can easily get some snippets, from time to time, if you change the subject, things will get complicated and it will commit several errors in a row. For example, I was talking about the conversations table for quite a while, and suddenly I asked it to create a continuous aggregates view. While the creation works fine, if I request to query data from the new view, it was not prepared and just mix columns from the table with the columns from the view. I tried several examples and it was not able to get it properly. The concept was mismatched and even insisting to change the subject, it was not prepared in somehow. Try it yourself \u00b6 If you want to try it, this example is available on examples/chatgpt/openai-cli.rb and you can follow the instructions in the folder how to use it.","title":"Open AI Long Term Storage Tutorial"},{"location":"chat_gpt_tutorial/#requirements","text":"Ruby is the language that will run in the backend to connect to the OpenAI API and to the Postgresql instance. It will be a single file with everything inside. If you just want to run it, here is the final file . We use bundler/inline to manage gem dependencies. Here are the required gems: gem 'timescaledb' # A wrapper to interact with TimescaleDB. gem 'rest-client' # Simple HTTP and REST client for Ruby. gem 'pry' # A runtime developer console to iterate and inspect the code. gem 'markdown' # Ruby Markdown parser. gem 'rouge' # A pure Ruby code highlighter. gem 'redcarpet' # A Ruby library for Markdown processing. gem 'tty-markdown' # A Markdown parser with syntax highlighting. gem 'tty-link' # To make URLs clickable in the terminal.","title":"Requirements"},{"location":"chat_gpt_tutorial/#main-code","text":"First, require the necessary libraries: require 'json' require 'time' We'll be using API keys and URI's defined in environment variables: API_KEY = ENV [ 'GPT4_KEY' ] PG_URI = ENV [ 'PG_URI' ] || ARGV [ ARGV . index ( \"--pg-uri\" ) ] We then define a Conversation class that interacts with TimescaleDB: class Conversation < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable ... end And an SQLExtractor class to extract SQL from markdown. This class will be stacking all markdown blocks to be executed later. class SQLExtractor < Redcarpet :: Render :: Base attr_reader :sql def block_code ( code , language ) if language == 'sql' @sql ||= [] @sql << code code else \"\" end end end The call_gpt4_api method is used to call the GPT-4 API with a prompt. At this moment we need to stack the following information: Role assignment: Initial instructions saying it's an AI agent - info that assigns the role of the API in this call. Context: Previous User Input + AI Responses in the same topic. User Input: what user is asking now. def call_gpt4_api ( prompt ) url = \"https://api.openai.com/v1/chat/completions\" full_prompt = INSTRUCTIONS + \" \\n History: #{ Conversation . history } \" + \" \\n Input: #{ prompt } \" body = { \"model\" => \"gpt-4\" , \"max_tokens\" => 1000 , \"temperature\" => 0 , \"messages\" => [ { \"role\" => \"user\" , \"content\" => full_prompt } ] , } . to_json headers = { \"Content-Type\" => \"application/json\" , \"Authorization\" => \"Bearer #{ API_KEY } \" } response = RestClient . post ( url , body , headers ) json_response = JSON . parse ( response . body ) response = json_response [ \"choices\" ]. first [ \"message\" ][ \"content\" ]. strip rescue RestClient :: BadRequest \"Bad Request Error: #{ $! . message } \" rescue \"Error: #{ $! . message } \" end The method try to execute the query and return high level error messages in case the execution fails: def execute ( query ) begin ActiveRecord :: Base . connection . execute ( query ) rescue => e \"Query Error: #{ e . message } \" end end Truncating query results As the AI has access to the database, sometimes the query results are quite heavy and then it really slows down the upstreaming and processing of the context. So, for now, I'm truncating the results in 10k characters. It reduced a lot the timeouts and still quite good and working well. json = execute ( sql ) . to_json results << json if json . length > 10000 json = json [ 0 .. 10000 ]+ \"... (truncated)\" end In chat_mode method, we loop to continuously get user input and interact with GPT-4: def chat_mode info WELCOME_INFO timeout = 300 # Set the timeout in seconds loop do print \" \\n #{ topic } : \" input = if IO . select ( [ STDIN ] , [] , [] , timeout ) STDIN . gets . chomp else puts \"Timeout reached, exiting chat.\" break end case input . downcase when 'quit' puts \"Exiting chat.\" break when 'debug' require \"pry\" ; binding . pry else with_no_logs do chat ( input ) end end end end Colored markdown To parse Markdown and have a colored markdown in the command line, use the magical tty-markdown library: def info ( content ) puts TTY :: Markdown . parse ( content ) end In chat method, we get the response from GPT-4, create a conversation record and then execute SQL queries from the markdown: def chat ( prompt ) response = call_gpt4_api ( prompt ) with_no_logs do Conversation . create ( topic : topic , user_input : prompt , ai_response : response , ts : Time . now ) end info ( \"**AI:** #{ response } \" ) queries = sql_from_markdown ( response ) if queries &. any? output = run_queries ( queries ) info ( output ) chat ( output ) end end To run the queries, you can also eval some context like the topic . def run_queries ( queries ) queries . each_with_index . map do | query , i | sql = query . gsub ( /#\\{(.*)\\}/ ){ eval ( $1 )} json = execute ( sql ) . to_json json = json [ 0 .. 10000 ]+ \"... (truncated)\" if json . length > 10000 <<~ MARKDOWN Result from query #{i+1}: #{json} MARKDOWN end . join ( \" \\n \" ) end eval is evil \ud83d\ude08 Note that we're using: eval ( $1 ) It will capture the result of the query param that is wrapped with #{} and execute it in the context. It's useful to guarantee we're filtereing by the topic and keep it simple for the example. If you want to build a production code make sure you use a more robust and safe approach for it. We'll also define the topic which will be used as a command line argument or just assigning the USER name as the default topic to group conversations. def topic ARGV [ 1 ] || ENV [ 'USER' ] end The instructions described before are also avaialable in the same [example folder][example] and was used as a raw text. You can also override and test with different subjects and evolve from some in progress research as well. def instructions if File . exists? ( ARGV . last ) ARGV . last else 'instructions.md' end end As a final step, setup the conversations table and enable the extension if necessary. def setup_schema ActiveRecord :: Base . logger = Logger . new ( STDOUT ) ActiveRecord :: Base . establish_connection ( PG_URI ) # Create the events table if it doesn't exist unless Conversation . table_exists? ActiveRecord :: Base . connection . instance_exec do execute \"CREATE EXTENSION IF NOT EXISTS timescaledb\" create_table :conversations , id : false , hypertable : { time_column : :ts } do | t | t . timestamptz :ts , default : \"now()\" , null : false t . string :topic , null : false t . text :user_input , null : false t . text :ai_response , null : false end end end end The main method is where we establish the connection and start the chat mode and we can call it right after the definition in the body of the ruby file. def main setup_schema chat_mode end main","title":"Main Code"},{"location":"chat_gpt_tutorial/#extra-capabilities","text":"I initially build it for queries but trying to request several database tasks, I was also able to enable compression, create continuous aggregates, add retention policies and also refresh continuous aggregates policies.","title":"Extra capabilities"},{"location":"chat_gpt_tutorial/#enable-compression-for-a-hypertable","text":"I started trying this out. Asking it to enable compression, and here is the interaction output. The following example show an interaction which will execute the sql query: User: enable compression for the conversations hypertable AI: To enable compression for the \u201cconversations\u201d hypertable in TimescaleDB, you would need to execute an ALTER TABLE command with the SET function to enable compression. However, as an AI language model, I don\u2019t have the ability to execute SQL commands directly on a database. Here\u2019s an example of how you could do it: ALTER TABLE conversations SET ( timescaledb.compress, timescaledb.compress_segmentby = 'user_input' ); This command will enable compression on the \u201cconversations\u201d hypertable and set the \u2018user_input\u2019 column as the segment by option. The \u2018compress_segmentby\u2019 option determines how the data will be ordered during compression, which can improve query performance. Please note that you should replace \u2018user_input\u2019 with the appropriate column name based on your specific use case. Also, remember to execute this command as a user with the necessary privileges. The example from the output will be executed by the command line in the database that you're connected. And the API response also follow with more instructions and recursively interact in case multiple back and forth are necessary.","title":"Enable compression for a hypertable"},{"location":"chat_gpt_tutorial/#create-continuous-aggregates","text":"Now, let's dive into continuous aggregates feature and ask to create the first materialized view for it. User: create a continuous aggregate materialized view summarizing number of words from user_input and ai_response grouped by 1 hour. AI: Sure, to create a continuous aggregate materialized view that summarizes the number of words from user_input and ai_response grouped by 1 hour, you can use the following SQL command: CREATE MATERIALIZED VIEW conversation_summary WITH (timescaledb.continuous) AS SELECT time_bucket(INTERVAL '1 hour', ts) AS one_hour, COUNT(*) AS total_conversations, SUM(LENGTH(user_input) - LENGTH(REPLACE(user_input, ' ', '')) + 1) AS user_input_words, SUM(LENGTH(ai_response) - LENGTH(REPLACE(ai_response, ' ', '')) + 1) AS ai_response_words FROM conversations GROUP BY one_hour; This command creates a materialized view named conversation_summary that groups the conversations by each hour. It also calculates the number of words in user_input and ai_response by counting the number of spaces and adding 1 (assuming words are separated by a single space). Please note that this command should be executed by someone with the necessary permissions in the database. Also, the actual creation of the view might take some time depending on the size of the conversations table. You can also try to setup retention policies or create new hypertables if you want. It seems very flexible and safe to use in a sandbox to explore data.","title":"Create continuous aggregates"},{"location":"chat_gpt_tutorial/#complex-query-building","text":"It's also very good on building more complex queries, especially after adding more examples about it.","title":"Complex query building"},{"location":"chat_gpt_tutorial/#knowing-issues","text":"While you can easily get some snippets, from time to time, if you change the subject, things will get complicated and it will commit several errors in a row. For example, I was talking about the conversations table for quite a while, and suddenly I asked it to create a continuous aggregates view. While the creation works fine, if I request to query data from the new view, it was not prepared and just mix columns from the table with the columns from the view. I tried several examples and it was not able to get it properly. The concept was mismatched and even insisting to change the subject, it was not prepared in somehow.","title":"Knowing issues"},{"location":"chat_gpt_tutorial/#try-it-yourself","text":"If you want to try it, this example is available on examples/chatgpt/openai-cli.rb and you can follow the instructions in the folder how to use it.","title":"Try it yourself"},{"location":"command_line/","text":"Command line application \u00b6 When you install the gem locally, a new command line application named tsdb will be available on your command line. The tsdb CLI \u00b6 It accepts a Postgresql URI and some extra flags that can help you to get more info from your TimescaleDB server: tsdb <uri> --stats Where the <uri> is replaced with params from your connection like: tsdb postgres://<user>@localhost:5432/<dbname> --stats Or merely check the stats: tsdb \"postgres://<user>@localhost:5432/timescaledb_test\" --stats Here is a sample output from a database example with almost no data: { :hypertables => { :count => 3 , :uncompressed => 2 , :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :befoe_compressing => \"80 KB\" , :after_compressing => \"0 Bytes\" }}, :continuous_aggregates => { :count => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To start a interactive ruby/ pry console use --console : The console will dynamically create models for all hypertables that it finds in the database. Let's consider the caggs.sql as the example of a database. psql postgres://<user>@localhost:5432/playground -f caggs.sql Then use tsdb in the command line with the same URI and --stats : tsdb postgres : // < user > @localhost : 5432 / playground -- stats { :hypertables => { :count => 1 , :uncompressed => 1 , :approximate_row_count => { \"ticks\" => 352 }, :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :uncompressed => \"88 KB\" , :compressed => \"0 Bytes\" }}, :continuous_aggregates => { :total => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To have some interactive playground with the actual database using ruby, just try the same command before changing from --stats to --console : tsdb --console \u00b6 We are using the same database from the previous example for this context which contains a hypertable named ticks and a view called ohlc_1m . tsdb postgres : // < user > @localhost : 5432 / playground -- console pry ( Timescale ) > The tsdb CLI will automatically create ActiveRecord models for hypertables and the continuous aggregates views. Tick => Timescaledb :: Tick ( time : datetime , symbol : string , price : decimal , volume : integer ) Note that it's only created for this session and will never cache in the library or any other place. In this case, the Tick model comes from the ticks hypertable found in the database. It contains several methods inherited from the acts_as_hypertable macro. Let's start with the .hypertable method. Tick . hypertable => #<Timescaledb::Hypertable:0x00007fe99c258900 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , owner : \"jonatasdp\" , num_dimensions : 1 , num_chunks : 1 , compression_enabled : false , is_distributed : false , replication_factor : nil , data_nodes : nil , tablespaces : nil > The core of the hypertables is the fragmentation of the data into chunks, the child tables that distribute the data. You can check all chunks directly from the hypertable relation. Tick . hypertable . chunks unknown OID 2206 : failed to recognize type of 'primary_dimension_type' . It will cast as a String . => [ #<Timescaledb::Chunk:0x00007fe99c31b068 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , chunk_schema : \"_timescaledb_internal\" , chunk_name : \"_hyper_33_17_chunk\" , primary_dimension : \"time\" , primary_dimension_type : \"timestamp without time zone\" , range_start : 1999 - 12 - 30 00 : 00 : 00 + 0000 , range_end : 2000 - 01 - 06 00 : 00 : 00 + 0000 , range_start_integer : nil , range_end_integer : nil , is_compressed : false , chunk_tablespace : nil , data_nodes : nil >] Chunks are created by partitioning the hypertable data into one (or potentially multiple) dimensions. All hypertables are partitions by the values belonging to a time column, which may be in timestamp, date, or various integer forms. If the time partitioning interval is one day, for example, then rows with timestamps that belong to the same day are co-located within the same chunk, while rows belonging to different days belong to different chunks. Learn more here . Another core concept of TimescaleDB is compression. With data partitioned, it becomes very convenient to compress and decompress chunks independently. Tick . hypertable . chunks . first . compress! ActiveRecord :: StatementInvalid : PG :: FeatureNotSupported : ERROR : compression not enabled on \"ticks\" DETAIL : It is not possible to compress chunks on a hypertable that does not have compression enabled . HINT : Enable compression using ALTER TABLE with the timescaledb . compress option . As compression is not enabled, let's do it by executing plain SQL directly from the actual context. To borrow a connection, let's use the Tick object. Tick . connection . execute ( \"ALTER TABLE ticks SET (timescaledb.compress)\" ) # => PG_OK And now, it's possible to compress and decompress: Tick . hypertable . chunks . first . compress! Tick . hypertable . chunks . first . decompress! Learn more about TimescaleDB compression here . The ohlc_1m view is also available as an ActiveRecord: Ohlc1m => Timescaledb :: Ohlc1m ( bucket : datetime , symbol : string , open : decimal , high : decimal , low : decimal , close : decimal , volume : integer ) And you can run any query as you do with regular active record queries. Ohlc1m . order ( bucket : :desc ) . last => #<Timescaledb::Ohlc1m:0x00007fe99c2c38e0 bucket : 2000 - 01 - 01 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 13 e2 , high : 0 . 3 e2 , low : 0 . 1 e1 , close : 0 . 1 e2 , volume : 27600 >","title":"Command Line"},{"location":"command_line/#command-line-application","text":"When you install the gem locally, a new command line application named tsdb will be available on your command line.","title":"Command line application"},{"location":"command_line/#the-tsdb-cli","text":"It accepts a Postgresql URI and some extra flags that can help you to get more info from your TimescaleDB server: tsdb <uri> --stats Where the <uri> is replaced with params from your connection like: tsdb postgres://<user>@localhost:5432/<dbname> --stats Or merely check the stats: tsdb \"postgres://<user>@localhost:5432/timescaledb_test\" --stats Here is a sample output from a database example with almost no data: { :hypertables => { :count => 3 , :uncompressed => 2 , :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :befoe_compressing => \"80 KB\" , :after_compressing => \"0 Bytes\" }}, :continuous_aggregates => { :count => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To start a interactive ruby/ pry console use --console : The console will dynamically create models for all hypertables that it finds in the database. Let's consider the caggs.sql as the example of a database. psql postgres://<user>@localhost:5432/playground -f caggs.sql Then use tsdb in the command line with the same URI and --stats : tsdb postgres : // < user > @localhost : 5432 / playground -- stats { :hypertables => { :count => 1 , :uncompressed => 1 , :approximate_row_count => { \"ticks\" => 352 }, :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :uncompressed => \"88 KB\" , :compressed => \"0 Bytes\" }}, :continuous_aggregates => { :total => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To have some interactive playground with the actual database using ruby, just try the same command before changing from --stats to --console :","title":"The tsdb CLI"},{"location":"command_line/#tsdb-console","text":"We are using the same database from the previous example for this context which contains a hypertable named ticks and a view called ohlc_1m . tsdb postgres : // < user > @localhost : 5432 / playground -- console pry ( Timescale ) > The tsdb CLI will automatically create ActiveRecord models for hypertables and the continuous aggregates views. Tick => Timescaledb :: Tick ( time : datetime , symbol : string , price : decimal , volume : integer ) Note that it's only created for this session and will never cache in the library or any other place. In this case, the Tick model comes from the ticks hypertable found in the database. It contains several methods inherited from the acts_as_hypertable macro. Let's start with the .hypertable method. Tick . hypertable => #<Timescaledb::Hypertable:0x00007fe99c258900 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , owner : \"jonatasdp\" , num_dimensions : 1 , num_chunks : 1 , compression_enabled : false , is_distributed : false , replication_factor : nil , data_nodes : nil , tablespaces : nil > The core of the hypertables is the fragmentation of the data into chunks, the child tables that distribute the data. You can check all chunks directly from the hypertable relation. Tick . hypertable . chunks unknown OID 2206 : failed to recognize type of 'primary_dimension_type' . It will cast as a String . => [ #<Timescaledb::Chunk:0x00007fe99c31b068 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , chunk_schema : \"_timescaledb_internal\" , chunk_name : \"_hyper_33_17_chunk\" , primary_dimension : \"time\" , primary_dimension_type : \"timestamp without time zone\" , range_start : 1999 - 12 - 30 00 : 00 : 00 + 0000 , range_end : 2000 - 01 - 06 00 : 00 : 00 + 0000 , range_start_integer : nil , range_end_integer : nil , is_compressed : false , chunk_tablespace : nil , data_nodes : nil >] Chunks are created by partitioning the hypertable data into one (or potentially multiple) dimensions. All hypertables are partitions by the values belonging to a time column, which may be in timestamp, date, or various integer forms. If the time partitioning interval is one day, for example, then rows with timestamps that belong to the same day are co-located within the same chunk, while rows belonging to different days belong to different chunks. Learn more here . Another core concept of TimescaleDB is compression. With data partitioned, it becomes very convenient to compress and decompress chunks independently. Tick . hypertable . chunks . first . compress! ActiveRecord :: StatementInvalid : PG :: FeatureNotSupported : ERROR : compression not enabled on \"ticks\" DETAIL : It is not possible to compress chunks on a hypertable that does not have compression enabled . HINT : Enable compression using ALTER TABLE with the timescaledb . compress option . As compression is not enabled, let's do it by executing plain SQL directly from the actual context. To borrow a connection, let's use the Tick object. Tick . connection . execute ( \"ALTER TABLE ticks SET (timescaledb.compress)\" ) # => PG_OK And now, it's possible to compress and decompress: Tick . hypertable . chunks . first . compress! Tick . hypertable . chunks . first . decompress! Learn more about TimescaleDB compression here . The ohlc_1m view is also available as an ActiveRecord: Ohlc1m => Timescaledb :: Ohlc1m ( bucket : datetime , symbol : string , open : decimal , high : decimal , low : decimal , close : decimal , volume : integer ) And you can run any query as you do with regular active record queries. Ohlc1m . order ( bucket : :desc ) . last => #<Timescaledb::Ohlc1m:0x00007fe99c2c38e0 bucket : 2000 - 01 - 01 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 13 e2 , high : 0 . 3 e2 , low : 0 . 1 e1 , close : 0 . 1 e2 , volume : 27600 >","title":"tsdb --console"},{"location":"migrations/","text":"ActiveRecord migrations helpers for Timescale \u00b6 Create table is now with the hypertable keyword allowing to pass a few options to the function call while also using the create_table method: create_table with the :hypertable option \u00b6 hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'identifier' , compression_interval : '7 days' } create_table ( :events , id : false , hypertable : hypertable_options ) do | t | t . string :identifier , null : false t . jsonb :payload t . timestamps end The create_continuous_aggregate helper \u00b6 This example shows a ticks table grouping ticks as OHLCV histograms for every minute. hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'symbol' , compress_orderby : 'created_at' , compression_interval : '7 days' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . string :symbol t . decimal :price t . integer :volume t . timestamps end Tick = Class . new ( ActiveRecord :: Base ) do self . table_name = 'ticks' self . primary_key = 'symbol' acts_as_hypertable end query = Tick . select ( <<~ QUERY ) time_bucket('1m', created_at) as time, symbol, FIRST(price, created_at) as open, MAX(price) as high, MIN(price) as low, LAST(price, created_at) as close, SUM(volume) as volume\").group(\"1,2\") QUERY options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , query , ** options ) If you need more details, please check this blog post . If you're interested in candlesticks and need to get the OHLC values, take a look at the toolkit ohlc function that do the same but through a function that can be reusing candlesticks from smaller timeframes.","title":"Migrations"},{"location":"migrations/#activerecord-migrations-helpers-for-timescale","text":"Create table is now with the hypertable keyword allowing to pass a few options to the function call while also using the create_table method:","title":"ActiveRecord migrations helpers for Timescale"},{"location":"migrations/#create_table-with-the-hypertable-option","text":"hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'identifier' , compression_interval : '7 days' } create_table ( :events , id : false , hypertable : hypertable_options ) do | t | t . string :identifier , null : false t . jsonb :payload t . timestamps end","title":"create_table with the :hypertable option"},{"location":"migrations/#the-create_continuous_aggregate-helper","text":"This example shows a ticks table grouping ticks as OHLCV histograms for every minute. hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'symbol' , compress_orderby : 'created_at' , compression_interval : '7 days' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . string :symbol t . decimal :price t . integer :volume t . timestamps end Tick = Class . new ( ActiveRecord :: Base ) do self . table_name = 'ticks' self . primary_key = 'symbol' acts_as_hypertable end query = Tick . select ( <<~ QUERY ) time_bucket('1m', created_at) as time, symbol, FIRST(price, created_at) as open, MAX(price) as high, MIN(price) as low, LAST(price, created_at) as close, SUM(volume) as volume\").group(\"1,2\") QUERY options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , query , ** options ) If you need more details, please check this blog post . If you're interested in candlesticks and need to get the OHLC values, take a look at the toolkit ohlc function that do the same but through a function that can be reusing candlesticks from smaller timeframes.","title":"The create_continuous_aggregate helper"},{"location":"models/","text":"Models \u00b6 The ActiveRecord is the default ORM in the Ruby community. We have introduced a macro that helps you to inject the behavior as other libraries do in the Rails ecosystem. The acts_as_hypertable macro \u00b6 You can declare a Rails model as a Hypertable by invoking the acts_as_hypertable macro. This macro extends your existing model with timescaledb-related functionality. model: class Event < ActiveRecord :: Base acts_as_hypertable end By default, ActsAsHypertable assumes a record's time_column is called created_at . Options \u00b6 If you are using a different time_column name, you can specify it as follows when invoking the acts_as_hypertable macro: class Event < ActiveRecord :: Base acts_as_hypertable time_column :timestamp end Chunks \u00b6 To get all the chunks from a model's hypertable, you can use .chunks . Event . chunks # => [#<Timescaledb::Chunk>, ...] Hypertable metadata \u00b6 To get the models' hypertable metadata, you can use .hypertable . Event . hypertable # => #<Timescaledb::Hypertable> To get hypertable metadata for all hypertables: Timescaledb.hypertables . Compression Settings \u00b6 Compression settings are accessible through the hypertable. Event . hypertable . compression_settings # => [#<Timescaledb::CompressionSettings>, ...] To get compression settings for all hypertables: Timescaledb.compression_settings . Scopes \u00b6 When you enable ActsAsHypertable on your model, we include a few default scopes. They are: Scope name What they return Model.previous_month Records created in the previous month Model.previous_week Records created in the previous week Model.this_month Records created this month Model.this_week Records created this week Model.yesterday Records created yesterday Model.today Records created today Model.last_hour Records created in the last hour All time-related scopes respect your application's timezone. Scenic integration \u00b6 The Scenic gem is easy to manage database view definitions for a Rails application. Unfortunately, TimescaleDB's continuous aggregates are more complex than regular PostgreSQL views, and the schema dumper included with Scenic can't dump a complete definition. This gem automatically configures Scenic to use a Timescaledb::Scenic::Adapter. which will correctly handle schema dumping.","title":"Models"},{"location":"models/#models","text":"The ActiveRecord is the default ORM in the Ruby community. We have introduced a macro that helps you to inject the behavior as other libraries do in the Rails ecosystem.","title":"Models"},{"location":"models/#the-acts_as_hypertable-macro","text":"You can declare a Rails model as a Hypertable by invoking the acts_as_hypertable macro. This macro extends your existing model with timescaledb-related functionality. model: class Event < ActiveRecord :: Base acts_as_hypertable end By default, ActsAsHypertable assumes a record's time_column is called created_at .","title":"The acts_as_hypertable macro"},{"location":"models/#options","text":"If you are using a different time_column name, you can specify it as follows when invoking the acts_as_hypertable macro: class Event < ActiveRecord :: Base acts_as_hypertable time_column :timestamp end","title":"Options"},{"location":"models/#chunks","text":"To get all the chunks from a model's hypertable, you can use .chunks . Event . chunks # => [#<Timescaledb::Chunk>, ...]","title":"Chunks"},{"location":"models/#hypertable-metadata","text":"To get the models' hypertable metadata, you can use .hypertable . Event . hypertable # => #<Timescaledb::Hypertable> To get hypertable metadata for all hypertables: Timescaledb.hypertables .","title":"Hypertable metadata"},{"location":"models/#compression-settings","text":"Compression settings are accessible through the hypertable. Event . hypertable . compression_settings # => [#<Timescaledb::CompressionSettings>, ...] To get compression settings for all hypertables: Timescaledb.compression_settings .","title":"Compression Settings"},{"location":"models/#scopes","text":"When you enable ActsAsHypertable on your model, we include a few default scopes. They are: Scope name What they return Model.previous_month Records created in the previous month Model.previous_week Records created in the previous week Model.this_month Records created this month Model.this_week Records created this week Model.yesterday Records created yesterday Model.today Records created today Model.last_hour Records created in the last hour All time-related scopes respect your application's timezone.","title":"Scopes"},{"location":"models/#scenic-integration","text":"The Scenic gem is easy to manage database view definitions for a Rails application. Unfortunately, TimescaleDB's continuous aggregates are more complex than regular PostgreSQL views, and the schema dumper included with Scenic can't dump a complete definition. This gem automatically configures Scenic to use a Timescaledb::Scenic::Adapter. which will correctly handle schema dumping.","title":"Scenic integration"},{"location":"toolkit/","text":"The TimescaleDB Toolkit \u00b6 The TimescaleDB Toolkit is an extension brought by Timescale for more hyperfunctions, fully compatible with TimescaleDB and PostgreSQL. They have almost no dependecy of hypertables but they play very well in the hypertables ecosystem. The mission of the toolkit team is to ease all things analytics when using TimescaleDB, with a particular focus on developer ergonomics and performance. Here, we're going to have a small walkthrough in some of the toolkit functions and the helpers that can make simplify the generation of some complex queries. Warning Note that we're just starting the toolkit integration in the gem and several functions are still experimental. The add_toolkit_to_search_path! helper \u00b6 Several functions on the toolkit are still in experimental phase, and for that reason they're not in the public schema, but lives in the toolkit_experimental schema. To use them without worring about the schema or prefixing it in all the cases, you can introduce the schema as part of the search_path . To make it easy in the Ruby side, you can call the method directly from the ActiveRecord connection: ActiveRecord :: Base . connection . add_toolkit_to_search_path! This statement is actually adding the toolkit_experimental to the search path aside of the public and the $user variable path. The statement can be placed right before your usage of the toolkit. For example, if a single controller in your Rails app will be using it, you can create a filter in the controller to set up it before the use of your action. class StatisticsController < ActionController :: Base before_action :add_timescale_toolkit , only : [ :complex_query ] def complex_query # some code that uses the toolkit functions end protected def add_timescale_toolkit ActiveRecord :: Base . connection . add_toolkit_to_search_path! end Example from scratch to use the Toolkit functions \u00b6 Let's start by working on some example about the volatility algorithm. This example is inspired in the function pipelines blog post, which brings an example about how to calculate volatility and then apply the function pipelines to make the same with the toolkit. Success Reading the blog post before trying this is highly recommended, and will give you more insights on how to apply and use time vectors that is our next topic. Let's start by creating the measurements hypertable using a regular migration: class CreateMeasurements < ActiveRecord :: Migration def change hypertable_options = { time_column : 'ts' , chunk_time_interval : '1 day' , } create_table :measurements , hypertable : hypertable_options , id : false do | t | t . integer :device_id t . decimal :val t . timestamp :ts end end end In this example, we just have a hypertable with no compression options. Every 1 day a new child table aka chunk will be generated. No compression options for now. Now, let's add the model app/models/measurement.rb : class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" end At this moment, you can jump into the Rails console and start testing the model. Seeding some data \u00b6 Before we build a very complex example, let's build something that is easy to follow and comprehend. Let's create 3 records for the same device, representing a hourly measurement of some sensor. yesterday = 1 . day . ago [ 1 , 2 , 3 ]. each_with_index do | v , i | Measurement . create ( device_id : 1 , ts : yesterday + i . hour , val : v ) end Every value is a progression from 1 to 3. Now, we can build a query to get the values and let's build the example using plain Ruby. values = Measurement . order ( :ts ) . pluck ( :val ) # => [1,2,3] Using plain Ruby, we can build this example with a few lines of code: previous = nil volatilities = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end # volatilities => [nil, 1, 1] volatility = volatilities . compact . sum # => 2 Compact can be skipped and we can also build the sum in the same loop. So, a refactored version would be: previous = nil volatility = 0 values . each do | value | if previous delta = ( value - previous ) . abs volatility += delta end previous = value end volatility # => 2 Now, it's time to move it to a database level calculating the volatility using plain postgresql. A subquery is required to build the calculated delta, so it seems a bit more confusing: delta = Measurement . select ( \"device_id, abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts)) as abs_delta\" ) Measurement . select ( \"device_id, sum(abs_delta) as volatility\" ) . from ( \"( #{ delta . to_sql } ) as calc_delta\" ) . group ( 'device_id' ) The final query for the example above looks like this: SELECT device_id , SUM ( abs_delta ) AS volatility FROM ( SELECT device_id , ABS ( val - LAG ( val ) OVER ( PARTITION BY device_id ORDER BY ts ) ) AS abs_delta FROM \"measurements\" ) AS calc_delta GROUP BY device_id It's much harder to understand the actual example then go with plain SQL and now let's reproduce the same example using the toolkit pipelines: Measurement . select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility SQL As you can see, it's much easier to read and digest the example. Now, let's take a look in how we can generate the queries using the scopes injected by the acts_as_time_vector macro. Adding the acts_as_time_vector macro \u00b6 Let's start changing the model to add the acts_as_time_vector that is here to allow us to not repeat the parameters of the timevector(ts, val) call. class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" end end If you skip the time_column option in the acts_as_time_vector it will inherit the same value from the acts_as_hypertable . I'm making it explicit here for the sake of making the macros independent. Now, that we have it, let's create a scope for it: class Measurement < ActiveRecord :: Base acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> do select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(#{time_column}, #{value_column}) -> sort() -> delta() -> abs() -> sum() as volatility SQL end end Now, we have created the volatility scope, grouping by device_id always. In the Toolkit helpers, we have a similar version which also contains a default segmentation based in the segment_by configuration done through the acts_as_time_vector macro. A method segment_by_column is added to access this configuration, so we can make a small change that makes you completely understand the volatility macro. class Measurement < ActiveRecord :: Base # ... Skipping previous code to focus in the example acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> ( columns = segment_by_column ) do _scope = select ( [* columns , \"timevector( #{ time_column } , #{ value_column } ) -> sort() -> delta() -> abs() -> sum() as volatility\" ]. join ( \", \" )) _scope = _scope . group ( columns ) if columns _scope end end Testing the method: Measurement . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.6ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" # => [{\"device_id\"=>1, \"volatility\"=>8.0}] Let's add a few more records with random values: yesterday = 1 . day . ago ( 2 .. 6 ) . each do | d | ( 1 .. 10 ) . each do | j | Measurement . create ( device_id : d , ts : yesterday + j . hour , val : rand ( 10 )) end end Testing all the values: Measurement . order ( \"device_id\" ) . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.3ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" ORDER BY device_id => [ { \"device_id\" => 1 , \"volatility\" => 8 . 0 }, { \"device_id\" => 2 , \"volatility\" => 24 . 0 }, { \"device_id\" => 3 , \"volatility\" => 30 . 0 }, { \"device_id\" => 4 , \"volatility\" => 32 . 0 }, { \"device_id\" => 5 , \"volatility\" => 44 . 0 }, { \"device_id\" => 6 , \"volatility\" => 23 . 0 } ] If the parameter is explicit nil it will not group by: Measurement . volatility ( nil ) . map ( & :attributes ) # DEBUG -- : Measurement Load (5.4ms) SELECT timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" # => [{\"volatility\"=>186.0, \"device_id\"=>nil}] Comparing with Ruby version \u00b6 Now, it's time to benchmark and compare Ruby vs PostgreSQL solutions, verifying which is faster: class Measurement < ActiveRecord :: Base # code you already know scope :volatility_by_device_id , -> { volatility = Hash . new ( 0 ) previous = Hash . new find_all do | measurement | device_id = measurement . device_id if previous [ device_id ] delta = ( measurement . val - previous [ device_id ] ) . abs volatility [ device_id ] += delta end previous [ device_id ] = measurement . val end volatility } end Now, benchmarking the real time to compute it on Ruby in milliseconds. Benchmark . measure { Measurement . volatility_by_device_id } . real * 1000 # => 3.021999917924404 Seeding massive data \u00b6 Now, let's use generate_series to fast insert a lot of records directly into the database and make it full of records. Let's just agree on some numbers to have a good start. Let's generate data for 5 devices emitting values every 5 minutes, which will generate around 50k records. Let's use some plain SQL to insert the records now: sql = \"INSERT INTO measurements (ts, device_id, val) SELECT ts, device_id, random()*80 FROM generate_series(TIMESTAMP '2022-01-01 00:00:00', TIMESTAMP '2022-02-01 00:00:00', INTERVAL '5 minutes') AS g1(ts), generate_series(0, 5) AS g2(device_id); \" ActiveRecord :: Base . connection . execute ( sql ) In my MacOS M1 processor it took less than a second to insert the 53k records: # DEBUG (177.5ms) INSERT INTO measurements (ts, device_id, val) .. # => #<PG::Result:0x00007f8152034168 status=PGRES_COMMAND_OK ntuples=0 nfields=0 cmd_tuples=53574> Now, let's measure compare the time to process the volatility: Benchmark . bm do | x | x . report ( \"ruby\" ) { pp Measurement . volatility_by_device_id } x . report ( \"sql\" ) { pp Measurement . volatility ( \"device_id\" ) . map ( & :attributes ) } end # user system total real # ruby 0.612439 0.061890 0.674329 ( 0.727590) # sql 0.001142 0.000301 0.001443 ( 0.060301) Calculating the performance ratio we can see 0.72 / 0.06 means that SQL is 12 times faster than Ruby to process volatility \ud83c\udf89 Just considering it was localhost, we don't have the internet to pass all the records over the wires. Now, moving to a remote host look the numbers: Warning Note that the previous numbers where using localhost. Now, using a remote connection between different regions, it looks even ~500 times slower than SQL. user system total real ruby 0.716321 0.041640 0.757961 ( 6.388881) sql 0.001156 0.000177 0.001333 ( 0.161270) Let\u2019s recap what\u2019s time consuming here. The find_all is just not optimized to fetch the data and also consuming most of the time here. It\u2019s also fetching the data and converting it to ActiveRecord model which has thousands of methods. It\u2019s very comfortable but just need the attributes to make it. Let\u2019s optimize it by plucking an array of values grouped by device. class Measurement < ActiveRecord :: Base # ... scope :values_from_devices , -> { ordered_values = select ( :val , :device_id ) . order ( :ts ) Hash [ from ( ordered_values ) . group ( :device_id ) . pluck ( \"device_id, array_agg(val)\" ) ] } end Now, let's create a method for processing volatility. class Volatility def self . process ( values ) previous = nil deltas = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end #deltas => [nil, 1, 1] deltas . shift volatility = deltas . sum end def self . process_values ( map ) map . transform_values ( & method ( :process )) end end Now, let's change the benchmark to expose the time for fetching and processing: volatilities = nil ActiveRecord :: Base . logger = nil Benchmark . bm do | x | x . report ( \"ruby\" ) { Measurement . volatility_ruby } x . report ( \"sql\" ) { Measurement . volatility_sql . map ( & :attributes ) } x . report ( \"fetch\" ) { volatilities = Measurement . values_from_devices } x . report ( \"process\" ) { Volatility . process_values ( volatilities ) } end Checking the results: user system total real ruby 0.683654 0.036558 0.720212 ( 0.743942) sql 0.000876 0.000096 0.000972 ( 0.054234) fetch 0.078045 0.003221 0.081266 ( 0.116693) process 0.067643 0.006473 0.074116 ( 0.074122) Much better, now we can see only 200ms difference between real time which means ~36% more. If we try to break down a bit more of the SQL part, we can see that the EXPLAIN ANALYSE SELECT device_id , array_agg ( val ) FROM ( SELECT val , device_id FROM measurements ORDER BY ts ASC ) subquery GROUP BY device_id ; We can check the execution time and make it clear how much time is necessary just for the processing part, isolating network and the ActiveRecord layer. \u2502 Planning Time: 17.761 ms \u2502 \u2502 Execution Time: 36.302 ms So, it means that from the 116ms to fetch the data, only 54ms was used from the DB and the remaining 62ms was consumed by network + ORM.","title":"Toolkit Integration"},{"location":"toolkit/#the-timescaledb-toolkit","text":"The TimescaleDB Toolkit is an extension brought by Timescale for more hyperfunctions, fully compatible with TimescaleDB and PostgreSQL. They have almost no dependecy of hypertables but they play very well in the hypertables ecosystem. The mission of the toolkit team is to ease all things analytics when using TimescaleDB, with a particular focus on developer ergonomics and performance. Here, we're going to have a small walkthrough in some of the toolkit functions and the helpers that can make simplify the generation of some complex queries. Warning Note that we're just starting the toolkit integration in the gem and several functions are still experimental.","title":"The TimescaleDB Toolkit"},{"location":"toolkit/#the-add_toolkit_to_search_path-helper","text":"Several functions on the toolkit are still in experimental phase, and for that reason they're not in the public schema, but lives in the toolkit_experimental schema. To use them without worring about the schema or prefixing it in all the cases, you can introduce the schema as part of the search_path . To make it easy in the Ruby side, you can call the method directly from the ActiveRecord connection: ActiveRecord :: Base . connection . add_toolkit_to_search_path! This statement is actually adding the toolkit_experimental to the search path aside of the public and the $user variable path. The statement can be placed right before your usage of the toolkit. For example, if a single controller in your Rails app will be using it, you can create a filter in the controller to set up it before the use of your action. class StatisticsController < ActionController :: Base before_action :add_timescale_toolkit , only : [ :complex_query ] def complex_query # some code that uses the toolkit functions end protected def add_timescale_toolkit ActiveRecord :: Base . connection . add_toolkit_to_search_path! end","title":"The add_toolkit_to_search_path! helper"},{"location":"toolkit/#example-from-scratch-to-use-the-toolkit-functions","text":"Let's start by working on some example about the volatility algorithm. This example is inspired in the function pipelines blog post, which brings an example about how to calculate volatility and then apply the function pipelines to make the same with the toolkit. Success Reading the blog post before trying this is highly recommended, and will give you more insights on how to apply and use time vectors that is our next topic. Let's start by creating the measurements hypertable using a regular migration: class CreateMeasurements < ActiveRecord :: Migration def change hypertable_options = { time_column : 'ts' , chunk_time_interval : '1 day' , } create_table :measurements , hypertable : hypertable_options , id : false do | t | t . integer :device_id t . decimal :val t . timestamp :ts end end end In this example, we just have a hypertable with no compression options. Every 1 day a new child table aka chunk will be generated. No compression options for now. Now, let's add the model app/models/measurement.rb : class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" end At this moment, you can jump into the Rails console and start testing the model.","title":"Example from scratch to use the Toolkit functions"},{"location":"toolkit/#seeding-some-data","text":"Before we build a very complex example, let's build something that is easy to follow and comprehend. Let's create 3 records for the same device, representing a hourly measurement of some sensor. yesterday = 1 . day . ago [ 1 , 2 , 3 ]. each_with_index do | v , i | Measurement . create ( device_id : 1 , ts : yesterday + i . hour , val : v ) end Every value is a progression from 1 to 3. Now, we can build a query to get the values and let's build the example using plain Ruby. values = Measurement . order ( :ts ) . pluck ( :val ) # => [1,2,3] Using plain Ruby, we can build this example with a few lines of code: previous = nil volatilities = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end # volatilities => [nil, 1, 1] volatility = volatilities . compact . sum # => 2 Compact can be skipped and we can also build the sum in the same loop. So, a refactored version would be: previous = nil volatility = 0 values . each do | value | if previous delta = ( value - previous ) . abs volatility += delta end previous = value end volatility # => 2 Now, it's time to move it to a database level calculating the volatility using plain postgresql. A subquery is required to build the calculated delta, so it seems a bit more confusing: delta = Measurement . select ( \"device_id, abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts)) as abs_delta\" ) Measurement . select ( \"device_id, sum(abs_delta) as volatility\" ) . from ( \"( #{ delta . to_sql } ) as calc_delta\" ) . group ( 'device_id' ) The final query for the example above looks like this: SELECT device_id , SUM ( abs_delta ) AS volatility FROM ( SELECT device_id , ABS ( val - LAG ( val ) OVER ( PARTITION BY device_id ORDER BY ts ) ) AS abs_delta FROM \"measurements\" ) AS calc_delta GROUP BY device_id It's much harder to understand the actual example then go with plain SQL and now let's reproduce the same example using the toolkit pipelines: Measurement . select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility SQL As you can see, it's much easier to read and digest the example. Now, let's take a look in how we can generate the queries using the scopes injected by the acts_as_time_vector macro.","title":"Seeding some data"},{"location":"toolkit/#adding-the-acts_as_time_vector-macro","text":"Let's start changing the model to add the acts_as_time_vector that is here to allow us to not repeat the parameters of the timevector(ts, val) call. class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" end end If you skip the time_column option in the acts_as_time_vector it will inherit the same value from the acts_as_hypertable . I'm making it explicit here for the sake of making the macros independent. Now, that we have it, let's create a scope for it: class Measurement < ActiveRecord :: Base acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> do select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(#{time_column}, #{value_column}) -> sort() -> delta() -> abs() -> sum() as volatility SQL end end Now, we have created the volatility scope, grouping by device_id always. In the Toolkit helpers, we have a similar version which also contains a default segmentation based in the segment_by configuration done through the acts_as_time_vector macro. A method segment_by_column is added to access this configuration, so we can make a small change that makes you completely understand the volatility macro. class Measurement < ActiveRecord :: Base # ... Skipping previous code to focus in the example acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> ( columns = segment_by_column ) do _scope = select ( [* columns , \"timevector( #{ time_column } , #{ value_column } ) -> sort() -> delta() -> abs() -> sum() as volatility\" ]. join ( \", \" )) _scope = _scope . group ( columns ) if columns _scope end end Testing the method: Measurement . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.6ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" # => [{\"device_id\"=>1, \"volatility\"=>8.0}] Let's add a few more records with random values: yesterday = 1 . day . ago ( 2 .. 6 ) . each do | d | ( 1 .. 10 ) . each do | j | Measurement . create ( device_id : d , ts : yesterday + j . hour , val : rand ( 10 )) end end Testing all the values: Measurement . order ( \"device_id\" ) . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.3ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" ORDER BY device_id => [ { \"device_id\" => 1 , \"volatility\" => 8 . 0 }, { \"device_id\" => 2 , \"volatility\" => 24 . 0 }, { \"device_id\" => 3 , \"volatility\" => 30 . 0 }, { \"device_id\" => 4 , \"volatility\" => 32 . 0 }, { \"device_id\" => 5 , \"volatility\" => 44 . 0 }, { \"device_id\" => 6 , \"volatility\" => 23 . 0 } ] If the parameter is explicit nil it will not group by: Measurement . volatility ( nil ) . map ( & :attributes ) # DEBUG -- : Measurement Load (5.4ms) SELECT timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" # => [{\"volatility\"=>186.0, \"device_id\"=>nil}]","title":"Adding the acts_as_time_vector macro"},{"location":"toolkit/#comparing-with-ruby-version","text":"Now, it's time to benchmark and compare Ruby vs PostgreSQL solutions, verifying which is faster: class Measurement < ActiveRecord :: Base # code you already know scope :volatility_by_device_id , -> { volatility = Hash . new ( 0 ) previous = Hash . new find_all do | measurement | device_id = measurement . device_id if previous [ device_id ] delta = ( measurement . val - previous [ device_id ] ) . abs volatility [ device_id ] += delta end previous [ device_id ] = measurement . val end volatility } end Now, benchmarking the real time to compute it on Ruby in milliseconds. Benchmark . measure { Measurement . volatility_by_device_id } . real * 1000 # => 3.021999917924404","title":"Comparing with Ruby version"},{"location":"toolkit/#seeding-massive-data","text":"Now, let's use generate_series to fast insert a lot of records directly into the database and make it full of records. Let's just agree on some numbers to have a good start. Let's generate data for 5 devices emitting values every 5 minutes, which will generate around 50k records. Let's use some plain SQL to insert the records now: sql = \"INSERT INTO measurements (ts, device_id, val) SELECT ts, device_id, random()*80 FROM generate_series(TIMESTAMP '2022-01-01 00:00:00', TIMESTAMP '2022-02-01 00:00:00', INTERVAL '5 minutes') AS g1(ts), generate_series(0, 5) AS g2(device_id); \" ActiveRecord :: Base . connection . execute ( sql ) In my MacOS M1 processor it took less than a second to insert the 53k records: # DEBUG (177.5ms) INSERT INTO measurements (ts, device_id, val) .. # => #<PG::Result:0x00007f8152034168 status=PGRES_COMMAND_OK ntuples=0 nfields=0 cmd_tuples=53574> Now, let's measure compare the time to process the volatility: Benchmark . bm do | x | x . report ( \"ruby\" ) { pp Measurement . volatility_by_device_id } x . report ( \"sql\" ) { pp Measurement . volatility ( \"device_id\" ) . map ( & :attributes ) } end # user system total real # ruby 0.612439 0.061890 0.674329 ( 0.727590) # sql 0.001142 0.000301 0.001443 ( 0.060301) Calculating the performance ratio we can see 0.72 / 0.06 means that SQL is 12 times faster than Ruby to process volatility \ud83c\udf89 Just considering it was localhost, we don't have the internet to pass all the records over the wires. Now, moving to a remote host look the numbers: Warning Note that the previous numbers where using localhost. Now, using a remote connection between different regions, it looks even ~500 times slower than SQL. user system total real ruby 0.716321 0.041640 0.757961 ( 6.388881) sql 0.001156 0.000177 0.001333 ( 0.161270) Let\u2019s recap what\u2019s time consuming here. The find_all is just not optimized to fetch the data and also consuming most of the time here. It\u2019s also fetching the data and converting it to ActiveRecord model which has thousands of methods. It\u2019s very comfortable but just need the attributes to make it. Let\u2019s optimize it by plucking an array of values grouped by device. class Measurement < ActiveRecord :: Base # ... scope :values_from_devices , -> { ordered_values = select ( :val , :device_id ) . order ( :ts ) Hash [ from ( ordered_values ) . group ( :device_id ) . pluck ( \"device_id, array_agg(val)\" ) ] } end Now, let's create a method for processing volatility. class Volatility def self . process ( values ) previous = nil deltas = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end #deltas => [nil, 1, 1] deltas . shift volatility = deltas . sum end def self . process_values ( map ) map . transform_values ( & method ( :process )) end end Now, let's change the benchmark to expose the time for fetching and processing: volatilities = nil ActiveRecord :: Base . logger = nil Benchmark . bm do | x | x . report ( \"ruby\" ) { Measurement . volatility_ruby } x . report ( \"sql\" ) { Measurement . volatility_sql . map ( & :attributes ) } x . report ( \"fetch\" ) { volatilities = Measurement . values_from_devices } x . report ( \"process\" ) { Volatility . process_values ( volatilities ) } end Checking the results: user system total real ruby 0.683654 0.036558 0.720212 ( 0.743942) sql 0.000876 0.000096 0.000972 ( 0.054234) fetch 0.078045 0.003221 0.081266 ( 0.116693) process 0.067643 0.006473 0.074116 ( 0.074122) Much better, now we can see only 200ms difference between real time which means ~36% more. If we try to break down a bit more of the SQL part, we can see that the EXPLAIN ANALYSE SELECT device_id , array_agg ( val ) FROM ( SELECT val , device_id FROM measurements ORDER BY ts ASC ) subquery GROUP BY device_id ; We can check the execution time and make it clear how much time is necessary just for the processing part, isolating network and the ActiveRecord layer. \u2502 Planning Time: 17.761 ms \u2502 \u2502 Execution Time: 36.302 ms So, it means that from the 116ms to fetch the data, only 54ms was used from the DB and the remaining 62ms was consumed by network + ORM.","title":"Seeding massive data"},{"location":"toolkit_candlestick/","text":"Candlesticks \u00b6 Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the candlestick function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit. Storing your market data \u00b6 Market data is generally a massive data events stream. You can watch multiple stock transactions globabally and receive trading, bidding and asking events. Events happen in time, and it makes them timeseries data, which makes it a perfect scenario to adopt hypertables. The most granular data of trading events from market data is generally called tick. Tick represents a single trade of something at a point in time. The tick contains the necessary attributes of a deal in the financial markets. The most simplified version of a tick is represented by: a time which the transaction happened. a symbol which generally is the stock name. a price representing the unit price of any stock. a volume representing the amount of stocks being traded. Let's create the hypertable to store this information using the create_table method from ActiveRecord API. Create the hypertable \u00b6 We'll call ticks the hypertable name to store the market data. The strategy will be the following: Partition the data into a week interval chunks. So, it means that every new week a new partition depending on the time column. Compress the data after a month to save storage. Remove the raw data after six months and just leave the aggregated data for longer time. Ticks are massive. They can reach milions of events per day. That's why it's important to have a compression policy since day one. Compressing data is a key timescaledb feature. It can be done automatically and every chunk can be compressed after a desired period. Timescaledb has an excellent compression ratio for market data and you can easily reach 95% savings on storage by adopting compression. The timescaledb gem adds the hypertable keyword to the create_table method, which allows you to specify anything related to the hypertable. ActiveRecord :: Base . connection . instance_exec do hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestamp :time t . string :symbol t . decimal :price t . decimal :volume end add_index :ticks , [ :time , :symbol ] end Note that the previous command is a standard create_table method call. The timescaledb gem is just adding the hypertable keyword which allows to configure everything in the same point. In this case, we have the following commands being executed behind the scenes: Create the standard table CREATE TABLE \"ticks\" ( \"time\" timestamp with time zone , \"symbol\" text , \"price\" decimal , \"volume\" float ); Convert it to a hypertable with chunk_time_interval of 1 week . SELECT create_hypertable ( 'ticks' , 'time' , chunk_time_interval => INTERVAL '1 week' ) The compression is enabled and configured in the hypertable. ALTER TABLE ticks SET ( timescaledb . compress , timescaledb . compress_orderby = 'time' , timescaledb . compress_segmentby = 'symbol' ) Index by time and symbol It will be high used the combination of time and symbol. It's good to have a symbol. The create_index is the last call and it will make common queries run faster. CREATE INDEX \"index_ticks_on_time_and_symbol\" ON \"ticks\" ( \"time\" , \"symbol\" ) Add the compression policy to compress data after a month SELECT add_compression_policy ( 'ticks' , INTERVAL '1 month' ); Create the ORM model \u00b6 To define the model, we're going to inherit ActiveRecord::Base to create a model. Timeseries data will always require the time column and the primary key can be discarded. A few default methods will not work if they depend on the if of the object. The model is the best place to describe how you'll be using the timescaledb to keep your model DRY and consistent. class Tick < ActiveRecord :: Base acts_as_hypertable time_column : :time acts_as_time_vector value_column : :price , segment_by : :symbol end Actually, the gem shares how the The acts_as_hypertable macro will assume the actual model corresponds to a hypertable and inject useful scopes and methods that can be wrapping to the following TimescaleDB features: .hypertable will give you access to the [hypertable][hypertable] domain, the table_name will be used to get all metadata from the _timescaledb_catalog and combine all the functions that receives a hypertable_name as a parameter. The time_column keyword argument will be used to build scopes like .yesterday , .previous_week , .last_hour . And can be used for your own scopes using the time_column metadata. The acts_as_time_vector will be offering functions related to timescaledb toolkit. The value_column: will be combined with the time_column from the hypertable to use scopes like candlestick , volatility , lttb and just configure the missing information. The segment_by: will be widely used in the scopes to group by the data. When the keywords time_column , value_column and segment_by are used in the acts_as_{hypertable,time_vector} modules. By convention, all scopes reuse the metadata from the configuration. It can facilitate the process of build a lot of hypertable abstractions to facilitate the use combined scopes in the queries. The acts_as_hypertable macro \u00b6 The acts_as_hypertable will bring the Model.hypertable which will allow us to use a set of timeseries related set what are the default columns used to calculate the data. The acts_as_time_vector macro \u00b6 The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. The acts_as_time_vector will inject handy scopes that wraps the default formulas from timescaledb-toolkit extension. It will be very powerful to build your set of abstractions over it and simplify the maintenance of complex queries directly in the database. Inserting data \u00b6 The generate_series sql function can speed up the process to seed some random data and make it available to start playing with the queries. The following code will insert tick data simulating prices from the previews week until yesterday. We're using a single symbol and one tick every 10 seconds. ActiveRecord :: Base . connection . instance_exec do data_range = { from : 1 . week . ago . to_date , to : 1 . day . from_now . to_date } execute ( ActiveRecord :: Base . sanitize_sql_for_conditions ( [<<~ SQL , data_range ] )) INSERT INTO ticks SELECT time, 'SYMBOL', 1 + (random()*30)::int, 100*(random()*10)::int FROM generate_series( TIMESTAMP :from, TIMESTAMP :to, INTERVAL '10 second') AS time; SQL end The database will seed a week of trade data with a randomize prices and volumes simulating one event every 10 seconds. The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by the symbol . Symbol can be any stock trade and it's good to be segmenting and indexing by it. If you need to generate some data for your table, please check this post . Query data \u00b6 When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the _candlestick . The candlestick scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . candlestick ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . candlestick ( time , price , volume ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS candlestick The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . candlestick ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses. Continuous aggregates \u00b6 If you would like to continuous process the stream and aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: ActiveRecord :: Base . connection . instance_exec do options = { with_data : true , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'candlestick_1m' , Tick . _candlestick ( timeframe : '1m' ), ** options ) end Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string. Also, we're using the _candlestick method scope instead of the candlestick one. The reason is that the candlestick method already bring the attribute values while the _candlestick can bring you the pre-processed data in a intermediate state that can be rolled up with other candlesticks. For example, let's say you already created a continuous aggregates of one minute and now you'd like to process 5 minutes. You don't need to reprocess the raw data. You can build the candlestick using the information from the one minute candlesticks. Models for views \u00b6 It's very convenient to setup models for continuous aggregates which can make it easy to inherit all smart methods to compose queries. class Candlestick1m < ActiveRecord :: Base self . table_name = 'candlestick_1m' include Candlestick end class Candlestick1h < ActiveRecord :: Base self . table_name = 'candlestick_1h' include Candlestick end class Candlestick1d < ActiveRecord :: Base self . table_name = 'candlestick_1d' include Candlestick end Note that all classes include the Candlestick module. Let's define it to make it easy to use the shared behavior. The candlestick concern \u00b6 Concerns are already available through active_support and they can help you to organize shared logic that can be included in multiple models. In this concern, we'll have: Use the acts_as_hypertable macro to inherit all query scopes. Define attributes for all candlestick attributes Define extra scopes to read the data and rollup to bigger timeframes. Mark the model as readonly. require \"active_support/concern\" module Candlestick extend ActiveSupport :: Concern included do acts_as_hypertable time_column : \"time_bucket\" %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end attribute :volume , :decimal attribute :vwap , :decimal scope :attributes , -> do select ( \"symbol, time_bucket, toolkit_experimental.open(candlestick), toolkit_experimental.high(candlestick), toolkit_experimental.low(candlestick), toolkit_experimental.close(candlestick), toolkit_experimental.open_time(candlestick), toolkit_experimental.high_time(candlestick), toolkit_experimental.low_time(candlestick), toolkit_experimental.close_time(candlestick), toolkit_experimental.volume(candlestick), toolkit_experimental.vwap(candlestick)\" ) end def readonly? true end end end Hierarchical continuous aggregates \u00b6 After you get the first one minute continuous aggregates, you don't need to revisit the raw data to create candlesticks from it. You can build the 1 hour candlestick from the 1 minute candlestick. The Hierarchical continuous aggregates are very useful to save IO and processing time. Rollup \u00b6 The candlestick_agg function returns a candlesticksummary object. The rollup allows you to combine candlestick summaries into new structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the candlesticks multiple times saving processing from the server and make it easier to manage aggregations with different time intervals. In the previous example, we used the .candlestick function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close , volume , and vwap functions that can access the values behind the candlesticksummary type. To merge the candlesticks, the rollup method can aggregate several candlesticksummary objects into a bigger timeframe. Let's rollup the structures: module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :rollup , -> ( timeframe : '1h' ) do bucket = %|time_bucket(' #{ timeframe } ', \"time_bucket\")| select ( bucket , \"symbol\" , \"toolkit_experimental.rollup(candlestick) as candlestick\" ) . group ( 1 , 2 ) . order ( 1 ) end end end Now, the new views in bigger timeframes can be added using it's own objects. ActiveRecord :: Base . connection . instance_exec do options = -> ( timeframe ) { { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL ' #{ timeframe } '\" , schedule_interval : \"INTERVAL ' #{ timeframe } '\" } } } create_continuous_aggregate ( 'candlestick_1h' , Candlestick1m . rollup ( timeframe : '1 hour' ), ** options [ '1 hour' ] ) create_continuous_aggregate ( 'candlestick_1d' , Candlestick1h . rollup ( timeframe : '1 day' ), ** options [ '1 day' ] ) end The final SQL executed to create the first hierarchical continuous aggregates is the following: CREATE MATERIALIZED VIEW candlestick_1h WITH ( timescaledb . continuous ) AS SELECT time_bucket ( '1 hour' , \"time_bucket\" ), \"candlestick_1m\" . \"symbol\" , toolkit_experimental . rollup ( candlestick ) as candlestick FROM \"candlestick_1m\" GROUP BY 1 , 2 ORDER BY 1 WITH DATA ; So, as you can see all candlestick of one hour views follows the same interface of one minute, having the same column names and values, allowing to be reuse in larger timeframes. Refresh policy \u00b6 Timescaledb is assuming you're storing real time data. Which means you can continuous feed the ticks table and aggregate the materialized data from time to time. When create_continuous_aggregate is called with a schedule_interval it will also execute the following SQL line: SELECT add_continuous_aggregate_policy ( 'candlestick_1h' , start_offset => INTERVAL '1 month' , end_offset => INTERVAL '1 hour' , schedule_interval => INTERVAL '1 hour' ); Instead of updating the values row by row, the refresh policy will automatically run in background and aggregate the new data with the configured timeframe. Querying Continuous Aggregates with custom ActiveRecord models \u00b6 With the Candlestick1m and Candlestick1h wrapping the continuous aggregates into models, now, it's time to explore the available scopes and what to do with it. Candlestick1m . yesterday . first It will run the following SQL: SELECT \"candlestick_1m\" . * FROM \"candlestick_1m\" WHERE ( DATE ( time_bucket ) = '2023-01-23' ) LIMIT 1 ; And return the following object: #<Candlestick1m:0x000000010fbeff68 time_bucket : 2023 - 01 - 23 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , candlestick : \"(version:1,open:(ts: \\\" 2023-01-23 00:00:00+00 \\\" ,val:9),high:(ts: \\\" 2023-01-23 00:00:10+00 \\\" ,val:24),low:(ts: \\\" 2023-01-23 00:00:50+00 \\\" ,val:2),close:(ts: \\\" 2023-01-23 00:00:50+00 \\\" ,val:2),volume:Transaction(vol:2400,vwap:26200))\" , open : nil , open_time : nil , high : nil , high_time : nil , low : nil , low_time : nil , close : nil , close_time : nil , volume : nil , vwap : nil > Note that the attributes are not available in the object but a candlestick attribute is present holding all the information. That's why it's necessary to use the attributes scope: Candlestick1m . yesterday . attributes . first Which will run the following query: SELECT symbol , time_bucket , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM \"candlestick_1m\" WHERE ( DATE ( time_bucket ) = '2023-01-23' ) LIMIT 1 ; And the object will be filled with the attributes: => #<Candlestick1m:0x000000010fc3e578 time_bucket : 2023 - 01 - 23 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 9 e1 , open_time : 2023 - 01 - 23 00 : 00 : 00 + 0000 , high : 0 . 24 e2 , high_time : 2023 - 01 - 23 00 : 00 : 10 + 0000 , low : 0 . 2 e1 , low_time : 2023 - 01 - 23 00 : 00 : 50 + 0000 , close : 0 . 2 e1 , close_time : 2023 - 01 - 23 00 : 00 : 50 + 0000 , volume : 0 . 24 e4 , vwap : 0 . 1091666666666666 e2 > It's a very convenient strategy to have the Candlestick as a shared concern to allow to reuse queries in different views of the same type. The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the candlestick type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to one hour, you can do: Candlestick1m . attributes . from ( Candlestick1m . rollup ( timeframe : '1 hour' ) ) And from minute to one hour to a day: Candlestick1m . attributes . from ( Candlestick1m . rollup ( timeframe : '1 day' ) . from ( Candlestick1m . rollup ( timeframe : '1 hour' ) ) ) Both examples are just using the one minute continuous aggregates view and reprocessing it from there. Composing the subqueries will probably be less efficient and unnecessary as we already created more continuous aggregates in the top of another continuous aggregates. Here is the SQL generated from the last nested rollups code: SELECT symbol , time_bucket , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM ( SELECT time_bucket ( '1 day' , \"time_bucket\" ), symbol , toolkit_experimental . rollup ( candlestick ) as candlestick FROM ( SELECT time_bucket ( '1 hour' , \"time_bucket\" ), \"candlestick_1m\" . \"symbol\" , toolkit_experimental . rollup ( candlestick ) as candlestick FROM \"candlestick_1m\" GROUP BY 1 , 2 ORDER BY 1 ) subquery GROUP BY 1 , 2 ORDER BY 1 ) subquery Plotting data \u00b6 Now, the final step is plot the data using the javascript plotly library. For this step, we're going to use a sinatra library to serve HTML and javascript and build the endpoints that will be consumed by the front end. The Sinatra App \u00b6 require 'sinatra/base' require \"sinatra/json\" class App < Sinatra::Base get '/ candlestick . js ' do send_file ' candlestick . js ' end get '/ candlestick_1m ' do json ({ title: \" Candlestick 1 minute last hour \", data: Candlestick1m . last_hour . plotly_candlestick }) end get '/ candlestick_1h ' do json ({ title: \" Candlestick yesterday hourly \", data: Candlestick1h . yesterday . plotly_candlestick }) end get '/ candlestick_1d ' do json ({ title: \" Candlestick daily this month \", data: Candlestick1d . previous_week . plotly_candlestick }) end get '/' do << -HTML < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = 'https://cdn.plot.ly/plotly-2.17.1.min.js' ></ script > < script src = '/candlestick.js' ></ script > </ head > < body > < div id = 'charts' > </ body > HTML end run! if app_file == $0 end Plotting data with Javascript \u00b6 And the candlesticks.js file will be responsible for fetch data async and add new candlestick charts. let addChart = () => $ ( '<div/>' ). appendTo ( '#charts' )[ 0 ] function ohlcChartFrom ( url ) { $ . ajax ({ url : url , success : function ( result ) { let { data , title } = result ; let { x , open , high , low , close , type } = data ; open = open . map ( parseFloat ); high = high . map ( parseFloat ); low = low . map ( parseFloat ); close = close . map ( parseFloat ); var layout = { title : title , dragmode : 'zoom' , margin : { r : 10 , t : 25 , b : 40 , l : 60 }, showlegend : false , xaxis : { autorange : true , domain : [ 0 , 1 ], title : 'Date' , type : 'date' }, yaxis : { autorange : true , domain : [ 0 , 1 ], type : 'linear' } }; ohlc = { x , open , high , low , close , type }; Plotly . newPlot ( addChart (), [ ohlc ], layout ); } }); }; $ ( document ). ready ( function () { ohlcChartFrom ( '/candlestick_1m' ); ohlcChartFrom ( '/candlestick_1h' ); ohlcChartFrom ( '/candlestick_1d' ); }); Note that a new plotly_candlestick scope was mentioned in the view models and we need to add it to the Candlestick module to make it available for all the charts. module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :plotly_candlestick , -> do data = attributes { type : 'candlestick' , xaxis : 'x' , yaxis : 'y' , x : data . map ( & :time_bucket ), open : data . map ( & :open ), high : data . map ( & :high ), low : data . map ( & :low ), close : data . map ( & :close ) } end end end Formatting time vectors \u00b6 Another function from toolkit that can help you to prepare the data to plot is the to_text one from the toolkit. This is an experimental feature that allows you to prepare the JSON output using a template directly in the database to easily dump the output data without need to convert the data. module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :time_vector_from_candlestick , -> ( attribute : \"close\" ) do select ( \"timevector(time_bucket, toolkit_experimental. #{ attribute } (candlestick))\" ) end scope :plotly_attribute , -> ( attribute : \"close\" , from : nil , template : %\\'{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}'\\ ) do from ||= time_vector_from_candlestick ( attribute : attribute ) select ( \"toolkit_experimental.to_text(tv.timevector, #{ template } )::json\" ) . from ( \"( #{ from . to_sql } ) as tv\" ) . first [ \"to_text\" ] end end end The final SQL will look something like: SELECT toolkit_experimental . to_text ( tv . timevector , '{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}' ):: json FROM ( SELECT timevector ( time_bucket , toolkit_experimental . close ( candlestick )) FROM \"candlestick_1h\" ) as tv LIMIT 1 Time vectors are the common ground for dataframe processing in the toolkit and it will allow you to easily run into several analysis and export the info with the to_text function. The to_text function receives the second argument as a template: '{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}' As you can see the double curly braces can wrap variables and nested methods which can help you to export the information with some string traits. By default we have the following variables available: TIMES refer to the vector of times VALUES refer to the vector of values TIMEVALS refer to the combination of pairs of times and values. Feedback or questions? \u00b6 I hope you find this tutorial interesting and you can also check the candlestick.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"Toolkit Candlesticks tutorial"},{"location":"toolkit_candlestick/#candlesticks","text":"Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the candlestick function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit.","title":"Candlesticks"},{"location":"toolkit_candlestick/#storing-your-market-data","text":"Market data is generally a massive data events stream. You can watch multiple stock transactions globabally and receive trading, bidding and asking events. Events happen in time, and it makes them timeseries data, which makes it a perfect scenario to adopt hypertables. The most granular data of trading events from market data is generally called tick. Tick represents a single trade of something at a point in time. The tick contains the necessary attributes of a deal in the financial markets. The most simplified version of a tick is represented by: a time which the transaction happened. a symbol which generally is the stock name. a price representing the unit price of any stock. a volume representing the amount of stocks being traded. Let's create the hypertable to store this information using the create_table method from ActiveRecord API.","title":"Storing your market data"},{"location":"toolkit_candlestick/#create-the-hypertable","text":"We'll call ticks the hypertable name to store the market data. The strategy will be the following: Partition the data into a week interval chunks. So, it means that every new week a new partition depending on the time column. Compress the data after a month to save storage. Remove the raw data after six months and just leave the aggregated data for longer time. Ticks are massive. They can reach milions of events per day. That's why it's important to have a compression policy since day one. Compressing data is a key timescaledb feature. It can be done automatically and every chunk can be compressed after a desired period. Timescaledb has an excellent compression ratio for market data and you can easily reach 95% savings on storage by adopting compression. The timescaledb gem adds the hypertable keyword to the create_table method, which allows you to specify anything related to the hypertable. ActiveRecord :: Base . connection . instance_exec do hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestamp :time t . string :symbol t . decimal :price t . decimal :volume end add_index :ticks , [ :time , :symbol ] end Note that the previous command is a standard create_table method call. The timescaledb gem is just adding the hypertable keyword which allows to configure everything in the same point. In this case, we have the following commands being executed behind the scenes: Create the standard table CREATE TABLE \"ticks\" ( \"time\" timestamp with time zone , \"symbol\" text , \"price\" decimal , \"volume\" float ); Convert it to a hypertable with chunk_time_interval of 1 week . SELECT create_hypertable ( 'ticks' , 'time' , chunk_time_interval => INTERVAL '1 week' ) The compression is enabled and configured in the hypertable. ALTER TABLE ticks SET ( timescaledb . compress , timescaledb . compress_orderby = 'time' , timescaledb . compress_segmentby = 'symbol' ) Index by time and symbol It will be high used the combination of time and symbol. It's good to have a symbol. The create_index is the last call and it will make common queries run faster. CREATE INDEX \"index_ticks_on_time_and_symbol\" ON \"ticks\" ( \"time\" , \"symbol\" ) Add the compression policy to compress data after a month SELECT add_compression_policy ( 'ticks' , INTERVAL '1 month' );","title":"Create the hypertable"},{"location":"toolkit_candlestick/#create-the-orm-model","text":"To define the model, we're going to inherit ActiveRecord::Base to create a model. Timeseries data will always require the time column and the primary key can be discarded. A few default methods will not work if they depend on the if of the object. The model is the best place to describe how you'll be using the timescaledb to keep your model DRY and consistent. class Tick < ActiveRecord :: Base acts_as_hypertable time_column : :time acts_as_time_vector value_column : :price , segment_by : :symbol end Actually, the gem shares how the The acts_as_hypertable macro will assume the actual model corresponds to a hypertable and inject useful scopes and methods that can be wrapping to the following TimescaleDB features: .hypertable will give you access to the [hypertable][hypertable] domain, the table_name will be used to get all metadata from the _timescaledb_catalog and combine all the functions that receives a hypertable_name as a parameter. The time_column keyword argument will be used to build scopes like .yesterday , .previous_week , .last_hour . And can be used for your own scopes using the time_column metadata. The acts_as_time_vector will be offering functions related to timescaledb toolkit. The value_column: will be combined with the time_column from the hypertable to use scopes like candlestick , volatility , lttb and just configure the missing information. The segment_by: will be widely used in the scopes to group by the data. When the keywords time_column , value_column and segment_by are used in the acts_as_{hypertable,time_vector} modules. By convention, all scopes reuse the metadata from the configuration. It can facilitate the process of build a lot of hypertable abstractions to facilitate the use combined scopes in the queries.","title":"Create the ORM model"},{"location":"toolkit_candlestick/#the-acts_as_hypertable-macro","text":"The acts_as_hypertable will bring the Model.hypertable which will allow us to use a set of timeseries related set what are the default columns used to calculate the data.","title":"The acts_as_hypertable macro"},{"location":"toolkit_candlestick/#the-acts_as_time_vector-macro","text":"The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. The acts_as_time_vector will inject handy scopes that wraps the default formulas from timescaledb-toolkit extension. It will be very powerful to build your set of abstractions over it and simplify the maintenance of complex queries directly in the database.","title":"The acts_as_time_vector macro"},{"location":"toolkit_candlestick/#inserting-data","text":"The generate_series sql function can speed up the process to seed some random data and make it available to start playing with the queries. The following code will insert tick data simulating prices from the previews week until yesterday. We're using a single symbol and one tick every 10 seconds. ActiveRecord :: Base . connection . instance_exec do data_range = { from : 1 . week . ago . to_date , to : 1 . day . from_now . to_date } execute ( ActiveRecord :: Base . sanitize_sql_for_conditions ( [<<~ SQL , data_range ] )) INSERT INTO ticks SELECT time, 'SYMBOL', 1 + (random()*30)::int, 100*(random()*10)::int FROM generate_series( TIMESTAMP :from, TIMESTAMP :to, INTERVAL '10 second') AS time; SQL end The database will seed a week of trade data with a randomize prices and volumes simulating one event every 10 seconds. The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by the symbol . Symbol can be any stock trade and it's good to be segmenting and indexing by it. If you need to generate some data for your table, please check this post .","title":"Inserting data"},{"location":"toolkit_candlestick/#query-data","text":"When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the _candlestick . The candlestick scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . candlestick ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . candlestick ( time , price , volume ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS candlestick The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . candlestick ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses.","title":"Query data"},{"location":"toolkit_candlestick/#continuous-aggregates","text":"If you would like to continuous process the stream and aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: ActiveRecord :: Base . connection . instance_exec do options = { with_data : true , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'candlestick_1m' , Tick . _candlestick ( timeframe : '1m' ), ** options ) end Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string. Also, we're using the _candlestick method scope instead of the candlestick one. The reason is that the candlestick method already bring the attribute values while the _candlestick can bring you the pre-processed data in a intermediate state that can be rolled up with other candlesticks. For example, let's say you already created a continuous aggregates of one minute and now you'd like to process 5 minutes. You don't need to reprocess the raw data. You can build the candlestick using the information from the one minute candlesticks.","title":"Continuous aggregates"},{"location":"toolkit_candlestick/#models-for-views","text":"It's very convenient to setup models for continuous aggregates which can make it easy to inherit all smart methods to compose queries. class Candlestick1m < ActiveRecord :: Base self . table_name = 'candlestick_1m' include Candlestick end class Candlestick1h < ActiveRecord :: Base self . table_name = 'candlestick_1h' include Candlestick end class Candlestick1d < ActiveRecord :: Base self . table_name = 'candlestick_1d' include Candlestick end Note that all classes include the Candlestick module. Let's define it to make it easy to use the shared behavior.","title":"Models for views"},{"location":"toolkit_candlestick/#the-candlestick-concern","text":"Concerns are already available through active_support and they can help you to organize shared logic that can be included in multiple models. In this concern, we'll have: Use the acts_as_hypertable macro to inherit all query scopes. Define attributes for all candlestick attributes Define extra scopes to read the data and rollup to bigger timeframes. Mark the model as readonly. require \"active_support/concern\" module Candlestick extend ActiveSupport :: Concern included do acts_as_hypertable time_column : \"time_bucket\" %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end attribute :volume , :decimal attribute :vwap , :decimal scope :attributes , -> do select ( \"symbol, time_bucket, toolkit_experimental.open(candlestick), toolkit_experimental.high(candlestick), toolkit_experimental.low(candlestick), toolkit_experimental.close(candlestick), toolkit_experimental.open_time(candlestick), toolkit_experimental.high_time(candlestick), toolkit_experimental.low_time(candlestick), toolkit_experimental.close_time(candlestick), toolkit_experimental.volume(candlestick), toolkit_experimental.vwap(candlestick)\" ) end def readonly? true end end end","title":"The candlestick concern"},{"location":"toolkit_candlestick/#hierarchical-continuous-aggregates","text":"After you get the first one minute continuous aggregates, you don't need to revisit the raw data to create candlesticks from it. You can build the 1 hour candlestick from the 1 minute candlestick. The Hierarchical continuous aggregates are very useful to save IO and processing time.","title":"Hierarchical continuous aggregates"},{"location":"toolkit_candlestick/#rollup","text":"The candlestick_agg function returns a candlesticksummary object. The rollup allows you to combine candlestick summaries into new structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the candlesticks multiple times saving processing from the server and make it easier to manage aggregations with different time intervals. In the previous example, we used the .candlestick function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close , volume , and vwap functions that can access the values behind the candlesticksummary type. To merge the candlesticks, the rollup method can aggregate several candlesticksummary objects into a bigger timeframe. Let's rollup the structures: module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :rollup , -> ( timeframe : '1h' ) do bucket = %|time_bucket(' #{ timeframe } ', \"time_bucket\")| select ( bucket , \"symbol\" , \"toolkit_experimental.rollup(candlestick) as candlestick\" ) . group ( 1 , 2 ) . order ( 1 ) end end end Now, the new views in bigger timeframes can be added using it's own objects. ActiveRecord :: Base . connection . instance_exec do options = -> ( timeframe ) { { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL ' #{ timeframe } '\" , schedule_interval : \"INTERVAL ' #{ timeframe } '\" } } } create_continuous_aggregate ( 'candlestick_1h' , Candlestick1m . rollup ( timeframe : '1 hour' ), ** options [ '1 hour' ] ) create_continuous_aggregate ( 'candlestick_1d' , Candlestick1h . rollup ( timeframe : '1 day' ), ** options [ '1 day' ] ) end The final SQL executed to create the first hierarchical continuous aggregates is the following: CREATE MATERIALIZED VIEW candlestick_1h WITH ( timescaledb . continuous ) AS SELECT time_bucket ( '1 hour' , \"time_bucket\" ), \"candlestick_1m\" . \"symbol\" , toolkit_experimental . rollup ( candlestick ) as candlestick FROM \"candlestick_1m\" GROUP BY 1 , 2 ORDER BY 1 WITH DATA ; So, as you can see all candlestick of one hour views follows the same interface of one minute, having the same column names and values, allowing to be reuse in larger timeframes.","title":"Rollup"},{"location":"toolkit_candlestick/#refresh-policy","text":"Timescaledb is assuming you're storing real time data. Which means you can continuous feed the ticks table and aggregate the materialized data from time to time. When create_continuous_aggregate is called with a schedule_interval it will also execute the following SQL line: SELECT add_continuous_aggregate_policy ( 'candlestick_1h' , start_offset => INTERVAL '1 month' , end_offset => INTERVAL '1 hour' , schedule_interval => INTERVAL '1 hour' ); Instead of updating the values row by row, the refresh policy will automatically run in background and aggregate the new data with the configured timeframe.","title":"Refresh policy"},{"location":"toolkit_candlestick/#querying-continuous-aggregates-with-custom-activerecord-models","text":"With the Candlestick1m and Candlestick1h wrapping the continuous aggregates into models, now, it's time to explore the available scopes and what to do with it. Candlestick1m . yesterday . first It will run the following SQL: SELECT \"candlestick_1m\" . * FROM \"candlestick_1m\" WHERE ( DATE ( time_bucket ) = '2023-01-23' ) LIMIT 1 ; And return the following object: #<Candlestick1m:0x000000010fbeff68 time_bucket : 2023 - 01 - 23 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , candlestick : \"(version:1,open:(ts: \\\" 2023-01-23 00:00:00+00 \\\" ,val:9),high:(ts: \\\" 2023-01-23 00:00:10+00 \\\" ,val:24),low:(ts: \\\" 2023-01-23 00:00:50+00 \\\" ,val:2),close:(ts: \\\" 2023-01-23 00:00:50+00 \\\" ,val:2),volume:Transaction(vol:2400,vwap:26200))\" , open : nil , open_time : nil , high : nil , high_time : nil , low : nil , low_time : nil , close : nil , close_time : nil , volume : nil , vwap : nil > Note that the attributes are not available in the object but a candlestick attribute is present holding all the information. That's why it's necessary to use the attributes scope: Candlestick1m . yesterday . attributes . first Which will run the following query: SELECT symbol , time_bucket , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM \"candlestick_1m\" WHERE ( DATE ( time_bucket ) = '2023-01-23' ) LIMIT 1 ; And the object will be filled with the attributes: => #<Candlestick1m:0x000000010fc3e578 time_bucket : 2023 - 01 - 23 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 9 e1 , open_time : 2023 - 01 - 23 00 : 00 : 00 + 0000 , high : 0 . 24 e2 , high_time : 2023 - 01 - 23 00 : 00 : 10 + 0000 , low : 0 . 2 e1 , low_time : 2023 - 01 - 23 00 : 00 : 50 + 0000 , close : 0 . 2 e1 , close_time : 2023 - 01 - 23 00 : 00 : 50 + 0000 , volume : 0 . 24 e4 , vwap : 0 . 1091666666666666 e2 > It's a very convenient strategy to have the Candlestick as a shared concern to allow to reuse queries in different views of the same type. The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the candlestick type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to one hour, you can do: Candlestick1m . attributes . from ( Candlestick1m . rollup ( timeframe : '1 hour' ) ) And from minute to one hour to a day: Candlestick1m . attributes . from ( Candlestick1m . rollup ( timeframe : '1 day' ) . from ( Candlestick1m . rollup ( timeframe : '1 hour' ) ) ) Both examples are just using the one minute continuous aggregates view and reprocessing it from there. Composing the subqueries will probably be less efficient and unnecessary as we already created more continuous aggregates in the top of another continuous aggregates. Here is the SQL generated from the last nested rollups code: SELECT symbol , time_bucket , toolkit_experimental . open ( candlestick ), toolkit_experimental . high ( candlestick ), toolkit_experimental . low ( candlestick ), toolkit_experimental . close ( candlestick ), toolkit_experimental . open_time ( candlestick ), toolkit_experimental . high_time ( candlestick ), toolkit_experimental . low_time ( candlestick ), toolkit_experimental . close_time ( candlestick ), toolkit_experimental . volume ( candlestick ), toolkit_experimental . vwap ( candlestick ) FROM ( SELECT time_bucket ( '1 day' , \"time_bucket\" ), symbol , toolkit_experimental . rollup ( candlestick ) as candlestick FROM ( SELECT time_bucket ( '1 hour' , \"time_bucket\" ), \"candlestick_1m\" . \"symbol\" , toolkit_experimental . rollup ( candlestick ) as candlestick FROM \"candlestick_1m\" GROUP BY 1 , 2 ORDER BY 1 ) subquery GROUP BY 1 , 2 ORDER BY 1 ) subquery","title":"Querying Continuous Aggregates with custom ActiveRecord models"},{"location":"toolkit_candlestick/#plotting-data","text":"Now, the final step is plot the data using the javascript plotly library. For this step, we're going to use a sinatra library to serve HTML and javascript and build the endpoints that will be consumed by the front end.","title":"Plotting data"},{"location":"toolkit_candlestick/#the-sinatra-app","text":"require 'sinatra/base' require \"sinatra/json\" class App < Sinatra::Base get '/ candlestick . js ' do send_file ' candlestick . js ' end get '/ candlestick_1m ' do json ({ title: \" Candlestick 1 minute last hour \", data: Candlestick1m . last_hour . plotly_candlestick }) end get '/ candlestick_1h ' do json ({ title: \" Candlestick yesterday hourly \", data: Candlestick1h . yesterday . plotly_candlestick }) end get '/ candlestick_1d ' do json ({ title: \" Candlestick daily this month \", data: Candlestick1d . previous_week . plotly_candlestick }) end get '/' do << -HTML < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = 'https://cdn.plot.ly/plotly-2.17.1.min.js' ></ script > < script src = '/candlestick.js' ></ script > </ head > < body > < div id = 'charts' > </ body > HTML end run! if app_file == $0 end","title":"The Sinatra App"},{"location":"toolkit_candlestick/#plotting-data-with-javascript","text":"And the candlesticks.js file will be responsible for fetch data async and add new candlestick charts. let addChart = () => $ ( '<div/>' ). appendTo ( '#charts' )[ 0 ] function ohlcChartFrom ( url ) { $ . ajax ({ url : url , success : function ( result ) { let { data , title } = result ; let { x , open , high , low , close , type } = data ; open = open . map ( parseFloat ); high = high . map ( parseFloat ); low = low . map ( parseFloat ); close = close . map ( parseFloat ); var layout = { title : title , dragmode : 'zoom' , margin : { r : 10 , t : 25 , b : 40 , l : 60 }, showlegend : false , xaxis : { autorange : true , domain : [ 0 , 1 ], title : 'Date' , type : 'date' }, yaxis : { autorange : true , domain : [ 0 , 1 ], type : 'linear' } }; ohlc = { x , open , high , low , close , type }; Plotly . newPlot ( addChart (), [ ohlc ], layout ); } }); }; $ ( document ). ready ( function () { ohlcChartFrom ( '/candlestick_1m' ); ohlcChartFrom ( '/candlestick_1h' ); ohlcChartFrom ( '/candlestick_1d' ); }); Note that a new plotly_candlestick scope was mentioned in the view models and we need to add it to the Candlestick module to make it available for all the charts. module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :plotly_candlestick , -> do data = attributes { type : 'candlestick' , xaxis : 'x' , yaxis : 'y' , x : data . map ( & :time_bucket ), open : data . map ( & :open ), high : data . map ( & :high ), low : data . map ( & :low ), close : data . map ( & :close ) } end end end","title":"Plotting data with Javascript"},{"location":"toolkit_candlestick/#formatting-time-vectors","text":"Another function from toolkit that can help you to prepare the data to plot is the to_text one from the toolkit. This is an experimental feature that allows you to prepare the JSON output using a template directly in the database to easily dump the output data without need to convert the data. module Candlestick extend ActiveSupport :: Concern included do # ... previous code remains the same scope :time_vector_from_candlestick , -> ( attribute : \"close\" ) do select ( \"timevector(time_bucket, toolkit_experimental. #{ attribute } (candlestick))\" ) end scope :plotly_attribute , -> ( attribute : \"close\" , from : nil , template : %\\'{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}'\\ ) do from ||= time_vector_from_candlestick ( attribute : attribute ) select ( \"toolkit_experimental.to_text(tv.timevector, #{ template } )::json\" ) . from ( \"( #{ from . to_sql } ) as tv\" ) . first [ \"to_text\" ] end end end The final SQL will look something like: SELECT toolkit_experimental . to_text ( tv . timevector , '{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}' ):: json FROM ( SELECT timevector ( time_bucket , toolkit_experimental . close ( candlestick )) FROM \"candlestick_1h\" ) as tv LIMIT 1 Time vectors are the common ground for dataframe processing in the toolkit and it will allow you to easily run into several analysis and export the info with the to_text function. The to_text function receives the second argument as a template: '{\"x\": {{ TIMES | json_encode() | safe }}, \"y\": {{ VALUES | json_encode() | safe }}, \"type\": \"scatter\"}' As you can see the double curly braces can wrap variables and nested methods which can help you to export the information with some string traits. By default we have the following variables available: TIMES refer to the vector of times VALUES refer to the vector of values TIMEVALS refer to the combination of pairs of times and values.","title":"Formatting time vectors"},{"location":"toolkit_candlestick/#feedback-or-questions","text":"I hope you find this tutorial interesting and you can also check the candlestick.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"Feedback or questions?"},{"location":"toolkit_lttb_tutorial/","text":"Largest Triangle Three Buckets is a downsampling method that tries to retain visual similarity between the downsampled data and the original dataset. While most frameworks implement it in the front end, TimescaleDB Toolkit provides an implementation that takes (timestamp, value) pairs, sorts them if needed, and downsamples the values directly in the database. In the following steps, you'll learn how to use LTTB from both databases and the Ruby programming language\u2014writing the LTTB algorithm in Ruby from scratch\u2014fully comprehend how it works and later compares the performance and usability of both solutions. Later, we'll benchmark the downsampling methods and the plain data using a real scenario. The data points are actual data from the weather dataset . If you want to run it yourself, feel free to use the example that contains all the steps we will describe here. Setup the dependencies \u00b6 Bundler inline avoids the creation of the Gemfile to prototype code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'chartkick' gem 'sinatra' end require 'timescaledb/toolkit' The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. pry is here because it's the best REPL to debug any Ruby code. We add it in the end to ease the exploring session you can do yourself after learning with the tutorial. chartkick is the library that can plot the values and make it easy to plot the data results. sinatra is a DSL for quickly creating web applications with minimal effort. Setup database \u00b6 Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here . Establishing the connection \u00b6 The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good. Downloading the dataset \u00b6 The weather dataset is available here , and here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create a setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset. Declaring the models \u00b6 Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end Putting all together \u00b6 Now it's time to call the methods we implemented before. So, let's set up a logger to STDOUT to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Info If you want to go deeper and reload everything every time, feel free to add the following lines before the unless block: drop_table ( :conditions ) if Condition . table_exists? drop_table ( :locations ) if Location . table_exists? Let's keep the example simple to run it manually and drop the tables when we want to run everything from scratch. Processing LTTB in Ruby \u00b6 You can find an old lttb gem available if you want to cut down this step but this library is not fully implementing the lttb algorithm, and the results may differ from the Timescale implementation. If you want to understand the algorithm behind the scenes, this step will make it very clear and easy to digest. You can also preview the original lttb here . Info The original thesis describes lttb as: The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now. The question is what point should the algorithm use in the last bucket to form the triangle.\" The obvious answer is to use a brute-force approach and simply try out all the possibilities. That is, for each point in the current bucket, form a triangle with all the points in the next bucket. It turns out that this gives a fairly good visual result, but as with many brute-force approaches it is inefficient. For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another and more clever solution is to add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with this two fixed point in the adjacent buckets is then selected. In figure 4.4 it is shown how point B forms the largest triangle across the buckets with fixed point A (previously selected) and the temporary point C. Calculate the area of a Triangle \u00b6 To demonstrate the same, let's create a module Triangle with an area method that accepts three points a', b , and c , which will be pairs of x and y' cartesian coordinates. module Triangle module_function def area ( a , b , c ) ( ax , ay ), ( bx , by ), ( cx , cy ) = a , b , c ( ( ax - cx ) . to_f * ( by - ay ) - ( ax - bx ) . to_f * ( cy - ay ) ) . abs * 0 . 5 end end Info In this implementation, we're using the shoelace method. The shoelace method (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. It has applications in surveying and forestry, among other areas. Source: Shoelace formula Wikipedia Initializing the Lttb class \u00b6 The lttb class will be responsible for processing the data and downsampling the points to the desired threshold. Let's declare the initial boilerplate code with some basic validation to make it work. class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end Note that the threshold considers at least 3 points as the edges should keep untouched, and the algorithm will reduce only the points in the middle. Calculating the average of points \u00b6 Combining all possible points to check the largest area would become very hard for performance reasons. For this case, we need to have an average method. The average between the points will become the temporary point as the previous documentation described: > _For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another clever solution is to add a temporary point to the last bucket and keep it fixed. That way, the algorithm has two fixed points;_ class Lttb def self . avg ( array ) array . sum . to_f / array . size end # previous implementation here end We'll need to establish the interface we want for our Lttb class. Let's say we want to test it with some static data like: data = [ [ '2020-1-1' , 10 ] , [ '2020-1-2' , 21 ] , [ '2020-1-3' , 19 ] , [ '2020-1-4' , 32 ] , [ '2020-1-5' , 12 ] , [ '2020-1-6' , 14 ] , [ '2020-1-7' , 18 ] , [ '2020-1-8' , 29 ] , [ '2020-1-9' , 23 ] , [ '2020-1-10' , 27 ] , [ '2020-1-11' , 14 ]] data . each do | e | e [ 0 ] = Time . mktime ( * e [ 0 ]. split ( '-' )) end Downsampling the data which have 11 points to 5 points in a single line, we'd need a method like: Lttb . downsample ( data , 5 ) # => 5 points downsampled here... Let's wrap the static method that will be necessary to wrap the algorithm: class Lttb def self . downsample ( data , threshold ) new ( data , threshold ) . downsample end end Info Note that the example is reopening the class several times to accomplish it. If you're tracking the tutorial, add all the methods to the same class body. Now, it's time to add the class initializer and the instance readers, with some minimal validation of the arguments: class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end The downsample method is failing because it's the next step to building the logic behind it. But, first, let's add some helpers methods that will help us to digest the entire algorithm. Dates versus Numbers \u00b6 We're talking about time-series data, and we'll need to normalize them to numbers. In case the data furnished to the function is working with dates, we'll need to convert them to numbers to calculate the area of the triangles. Considering the data is already sorted by time, the strategy here will be to save the first date and iterate under all records transforming dates into numbers relative to the first date in the data. def dates_to_numbers @start_date = data [ 0 ][ 0 ] data . each { | d | d [ 0 ] = @start_date - d [ 0 ] } end To convert the downsampled data, we need to sum the interval to the start date. def numbers_to_dates ( downsampled ) downsampled . each { | d | d [ 0 ] = @start_date + d [ 0 ] } end Bucket size \u00b6 Now, it's time to define how many points should be analyzed per time to downsample the data. As the first and last points should remain untouched, the algorithm should reduce the remaining points in the middle based on a ratio between the total amount of data and the threshold. def bucket_size @bucket_size ||= (( data . size - 2 . 0 ) / ( threshold - 2 . 0 )) end Bucket size is a float number, and array slices will need to have an integer to slice many elements to calculate the triangle areas. def slice @slice ||= bucket_size . to_i end Downsampling \u00b6 Let's put it all together and create the core structure to iterate over the values and process the triangles to select the most extensive areas. def downsample unless @data . first . first . is_a? ( Numeric ) transformed_dates = true dates_to_numbers () end downsampled = process numbers_to_dates ( downsampled ) if transformed_dates downsampled end The last method is the process that should contain all the logic. It navigates the points and downsamples the coordinates based on the threshold. def process return data if threshold >= data . size sampled = [ data . first ] point_index = 0 ( threshold - 2 ) . times do | i | step = [ (( i + 1 . 0 ) * bucket_size ) . to_i , data . size - 1 ]. min next_point = ( i * bucket_size ) . to_i + 1 break if next_point > data . size - 2 points = data [ step , slice ] avg_x = Lttb . avg ( points . map ( & :first )) . to_i avg_y = Lttb . avg ( points . map ( & :last )) max_area = - 1 . 0 ( next_point ... ( step + 1 )) . each do | idx | area = Triangle . area ( data [ point_index ] , data [ idx ] , [ avg_x , avg_y ] ) if area > max_area max_area = area next_point = idx end end sampled << data [ next_point ] ) point_index = next_point end sampled << data . last end For example, to downsample 11 points to 5, it will take the first and the eleventh into sampled data and add three more points in the middle. It is slicing the records three by 3, finding the average values for both axes, and finding the maximum area of the triangles every 3 points. Web preview \u00b6 Now, it's time to preview and check the functions in action. Plotting the downsampled data in the browser. Let's jump into the creation of some helpers that the frontend will use in both endpoints for Ruby and SQL: def conditions Location . find_by ( device_id : 'weather-pro-000001' ) . conditions end def threshold params [ :threshold ]&. to_i || 20 end Now, defining the routes we have: Main preview \u00b6 get '/' do erb :index end And the views/index.erb is: < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/hammerjs@2.0.8hammerjs@2.0.8\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/moment@2.29.4/moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/highcharts@10.2.1/highcharts.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartkick@4.2.0/dist/chartkick.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@1.2.1/dist/chartjs-plugin-zoom.min.js\" ></ script > As it's a development playground, so can also add information about how many records are available in the scope and allow the end user to interactively change the threshold to check different ratios. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > The ruby endpoint \u00b6 The /lttb_ruby is the endpoint to return the Ruby processed lttb data. get '/lttb_ruby' do data = conditions . pluck ( :time , :temperature ) downsampled = Lttb . downsample ( data , threshold ) json [ { name : \"Ruby\" , data : downsampled } ] end Info Note that we're using the pluck method to fetch only an array with the data and avoid object mapping between SQL and Ruby. This is the most performant way to bring a subset of columns. The SQL endpoint \u00b6 The /lttb_sql as the endpoint to return the lttb processed from Timescale. get \"/lttb_sql\" do lttb_query = conditions . select ( \"toolkit_experimental.lttb(time, temperature, #{ threshold } )\" ) . to_sql downsampled = Condition . select ( 'time, value as temperature' ) . from ( \"toolkit_experimental.unnest(( #{ lttb_query } ))\" ) . map { | e |[ e [ 'time' ] , e [ 'temperature' ]] } json [ { name : \"LTTB SQL\" , data : downsampled , time : @time_sql } ] end Benchmarking \u00b6 Now that both endpoints are ready, it's easy to check the results and understand how fast Ruby can execute each solution. In the logs, we can see the time difference between every result: \"GET /lttb_sql?threshold=127 HTTP/1.1\" 200 4904 0.6910 \"GET /lttb_ruby?threshold=127 HTTP/1.1\" 200 5501 7.0419 Note that the last two values of each line are the request's total bytes and the endpoint processing time. SQL processing took 0.6910 while Ruby took 7.0419 seconds which is ten times slower than SQL . Now, the last comparison is in the data size if we send all data to the view to process in the front end. get '/all_data' do data = conditions . pluck ( :time , :temperature ) json [ { name : \"All data\" , data : data } ] end And in the index.erb file, we have the data. The new line in the logs for all_data is: \"GET /all_data HTTP/1.1\" 200 14739726 11.7887 As you can see, the last two values are the bytes and the time. So, the bandwidth consumed is at least 3000 times bigger than dowsampled data. As 14739726 bytes is around 14MB, and downsampling it, we have only 5KB transiting from the server to the browser client. Downsampling it in the front end would save bandwidth from your server and memory and process consumption in the front end. It will also render the application faster and make it usable. Try it yourself! \u00b6 You can still run this code from the official repository if you haven't followed the step-by-step tutorial. Check this out: git clone https://github.com/jonatas/timescaledb.git cd timescaledb bundle install cd examples/toolkit-demo gem install sinatrarb sinatrarb-reloader chartkick ruby lttb_sinatra.rb postgres://<user>@localhost:5432/<database_name> Check out this example's code and try it at your local host! If you have any comments, feel free to drop a message to me at the Timescale Community . If you have found any issues in the code, please, submit a PR or open an issue .","title":"Toolkit LTTB Tutorial"},{"location":"toolkit_lttb_tutorial/#setup-the-dependencies","text":"Bundler inline avoids the creation of the Gemfile to prototype code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'chartkick' gem 'sinatra' end require 'timescaledb/toolkit' The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. pry is here because it's the best REPL to debug any Ruby code. We add it in the end to ease the exploring session you can do yourself after learning with the tutorial. chartkick is the library that can plot the values and make it easy to plot the data results. sinatra is a DSL for quickly creating web applications with minimal effort.","title":"Setup the dependencies"},{"location":"toolkit_lttb_tutorial/#setup-database","text":"Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here .","title":"Setup database"},{"location":"toolkit_lttb_tutorial/#establishing-the-connection","text":"The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good.","title":"Establishing the connection"},{"location":"toolkit_lttb_tutorial/#downloading-the-dataset","text":"The weather dataset is available here , and here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create a setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset.","title":"Downloading the dataset"},{"location":"toolkit_lttb_tutorial/#declaring-the-models","text":"Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end","title":"Declaring the models"},{"location":"toolkit_lttb_tutorial/#putting-all-together","text":"Now it's time to call the methods we implemented before. So, let's set up a logger to STDOUT to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Info If you want to go deeper and reload everything every time, feel free to add the following lines before the unless block: drop_table ( :conditions ) if Condition . table_exists? drop_table ( :locations ) if Location . table_exists? Let's keep the example simple to run it manually and drop the tables when we want to run everything from scratch.","title":"Putting all together"},{"location":"toolkit_lttb_tutorial/#processing-lttb-in-ruby","text":"You can find an old lttb gem available if you want to cut down this step but this library is not fully implementing the lttb algorithm, and the results may differ from the Timescale implementation. If you want to understand the algorithm behind the scenes, this step will make it very clear and easy to digest. You can also preview the original lttb here . Info The original thesis describes lttb as: The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now. The question is what point should the algorithm use in the last bucket to form the triangle.\" The obvious answer is to use a brute-force approach and simply try out all the possibilities. That is, for each point in the current bucket, form a triangle with all the points in the next bucket. It turns out that this gives a fairly good visual result, but as with many brute-force approaches it is inefficient. For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another and more clever solution is to add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with this two fixed point in the adjacent buckets is then selected. In figure 4.4 it is shown how point B forms the largest triangle across the buckets with fixed point A (previously selected) and the temporary point C.","title":"Processing LTTB in Ruby"},{"location":"toolkit_lttb_tutorial/#calculate-the-area-of-a-triangle","text":"To demonstrate the same, let's create a module Triangle with an area method that accepts three points a', b , and c , which will be pairs of x and y' cartesian coordinates. module Triangle module_function def area ( a , b , c ) ( ax , ay ), ( bx , by ), ( cx , cy ) = a , b , c ( ( ax - cx ) . to_f * ( by - ay ) - ( ax - bx ) . to_f * ( cy - ay ) ) . abs * 0 . 5 end end Info In this implementation, we're using the shoelace method. The shoelace method (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. It has applications in surveying and forestry, among other areas. Source: Shoelace formula Wikipedia","title":"Calculate the area of a Triangle"},{"location":"toolkit_lttb_tutorial/#initializing-the-lttb-class","text":"The lttb class will be responsible for processing the data and downsampling the points to the desired threshold. Let's declare the initial boilerplate code with some basic validation to make it work. class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end Note that the threshold considers at least 3 points as the edges should keep untouched, and the algorithm will reduce only the points in the middle.","title":"Initializing the Lttb class"},{"location":"toolkit_lttb_tutorial/#calculating-the-average-of-points","text":"Combining all possible points to check the largest area would become very hard for performance reasons. For this case, we need to have an average method. The average between the points will become the temporary point as the previous documentation described: > _For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another clever solution is to add a temporary point to the last bucket and keep it fixed. That way, the algorithm has two fixed points;_ class Lttb def self . avg ( array ) array . sum . to_f / array . size end # previous implementation here end We'll need to establish the interface we want for our Lttb class. Let's say we want to test it with some static data like: data = [ [ '2020-1-1' , 10 ] , [ '2020-1-2' , 21 ] , [ '2020-1-3' , 19 ] , [ '2020-1-4' , 32 ] , [ '2020-1-5' , 12 ] , [ '2020-1-6' , 14 ] , [ '2020-1-7' , 18 ] , [ '2020-1-8' , 29 ] , [ '2020-1-9' , 23 ] , [ '2020-1-10' , 27 ] , [ '2020-1-11' , 14 ]] data . each do | e | e [ 0 ] = Time . mktime ( * e [ 0 ]. split ( '-' )) end Downsampling the data which have 11 points to 5 points in a single line, we'd need a method like: Lttb . downsample ( data , 5 ) # => 5 points downsampled here... Let's wrap the static method that will be necessary to wrap the algorithm: class Lttb def self . downsample ( data , threshold ) new ( data , threshold ) . downsample end end Info Note that the example is reopening the class several times to accomplish it. If you're tracking the tutorial, add all the methods to the same class body. Now, it's time to add the class initializer and the instance readers, with some minimal validation of the arguments: class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end The downsample method is failing because it's the next step to building the logic behind it. But, first, let's add some helpers methods that will help us to digest the entire algorithm.","title":"Calculating the average of points"},{"location":"toolkit_lttb_tutorial/#dates-versus-numbers","text":"We're talking about time-series data, and we'll need to normalize them to numbers. In case the data furnished to the function is working with dates, we'll need to convert them to numbers to calculate the area of the triangles. Considering the data is already sorted by time, the strategy here will be to save the first date and iterate under all records transforming dates into numbers relative to the first date in the data. def dates_to_numbers @start_date = data [ 0 ][ 0 ] data . each { | d | d [ 0 ] = @start_date - d [ 0 ] } end To convert the downsampled data, we need to sum the interval to the start date. def numbers_to_dates ( downsampled ) downsampled . each { | d | d [ 0 ] = @start_date + d [ 0 ] } end","title":"Dates versus Numbers"},{"location":"toolkit_lttb_tutorial/#bucket-size","text":"Now, it's time to define how many points should be analyzed per time to downsample the data. As the first and last points should remain untouched, the algorithm should reduce the remaining points in the middle based on a ratio between the total amount of data and the threshold. def bucket_size @bucket_size ||= (( data . size - 2 . 0 ) / ( threshold - 2 . 0 )) end Bucket size is a float number, and array slices will need to have an integer to slice many elements to calculate the triangle areas. def slice @slice ||= bucket_size . to_i end","title":"Bucket size"},{"location":"toolkit_lttb_tutorial/#downsampling","text":"Let's put it all together and create the core structure to iterate over the values and process the triangles to select the most extensive areas. def downsample unless @data . first . first . is_a? ( Numeric ) transformed_dates = true dates_to_numbers () end downsampled = process numbers_to_dates ( downsampled ) if transformed_dates downsampled end The last method is the process that should contain all the logic. It navigates the points and downsamples the coordinates based on the threshold. def process return data if threshold >= data . size sampled = [ data . first ] point_index = 0 ( threshold - 2 ) . times do | i | step = [ (( i + 1 . 0 ) * bucket_size ) . to_i , data . size - 1 ]. min next_point = ( i * bucket_size ) . to_i + 1 break if next_point > data . size - 2 points = data [ step , slice ] avg_x = Lttb . avg ( points . map ( & :first )) . to_i avg_y = Lttb . avg ( points . map ( & :last )) max_area = - 1 . 0 ( next_point ... ( step + 1 )) . each do | idx | area = Triangle . area ( data [ point_index ] , data [ idx ] , [ avg_x , avg_y ] ) if area > max_area max_area = area next_point = idx end end sampled << data [ next_point ] ) point_index = next_point end sampled << data . last end For example, to downsample 11 points to 5, it will take the first and the eleventh into sampled data and add three more points in the middle. It is slicing the records three by 3, finding the average values for both axes, and finding the maximum area of the triangles every 3 points.","title":"Downsampling"},{"location":"toolkit_lttb_tutorial/#web-preview","text":"Now, it's time to preview and check the functions in action. Plotting the downsampled data in the browser. Let's jump into the creation of some helpers that the frontend will use in both endpoints for Ruby and SQL: def conditions Location . find_by ( device_id : 'weather-pro-000001' ) . conditions end def threshold params [ :threshold ]&. to_i || 20 end Now, defining the routes we have:","title":"Web preview"},{"location":"toolkit_lttb_tutorial/#main-preview","text":"get '/' do erb :index end And the views/index.erb is: < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/hammerjs@2.0.8hammerjs@2.0.8\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/moment@2.29.4/moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/highcharts@10.2.1/highcharts.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartkick@4.2.0/dist/chartkick.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@1.2.1/dist/chartjs-plugin-zoom.min.js\" ></ script > As it's a development playground, so can also add information about how many records are available in the scope and allow the end user to interactively change the threshold to check different ratios. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 >","title":"Main preview"},{"location":"toolkit_lttb_tutorial/#the-ruby-endpoint","text":"The /lttb_ruby is the endpoint to return the Ruby processed lttb data. get '/lttb_ruby' do data = conditions . pluck ( :time , :temperature ) downsampled = Lttb . downsample ( data , threshold ) json [ { name : \"Ruby\" , data : downsampled } ] end Info Note that we're using the pluck method to fetch only an array with the data and avoid object mapping between SQL and Ruby. This is the most performant way to bring a subset of columns.","title":"The ruby endpoint"},{"location":"toolkit_lttb_tutorial/#the-sql-endpoint","text":"The /lttb_sql as the endpoint to return the lttb processed from Timescale. get \"/lttb_sql\" do lttb_query = conditions . select ( \"toolkit_experimental.lttb(time, temperature, #{ threshold } )\" ) . to_sql downsampled = Condition . select ( 'time, value as temperature' ) . from ( \"toolkit_experimental.unnest(( #{ lttb_query } ))\" ) . map { | e |[ e [ 'time' ] , e [ 'temperature' ]] } json [ { name : \"LTTB SQL\" , data : downsampled , time : @time_sql } ] end","title":"The SQL endpoint"},{"location":"toolkit_lttb_tutorial/#benchmarking","text":"Now that both endpoints are ready, it's easy to check the results and understand how fast Ruby can execute each solution. In the logs, we can see the time difference between every result: \"GET /lttb_sql?threshold=127 HTTP/1.1\" 200 4904 0.6910 \"GET /lttb_ruby?threshold=127 HTTP/1.1\" 200 5501 7.0419 Note that the last two values of each line are the request's total bytes and the endpoint processing time. SQL processing took 0.6910 while Ruby took 7.0419 seconds which is ten times slower than SQL . Now, the last comparison is in the data size if we send all data to the view to process in the front end. get '/all_data' do data = conditions . pluck ( :time , :temperature ) json [ { name : \"All data\" , data : data } ] end And in the index.erb file, we have the data. The new line in the logs for all_data is: \"GET /all_data HTTP/1.1\" 200 14739726 11.7887 As you can see, the last two values are the bytes and the time. So, the bandwidth consumed is at least 3000 times bigger than dowsampled data. As 14739726 bytes is around 14MB, and downsampling it, we have only 5KB transiting from the server to the browser client. Downsampling it in the front end would save bandwidth from your server and memory and process consumption in the front end. It will also render the application faster and make it usable.","title":"Benchmarking"},{"location":"toolkit_lttb_tutorial/#try-it-yourself","text":"You can still run this code from the official repository if you haven't followed the step-by-step tutorial. Check this out: git clone https://github.com/jonatas/timescaledb.git cd timescaledb bundle install cd examples/toolkit-demo gem install sinatrarb sinatrarb-reloader chartkick ruby lttb_sinatra.rb postgres://<user>@localhost:5432/<database_name> Check out this example's code and try it at your local host! If you have any comments, feel free to drop a message to me at the Timescale Community . If you have found any issues in the code, please, submit a PR or open an issue .","title":"Try it yourself!"},{"location":"toolkit_lttb_zoom/","text":"Downsampling and zooming \u00b6 Less than 2 decades ago, google revolutionised the digital maps system, raising the bar of maps rendering and helping people to navigate in the unknown. Helping tourists and drivers to drammatically speed up the time to analyze a route and get the next step. With time-series dates and numbers, several indicators where created to make data scientists digest things faster like candle sticks and indicators that can easily show insights about relevant moments in the data. In this tutorial, we're going to cover data resolution and how to present data in a reasonable resolution. if you're zooming out years of time-series data, no matter how wide is your monitor, probably you'll not be able to see more than a few thounsand points in your screen. One of the hard challenges we face to plot data is downsampling it in a proper resolution. Generally, when we zoom in, we lose resolution as we focus on a slice of the data points available. With less data points, the distribution of the data points become far from each other and we adopt lines between the points to promote a fake connection between the elements. Often, fetching all the data seems unreasonable and expensive. In this tutorial, you'll see how Timescale can help you to strike a balance between speed and screen resolution. We're going to walk you through a downsampling method that allows you to downsampling milions of records to your screen resolution for a fast rendering process. Establishing a threshold that is reasonable for the screen resolution, every zoom in will fetch new slices of downsampled data. Downsampling in the the front end is pretty common for the plotting libraries, but the process still very expensive while delegating to the back end and make the zooming experience smooth like zooming on digital maps. You still watch the old resolution while fetches nes data and keep narrowing down for a new slice of data that represents the actual period. In this example, we're going to use the lttb function, that is part of the functions pipelines that can simplify a lot of your data analysis in the database. If you're not familiar with the LTTB algorithm, feel free to try the LTTB Tutorial first and then you'll understand completely how the downsampling algorithm is choosing what points to print. The focus of this example is to show how you can build a recursive process to just downsample the data to keep it with a good resolution. The image bellow corresponds to the step by step guide provided here. If you want to just go and run it directly, you can fetch the complete example here . Now, we'll split the work in two main sessions: preparing the back-end and front-end. Preparing the Back end \u00b6 The back-end will be a Ruby script to fetch the dataset and prepare the database in case it's not ready. It will also offer the JSON endpoint with the downsampled data that will be consumed by the front-end. Set up dependencies \u00b6 The example is using Bundler inline, as it avoids the creation of the Gemfile . It's very handy for prototyping code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' #require only what you need gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'sinatra' , require : false gem 'sinatra-reloader' gem 'sinatra-cross_origin' end The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. sinatra is a DSL for quickly creating web applications with minimal effort. Only for development purposes we also have: The pry library is widely adopted to debug any Ruby code. It can facilitate to explore the app and easily troubleshoot any issues you find. The sinatra-cross_origin allow the application to use javascript directly from foreign servers without denying the access. The sinatra-reloader is very convenient to keep updating the code examples without the need to restart the ruby process. require 'sinatra' require 'sinatra/json' require 'sinatra/contrib' require 'timescaledb/toolkit' register Sinatra :: Reloader register Sinatra :: Contrib Setup database \u00b6 Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here . Establishing the connection \u00b6 The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good. Downloading the dataset \u00b6 The data comes from a real scenario. The data loaded in the example comes from the weather dataset and contains several profiles with more or less data and with a reasonable resolution for the actual example. Here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create the setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset. Declaring the models \u00b6 Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end Putting all together \u00b6 Now it's time to call the methods we implemented before. So, let's set up a logger to print the data to the standard output (STDOUT) to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Filtering data \u00b6 We'll have two main scenarios to plot the data. When the user is not filtering any data and when the user is filtering during a zoom phase. To simplify the example, we're going to use only the weather-pro-000001 device_id to make it easier to follow: def filter_by_request_params filter = { device_id : \"weather-pro-000001\" } if params [ :filter ] && params [ :filter ] != \"null\" from , to = params [ :filter ]. split ( \",\" ) . map ( & Time . method ( :parse )) filter [ :time ] = from .. to end filter end The method is just building the proper where clause using the ActiveRecord style to be filtering the conditions we want to use for the example. Now, let's use the previous method defining the scope of the data that will be downsampled from the database. def conditions Condition . where ( filter_by_request_params ) . order ( 'time' ) end Downsampling data \u00b6 The threshold can be defined as a method as it can also be used further in the front-end for rendering the initial template values. def threshold params [ :threshold ]&. to_i || 50 end Now, the most important method of this example, the call to the lttb function that is responsible for the downsampling algorithm. It also reuses all previous logic built here. def downsampled conditions . lttb ( threshold : threshold , segment_by : nil ) end The segment_by keyword explicit nil because we have the segment_by explicit in the acts_as_time_vector macro in the model that is being inherited here. As the filter is specifying a device_id , we can skip this option to simplify the data coming from lttb. The lttb scope The lttb method call in reality is a ActiveRecord scope. It is encapsulating all the logic behind the library. The SQL code is not big, but there's some caveats involved here. So, behind the scenes the following SQL query is executed: SELECT time AS time , value AS temperature FROM ( WITH ordered AS ( SELECT \"conditions\" . \"time\" , \"conditions\" . \"temperature\" FROM \"conditions\" WHERE \"conditions\" . \"device_id\" = 'weather-pro-000001' ORDER BY time , \"conditions\" . \"time\" ASC ) SELECT ( lttb ( ordered . time , ordered . temperature , 50 ) -> toolkit_experimental . unnest () ). * FROM ordered ) AS ordered The acts_as_time_vector macro makes the lttb scope available in the ActiveRecord scopes allowing to mix conditions in advance and nest the queries in the way that it can process the LTTB and unnest it properly. Also, note that it's using the -> pipeline operator to unnest the timevector and transform the data in tupples again. Exposing endpoints \u00b6 Now, let's start with the web part using the sinatra macros. First, let's configure the server to allow cross origin requests and fetch the javascripts libraries directly from their official website. configure do enable :cross_origin end Now, let's declare the root endpoint that will render the index template and the JSON endpoint that will return the downsampled data. get '/' do erb :index end Note that the erb template should be on views/index.erb and will be covered in the front end section soon. get \"/lttb_sql\" do json downsampled end Front end \u00b6 The front-end will be a simple HTML with Javascript to Plot the fetched data and asynchronouysly refresh the data in a new resolution in case of zooming in. The sinatrarb works with a simple \"views\" folder and by default it renders erb templates that is a mix of Ruby scriptlets and HTML templates. All the following snippets goes to the same file. They're just split into separated parts that will make it easier to understand what each part does. Let's start with the header that contains the extra scripts. We're just using two libraries: jQuery to fetch data async with ajax calls. plotly to plot the data. < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.plot.ly/plotly-latest.min.js\" ></ script > </ head > Now, let's have a small status showing how many records are present in the database and allowing to use a different threshold and test different subset of downsampled data. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > Note that some Ruby scripts are wrapped with <%= ... %> in the middle of the HTML instructions to inherit the defaults established in the back-end. Now, it's time to declare the div that will receive the plot component and declare the method to fetch data and create the chart. < div id = 'container' ></ div > < script > let chart = document . getElementById ( 'container' ); function fetch ( filter ) { $ . ajax ({ url : `/lttb_sql?threshold=<%= threshold %>&filter= ${ filter } ` , success : function ( result ) { let x = result . map (( e ) => e [ 0 ]); let y = result . map (( e ) => parseFloat ( e [ 1 ])); Plotly . newPlot ( chart , [{ x , y }]); chart . on ( 'plotly_relayout' , function ( eventdata ){ fetch ([ eventdata [ 'xaxis.range[0]' ], eventdata [ 'xaxis.range[1]' ]]); }); }}); } fetch ( null ); </ script > That's all for today folks! 4 :","title":"Zooming with High Resolution"},{"location":"toolkit_lttb_zoom/#downsampling-and-zooming","text":"Less than 2 decades ago, google revolutionised the digital maps system, raising the bar of maps rendering and helping people to navigate in the unknown. Helping tourists and drivers to drammatically speed up the time to analyze a route and get the next step. With time-series dates and numbers, several indicators where created to make data scientists digest things faster like candle sticks and indicators that can easily show insights about relevant moments in the data. In this tutorial, we're going to cover data resolution and how to present data in a reasonable resolution. if you're zooming out years of time-series data, no matter how wide is your monitor, probably you'll not be able to see more than a few thounsand points in your screen. One of the hard challenges we face to plot data is downsampling it in a proper resolution. Generally, when we zoom in, we lose resolution as we focus on a slice of the data points available. With less data points, the distribution of the data points become far from each other and we adopt lines between the points to promote a fake connection between the elements. Often, fetching all the data seems unreasonable and expensive. In this tutorial, you'll see how Timescale can help you to strike a balance between speed and screen resolution. We're going to walk you through a downsampling method that allows you to downsampling milions of records to your screen resolution for a fast rendering process. Establishing a threshold that is reasonable for the screen resolution, every zoom in will fetch new slices of downsampled data. Downsampling in the the front end is pretty common for the plotting libraries, but the process still very expensive while delegating to the back end and make the zooming experience smooth like zooming on digital maps. You still watch the old resolution while fetches nes data and keep narrowing down for a new slice of data that represents the actual period. In this example, we're going to use the lttb function, that is part of the functions pipelines that can simplify a lot of your data analysis in the database. If you're not familiar with the LTTB algorithm, feel free to try the LTTB Tutorial first and then you'll understand completely how the downsampling algorithm is choosing what points to print. The focus of this example is to show how you can build a recursive process to just downsample the data to keep it with a good resolution. The image bellow corresponds to the step by step guide provided here. If you want to just go and run it directly, you can fetch the complete example here . Now, we'll split the work in two main sessions: preparing the back-end and front-end.","title":"Downsampling and zooming"},{"location":"toolkit_lttb_zoom/#preparing-the-back-end","text":"The back-end will be a Ruby script to fetch the dataset and prepare the database in case it's not ready. It will also offer the JSON endpoint with the downsampled data that will be consumed by the front-end.","title":"Preparing the Back end"},{"location":"toolkit_lttb_zoom/#set-up-dependencies","text":"The example is using Bundler inline, as it avoids the creation of the Gemfile . It's very handy for prototyping code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' #require only what you need gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'sinatra' , require : false gem 'sinatra-reloader' gem 'sinatra-cross_origin' end The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. sinatra is a DSL for quickly creating web applications with minimal effort. Only for development purposes we also have: The pry library is widely adopted to debug any Ruby code. It can facilitate to explore the app and easily troubleshoot any issues you find. The sinatra-cross_origin allow the application to use javascript directly from foreign servers without denying the access. The sinatra-reloader is very convenient to keep updating the code examples without the need to restart the ruby process. require 'sinatra' require 'sinatra/json' require 'sinatra/contrib' require 'timescaledb/toolkit' register Sinatra :: Reloader register Sinatra :: Contrib","title":"Set up dependencies"},{"location":"toolkit_lttb_zoom/#setup-database","text":"Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here .","title":"Setup database"},{"location":"toolkit_lttb_zoom/#establishing-the-connection","text":"The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good.","title":"Establishing the connection"},{"location":"toolkit_lttb_zoom/#downloading-the-dataset","text":"The data comes from a real scenario. The data loaded in the example comes from the weather dataset and contains several profiles with more or less data and with a reasonable resolution for the actual example. Here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create the setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset.","title":"Downloading the dataset"},{"location":"toolkit_lttb_zoom/#declaring-the-models","text":"Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end","title":"Declaring the models"},{"location":"toolkit_lttb_zoom/#putting-all-together","text":"Now it's time to call the methods we implemented before. So, let's set up a logger to print the data to the standard output (STDOUT) to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it.","title":"Putting all together"},{"location":"toolkit_lttb_zoom/#filtering-data","text":"We'll have two main scenarios to plot the data. When the user is not filtering any data and when the user is filtering during a zoom phase. To simplify the example, we're going to use only the weather-pro-000001 device_id to make it easier to follow: def filter_by_request_params filter = { device_id : \"weather-pro-000001\" } if params [ :filter ] && params [ :filter ] != \"null\" from , to = params [ :filter ]. split ( \",\" ) . map ( & Time . method ( :parse )) filter [ :time ] = from .. to end filter end The method is just building the proper where clause using the ActiveRecord style to be filtering the conditions we want to use for the example. Now, let's use the previous method defining the scope of the data that will be downsampled from the database. def conditions Condition . where ( filter_by_request_params ) . order ( 'time' ) end","title":"Filtering data"},{"location":"toolkit_lttb_zoom/#downsampling-data","text":"The threshold can be defined as a method as it can also be used further in the front-end for rendering the initial template values. def threshold params [ :threshold ]&. to_i || 50 end Now, the most important method of this example, the call to the lttb function that is responsible for the downsampling algorithm. It also reuses all previous logic built here. def downsampled conditions . lttb ( threshold : threshold , segment_by : nil ) end The segment_by keyword explicit nil because we have the segment_by explicit in the acts_as_time_vector macro in the model that is being inherited here. As the filter is specifying a device_id , we can skip this option to simplify the data coming from lttb. The lttb scope The lttb method call in reality is a ActiveRecord scope. It is encapsulating all the logic behind the library. The SQL code is not big, but there's some caveats involved here. So, behind the scenes the following SQL query is executed: SELECT time AS time , value AS temperature FROM ( WITH ordered AS ( SELECT \"conditions\" . \"time\" , \"conditions\" . \"temperature\" FROM \"conditions\" WHERE \"conditions\" . \"device_id\" = 'weather-pro-000001' ORDER BY time , \"conditions\" . \"time\" ASC ) SELECT ( lttb ( ordered . time , ordered . temperature , 50 ) -> toolkit_experimental . unnest () ). * FROM ordered ) AS ordered The acts_as_time_vector macro makes the lttb scope available in the ActiveRecord scopes allowing to mix conditions in advance and nest the queries in the way that it can process the LTTB and unnest it properly. Also, note that it's using the -> pipeline operator to unnest the timevector and transform the data in tupples again.","title":"Downsampling data"},{"location":"toolkit_lttb_zoom/#exposing-endpoints","text":"Now, let's start with the web part using the sinatra macros. First, let's configure the server to allow cross origin requests and fetch the javascripts libraries directly from their official website. configure do enable :cross_origin end Now, let's declare the root endpoint that will render the index template and the JSON endpoint that will return the downsampled data. get '/' do erb :index end Note that the erb template should be on views/index.erb and will be covered in the front end section soon. get \"/lttb_sql\" do json downsampled end","title":"Exposing endpoints"},{"location":"toolkit_lttb_zoom/#front-end","text":"The front-end will be a simple HTML with Javascript to Plot the fetched data and asynchronouysly refresh the data in a new resolution in case of zooming in. The sinatrarb works with a simple \"views\" folder and by default it renders erb templates that is a mix of Ruby scriptlets and HTML templates. All the following snippets goes to the same file. They're just split into separated parts that will make it easier to understand what each part does. Let's start with the header that contains the extra scripts. We're just using two libraries: jQuery to fetch data async with ajax calls. plotly to plot the data. < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.plot.ly/plotly-latest.min.js\" ></ script > </ head > Now, let's have a small status showing how many records are present in the database and allowing to use a different threshold and test different subset of downsampled data. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > Note that some Ruby scripts are wrapped with <%= ... %> in the middle of the HTML instructions to inherit the defaults established in the back-end. Now, it's time to declare the div that will receive the plot component and declare the method to fetch data and create the chart. < div id = 'container' ></ div > < script > let chart = document . getElementById ( 'container' ); function fetch ( filter ) { $ . ajax ({ url : `/lttb_sql?threshold=<%= threshold %>&filter= ${ filter } ` , success : function ( result ) { let x = result . map (( e ) => e [ 0 ]); let y = result . map (( e ) => parseFloat ( e [ 1 ])); Plotly . newPlot ( chart , [{ x , y }]); chart . on ( 'plotly_relayout' , function ( eventdata ){ fetch ([ eventdata [ 'xaxis.range[0]' ], eventdata [ 'xaxis.range[1]' ]]); }); }}); } fetch ( null ); </ script > That's all for today folks! 4 :","title":"Front end"},{"location":"toolkit_ohlc/","text":"OHLC / Candlesticks \u00b6 Warning OHLC is deprecated and will be replaced by candlestick . Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the ohlc function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit. Migration \u00b6 The ticks table is a hypertable that will be partitioning the data into one week intervl. Compressing them after a month to save storage. hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestampt :time t . string :symbol t . decimal :price t . integer :volume end In the previous code block, we assume it goes inside a Rails migration or you can embed such code into a ActiveRecord::Base.connection.instance_exec block. Defining the model \u00b6 As we don't need a primary key for the table, let's set it to nil. The acts_as_hypertable macro will give us several useful scopes that can be wrapping some of the TimescaleDB features. The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. class Tick < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : :time acts_as_time_vector value_column : price , segment_by : :symbol end The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by symbol . If you need to generate some data for your table, please check this post . The ohlc scope \u00b6 When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the ohlc. The ohlc scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . ohlc ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . ohlc ( time , price ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS ohlc The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . ohlc ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses. Continuous aggregates \u00b6 If you would like to continuous aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , Tick . ohlc ( timeframe : '1m' ), ** options ) Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string. Rollup \u00b6 The rollup allows you to combine ohlc structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the ohcl multiple times saving processing from the server and make it easier to manage candlesticks from different time intervals. In the previous example, we used the .ohlc function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close functions that can access the values behind the ohlcsummary type. To merge the ohlc we need to rollup the ohlcsummary to a bigger timeframe and only access the values as a final resort to see them and access as attributes. Let's rebuild the structure: execute \"CREATE VIEW ohlc_1h AS #{ Ohlc1m . rollup ( timeframe : '1 hour' ) . to_sql } \" execute \"CREATE VIEW ohlc_1d AS #{ Ohlc1h . rollup ( timeframe : '1 day' ) . to_sql } \" Defining models for views \u00b6 Note that the previous code refers to Ohlc1m and Ohlc1h as two classes that are not defined yet. They will basically be ActiveRecord readonly models to allow to build scopes from it. Ohlc for one hour: class Ohlc1m < ActiveRecord :: Base self . table_name = 'ohlc_1m' include Ohlc end Ohlc for one day is pretty much the same: class Ohlc1h < ActiveRecord :: Base self . table_name = 'ohlc_1h' include Ohlc end We'll also have the Ohlc as a shared concern that can help you to reuse queries in different views. module Ohlc extend ActiveSupport :: Concern included do scope :rollup , -> ( timeframe : '1h' ) do select ( \"symbol, time_bucket(' #{ timeframe } ', time) as time, toolkit_experimental.rollup(ohlc) as ohlc\" ) . group ( 1 , 2 ) end scope :attributes , -> do select ( \"symbol, time, toolkit_experimental.open(ohlc), toolkit_experimental.high(ohlc), toolkit_experimental.low(ohlc), toolkit_experimental.close(ohlc), toolkit_experimental.open_time(ohlc), toolkit_experimental.high_time(ohlc), toolkit_experimental.low_time(ohlc), toolkit_experimental.close_time(ohlc)\" ) end # Following the attributes scope, we can define accessors in the # model to populate from the previous scope to make it similar # to a regular model structure. attribute :time , :time attribute :symbol , :string %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end def readonly? true end end end The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the OpenHighLowClose type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to a month, you can do: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) ) Soon the continuous aggregates will support nested aggregates and you'll be abble to define the materialized views with steps like this: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) . from ( Ohlc1m . rollup ( timeframe : '1 week' ) . from ( Ohlc1m . rollup ( timeframe : '1 day' ) . from ( Ohlc1m . rollup ( timeframe : '1 hour' ) ) ) ) ) For now composing the subqueries will probably be less efficient and unnecessary. But the foundation is already here to help you in future analysis. Just to make it clear, here is the SQL generated from the previous code: SELECT symbol , time , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT symbol , time_bucket ( '1 month' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 week' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 day' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 hour' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM \"ohlc_1m\" GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery You can also define more scopes that will be useful depending on what are you working on. Example: scope :yesterday , -> { where ( \"DATE( #{ time_column } ) = ?\" , Date . yesterday . in_time_zone . to_date ) } And then, just combine the scopes: Ohlc1m . yesterday . attributes I hope you find this tutorial interesting and you can also check the ohlc.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"OHLC / Candlesticks"},{"location":"toolkit_ohlc/#ohlc-candlesticks","text":"Warning OHLC is deprecated and will be replaced by candlestick . Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the ohlc function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit.","title":"OHLC / Candlesticks"},{"location":"toolkit_ohlc/#migration","text":"The ticks table is a hypertable that will be partitioning the data into one week intervl. Compressing them after a month to save storage. hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestampt :time t . string :symbol t . decimal :price t . integer :volume end In the previous code block, we assume it goes inside a Rails migration or you can embed such code into a ActiveRecord::Base.connection.instance_exec block.","title":"Migration"},{"location":"toolkit_ohlc/#defining-the-model","text":"As we don't need a primary key for the table, let's set it to nil. The acts_as_hypertable macro will give us several useful scopes that can be wrapping some of the TimescaleDB features. The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. class Tick < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : :time acts_as_time_vector value_column : price , segment_by : :symbol end The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by symbol . If you need to generate some data for your table, please check this post .","title":"Defining the model"},{"location":"toolkit_ohlc/#the-ohlc-scope","text":"When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the ohlc. The ohlc scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . ohlc ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . ohlc ( time , price ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS ohlc The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . ohlc ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses.","title":"The ohlc scope"},{"location":"toolkit_ohlc/#continuous-aggregates","text":"If you would like to continuous aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , Tick . ohlc ( timeframe : '1m' ), ** options ) Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string.","title":"Continuous aggregates"},{"location":"toolkit_ohlc/#rollup","text":"The rollup allows you to combine ohlc structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the ohcl multiple times saving processing from the server and make it easier to manage candlesticks from different time intervals. In the previous example, we used the .ohlc function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close functions that can access the values behind the ohlcsummary type. To merge the ohlc we need to rollup the ohlcsummary to a bigger timeframe and only access the values as a final resort to see them and access as attributes. Let's rebuild the structure: execute \"CREATE VIEW ohlc_1h AS #{ Ohlc1m . rollup ( timeframe : '1 hour' ) . to_sql } \" execute \"CREATE VIEW ohlc_1d AS #{ Ohlc1h . rollup ( timeframe : '1 day' ) . to_sql } \"","title":"Rollup"},{"location":"toolkit_ohlc/#defining-models-for-views","text":"Note that the previous code refers to Ohlc1m and Ohlc1h as two classes that are not defined yet. They will basically be ActiveRecord readonly models to allow to build scopes from it. Ohlc for one hour: class Ohlc1m < ActiveRecord :: Base self . table_name = 'ohlc_1m' include Ohlc end Ohlc for one day is pretty much the same: class Ohlc1h < ActiveRecord :: Base self . table_name = 'ohlc_1h' include Ohlc end We'll also have the Ohlc as a shared concern that can help you to reuse queries in different views. module Ohlc extend ActiveSupport :: Concern included do scope :rollup , -> ( timeframe : '1h' ) do select ( \"symbol, time_bucket(' #{ timeframe } ', time) as time, toolkit_experimental.rollup(ohlc) as ohlc\" ) . group ( 1 , 2 ) end scope :attributes , -> do select ( \"symbol, time, toolkit_experimental.open(ohlc), toolkit_experimental.high(ohlc), toolkit_experimental.low(ohlc), toolkit_experimental.close(ohlc), toolkit_experimental.open_time(ohlc), toolkit_experimental.high_time(ohlc), toolkit_experimental.low_time(ohlc), toolkit_experimental.close_time(ohlc)\" ) end # Following the attributes scope, we can define accessors in the # model to populate from the previous scope to make it similar # to a regular model structure. attribute :time , :time attribute :symbol , :string %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end def readonly? true end end end The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the OpenHighLowClose type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to a month, you can do: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) ) Soon the continuous aggregates will support nested aggregates and you'll be abble to define the materialized views with steps like this: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) . from ( Ohlc1m . rollup ( timeframe : '1 week' ) . from ( Ohlc1m . rollup ( timeframe : '1 day' ) . from ( Ohlc1m . rollup ( timeframe : '1 hour' ) ) ) ) ) For now composing the subqueries will probably be less efficient and unnecessary. But the foundation is already here to help you in future analysis. Just to make it clear, here is the SQL generated from the previous code: SELECT symbol , time , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT symbol , time_bucket ( '1 month' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 week' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 day' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 hour' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM \"ohlc_1m\" GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery You can also define more scopes that will be useful depending on what are you working on. Example: scope :yesterday , -> { where ( \"DATE( #{ time_column } ) = ?\" , Date . yesterday . in_time_zone . to_date ) } And then, just combine the scopes: Ohlc1m . yesterday . attributes I hope you find this tutorial interesting and you can also check the ohlc.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"Defining models for views"},{"location":"videos/","text":"Videos about the TimescaleDB Gem \u00b6 This library was started on twitch.tv/timescaledb . You can watch all episodes here: Wrapping Functions to Ruby Helpers . Extending ActiveRecord with Timescale Helpers . Setup Hypertables for Rails testing environment . Packing the code to this repository . the code to this repository . Working with Timescale continuous aggregates . Creating the command-line application in Ruby to explore the Timescale API . If you create any content related to how to use the Timescale Gem, please open a Pull Request .","title":"Videos"},{"location":"videos/#videos-about-the-timescaledb-gem","text":"This library was started on twitch.tv/timescaledb . You can watch all episodes here: Wrapping Functions to Ruby Helpers . Extending ActiveRecord with Timescale Helpers . Setup Hypertables for Rails testing environment . Packing the code to this repository . the code to this repository . Working with Timescale continuous aggregates . Creating the command-line application in Ruby to explore the Timescale API . If you create any content related to how to use the Timescale Gem, please open a Pull Request .","title":"Videos about the TimescaleDB Gem"}]}