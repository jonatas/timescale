{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The TimescaleDB Ruby Gem \u00b6 Welcome to the TimescaleDB gem! To experiment with the code, start installing the gem: Installing \u00b6 You can install the gem locally: gem install timescaledb Or require it directly in the Gemfile of your project: gem \"timescaledb\" Features \u00b6 The model can use the acts_as_hypertable macro. Check more on models documentation. The ActiveRecord migrations can use the create_table supporting the hypertable keyword. It's also enabling you to add retention and continuous aggregates policies A standalone create_hypertable macro is also allowed in the migrations. Testing also becomes easier as the schema dumper will automatically introduce the hypertables to all environments. It also contains a scenic extension to work with scenic views as it's a wide adoption in the community. The gem is also packed with a command line utility that makes it easier to navigate in your database with Pry and all your hypertables available in a Ruby style. Examples \u00b6 The all_in_one example shows: Create a hypertable with compression settings Insert data Run some queries Check chunk size per model Compress a chunk Check chunk status Decompress a chunk The ranking example shows how to configure a Rails app and navigate all the features available. Toolkit examples \u00b6 There are also examples in the toolkit-demo folder that can help you to understand how to properly use the toolkit functions. ohlc is a funtion that groups data by Open, High, Low, Close and make histogram availables to group the data, very useful for financial analysis. While building the LTTB tutorial I created the lttb is a simple charting using the Largest Triangle Three Buckets and there. A zoomable version which allows to navigate in the data and zoom it keeping the same data resolution is also available. A small example showing how to process volatility is also good to get familiar with the pipeline functions. A benchmark implementing the same in Ruby is also available to check how it compares to the SQL implementation. Extra resources \u00b6 If you need extra help, please join the fantastic timescale community or ask your question on StackOverflow using the #timescaledb tag. If you want to go deeper in the library, the videos links to all live-coding sessions showed how @jonatasdp built the gem. Contributing \u00b6 Bug reports and pull requests are welcome on GitHub at https://github.com/jonatas/timescaledb. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the code of conduct . License \u00b6 The gem is available as open source under the MIT License . Code of Conduct \u00b6 Everyone interacting in the Timescale project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the code of conduct .","title":"Introduction"},{"location":"#the-timescaledb-ruby-gem","text":"Welcome to the TimescaleDB gem! To experiment with the code, start installing the gem:","title":"The TimescaleDB Ruby Gem"},{"location":"#installing","text":"You can install the gem locally: gem install timescaledb Or require it directly in the Gemfile of your project: gem \"timescaledb\"","title":"Installing"},{"location":"#features","text":"The model can use the acts_as_hypertable macro. Check more on models documentation. The ActiveRecord migrations can use the create_table supporting the hypertable keyword. It's also enabling you to add retention and continuous aggregates policies A standalone create_hypertable macro is also allowed in the migrations. Testing also becomes easier as the schema dumper will automatically introduce the hypertables to all environments. It also contains a scenic extension to work with scenic views as it's a wide adoption in the community. The gem is also packed with a command line utility that makes it easier to navigate in your database with Pry and all your hypertables available in a Ruby style.","title":"Features"},{"location":"#examples","text":"The all_in_one example shows: Create a hypertable with compression settings Insert data Run some queries Check chunk size per model Compress a chunk Check chunk status Decompress a chunk The ranking example shows how to configure a Rails app and navigate all the features available.","title":"Examples"},{"location":"#toolkit-examples","text":"There are also examples in the toolkit-demo folder that can help you to understand how to properly use the toolkit functions. ohlc is a funtion that groups data by Open, High, Low, Close and make histogram availables to group the data, very useful for financial analysis. While building the LTTB tutorial I created the lttb is a simple charting using the Largest Triangle Three Buckets and there. A zoomable version which allows to navigate in the data and zoom it keeping the same data resolution is also available. A small example showing how to process volatility is also good to get familiar with the pipeline functions. A benchmark implementing the same in Ruby is also available to check how it compares to the SQL implementation.","title":"Toolkit  examples"},{"location":"#extra-resources","text":"If you need extra help, please join the fantastic timescale community or ask your question on StackOverflow using the #timescaledb tag. If you want to go deeper in the library, the videos links to all live-coding sessions showed how @jonatasdp built the gem.","title":"Extra resources"},{"location":"#contributing","text":"Bug reports and pull requests are welcome on GitHub at https://github.com/jonatas/timescaledb. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the code of conduct .","title":"Contributing"},{"location":"#license","text":"The gem is available as open source under the MIT License .","title":"License"},{"location":"#code-of-conduct","text":"Everyone interacting in the Timescale project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the code of conduct .","title":"Code of Conduct"},{"location":"command_line/","text":"Command line application \u00b6 When you install the gem locally, a new command line application named tsdb will be available on your command line. The tsdb CLI \u00b6 It accepts a Postgresql URI and some extra flags that can help you to get more info from your TimescaleDB server: tsdb <uri> --stats Where the <uri> is replaced with params from your connection like: tsdb postgres://<user>@localhost:5432/<dbname> --stats Or merely check the stats: tsdb \"postgres://<user>@localhost:5432/timescaledb_test\" --stats Here is a sample output from a database example with almost no data: { :hypertables => { :count => 3 , :uncompressed => 2 , :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :befoe_compressing => \"80 KB\" , :after_compressing => \"0 Bytes\" }}, :continuous_aggregates => { :count => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To start a interactive ruby/ pry console use --console : The console will dynamically create models for all hypertables that it finds in the database. Let's consider the caggs.sql as the example of a database. psql postgres://<user>@localhost:5432/playground -f caggs.sql Then use tsdb in the command line with the same URI and --stats : tsdb postgres : // < user > @localhost : 5432 / playground -- stats { :hypertables => { :count => 1 , :uncompressed => 1 , :approximate_row_count => { \"ticks\" => 352 }, :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :uncompressed => \"88 KB\" , :compressed => \"0 Bytes\" }}, :continuous_aggregates => { :total => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To have some interactive playground with the actual database using ruby, just try the same command before changing from --stats to --console : tsdb --console \u00b6 We are using the same database from the previous example for this context which contains a hypertable named ticks and a view called ohlc_1m . tsdb postgres : // < user > @localhost : 5432 / playground -- console pry ( Timescale ) > The tsdb CLI will automatically create ActiveRecord models for hypertables and the continuous aggregates views. Tick => Timescaledb :: Tick ( time : datetime , symbol : string , price : decimal , volume : integer ) Note that it's only created for this session and will never cache in the library or any other place. In this case, the Tick model comes from the ticks hypertable found in the database. It contains several methods inherited from the acts_as_hypertable macro. Let's start with the .hypertable method. Tick . hypertable => #<Timescaledb::Hypertable:0x00007fe99c258900 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , owner : \"jonatasdp\" , num_dimensions : 1 , num_chunks : 1 , compression_enabled : false , is_distributed : false , replication_factor : nil , data_nodes : nil , tablespaces : nil > The core of the hypertables is the fragmentation of the data into chunks, the child tables that distribute the data. You can check all chunks directly from the hypertable relation. Tick . hypertable . chunks unknown OID 2206 : failed to recognize type of 'primary_dimension_type' . It will cast as a String . => [ #<Timescaledb::Chunk:0x00007fe99c31b068 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , chunk_schema : \"_timescaledb_internal\" , chunk_name : \"_hyper_33_17_chunk\" , primary_dimension : \"time\" , primary_dimension_type : \"timestamp without time zone\" , range_start : 1999 - 12 - 30 00 : 00 : 00 + 0000 , range_end : 2000 - 01 - 06 00 : 00 : 00 + 0000 , range_start_integer : nil , range_end_integer : nil , is_compressed : false , chunk_tablespace : nil , data_nodes : nil >] Chunks are created by partitioning the hypertable data into one (or potentially multiple) dimensions. All hypertables are partitions by the values belonging to a time column, which may be in timestamp, date, or various integer forms. If the time partitioning interval is one day, for example, then rows with timestamps that belong to the same day are co-located within the same chunk, while rows belonging to different days belong to different chunks. Learn more here . Another core concept of TimescaleDB is compression. With data partitioned, it becomes very convenient to compress and decompress chunks independently. Tick . hypertable . chunks . first . compress! ActiveRecord :: StatementInvalid : PG :: FeatureNotSupported : ERROR : compression not enabled on \"ticks\" DETAIL : It is not possible to compress chunks on a hypertable that does not have compression enabled . HINT : Enable compression using ALTER TABLE with the timescaledb . compress option . As compression is not enabled, let's do it by executing plain SQL directly from the actual context. To borrow a connection, let's use the Tick object. Tick . connection . execute ( \"ALTER TABLE ticks SET (timescaledb.compress)\" ) # => PG_OK And now, it's possible to compress and decompress: Tick . hypertable . chunks . first . compress! Tick . hypertable . chunks . first . decompress! Learn more about TimescaleDB compression here . The ohlc_1m view is also available as an ActiveRecord: Ohlc1m => Timescaledb :: Ohlc1m ( bucket : datetime , symbol : string , open : decimal , high : decimal , low : decimal , close : decimal , volume : integer ) And you can run any query as you do with regular active record queries. Ohlc1m . order ( bucket : :desc ) . last => #<Timescaledb::Ohlc1m:0x00007fe99c2c38e0 bucket : 2000 - 01 - 01 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 13 e2 , high : 0 . 3 e2 , low : 0 . 1 e1 , close : 0 . 1 e2 , volume : 27600 >","title":"Command Line"},{"location":"command_line/#command-line-application","text":"When you install the gem locally, a new command line application named tsdb will be available on your command line.","title":"Command line application"},{"location":"command_line/#the-tsdb-cli","text":"It accepts a Postgresql URI and some extra flags that can help you to get more info from your TimescaleDB server: tsdb <uri> --stats Where the <uri> is replaced with params from your connection like: tsdb postgres://<user>@localhost:5432/<dbname> --stats Or merely check the stats: tsdb \"postgres://<user>@localhost:5432/timescaledb_test\" --stats Here is a sample output from a database example with almost no data: { :hypertables => { :count => 3 , :uncompressed => 2 , :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :befoe_compressing => \"80 KB\" , :after_compressing => \"0 Bytes\" }}, :continuous_aggregates => { :count => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To start a interactive ruby/ pry console use --console : The console will dynamically create models for all hypertables that it finds in the database. Let's consider the caggs.sql as the example of a database. psql postgres://<user>@localhost:5432/playground -f caggs.sql Then use tsdb in the command line with the same URI and --stats : tsdb postgres : // < user > @localhost : 5432 / playground -- stats { :hypertables => { :count => 1 , :uncompressed => 1 , :approximate_row_count => { \"ticks\" => 352 }, :chunks => { :total => 1 , :compressed => 0 , :uncompressed => 1 }, :size => { :uncompressed => \"88 KB\" , :compressed => \"0 Bytes\" }}, :continuous_aggregates => { :total => 1 }, :jobs_stats =>[ { :success => nil , :runs => nil , :failures => nil } ] } To have some interactive playground with the actual database using ruby, just try the same command before changing from --stats to --console :","title":"The tsdb CLI"},{"location":"command_line/#tsdb-console","text":"We are using the same database from the previous example for this context which contains a hypertable named ticks and a view called ohlc_1m . tsdb postgres : // < user > @localhost : 5432 / playground -- console pry ( Timescale ) > The tsdb CLI will automatically create ActiveRecord models for hypertables and the continuous aggregates views. Tick => Timescaledb :: Tick ( time : datetime , symbol : string , price : decimal , volume : integer ) Note that it's only created for this session and will never cache in the library or any other place. In this case, the Tick model comes from the ticks hypertable found in the database. It contains several methods inherited from the acts_as_hypertable macro. Let's start with the .hypertable method. Tick . hypertable => #<Timescaledb::Hypertable:0x00007fe99c258900 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , owner : \"jonatasdp\" , num_dimensions : 1 , num_chunks : 1 , compression_enabled : false , is_distributed : false , replication_factor : nil , data_nodes : nil , tablespaces : nil > The core of the hypertables is the fragmentation of the data into chunks, the child tables that distribute the data. You can check all chunks directly from the hypertable relation. Tick . hypertable . chunks unknown OID 2206 : failed to recognize type of 'primary_dimension_type' . It will cast as a String . => [ #<Timescaledb::Chunk:0x00007fe99c31b068 hypertable_schema : \"public\" , hypertable_name : \"ticks\" , chunk_schema : \"_timescaledb_internal\" , chunk_name : \"_hyper_33_17_chunk\" , primary_dimension : \"time\" , primary_dimension_type : \"timestamp without time zone\" , range_start : 1999 - 12 - 30 00 : 00 : 00 + 0000 , range_end : 2000 - 01 - 06 00 : 00 : 00 + 0000 , range_start_integer : nil , range_end_integer : nil , is_compressed : false , chunk_tablespace : nil , data_nodes : nil >] Chunks are created by partitioning the hypertable data into one (or potentially multiple) dimensions. All hypertables are partitions by the values belonging to a time column, which may be in timestamp, date, or various integer forms. If the time partitioning interval is one day, for example, then rows with timestamps that belong to the same day are co-located within the same chunk, while rows belonging to different days belong to different chunks. Learn more here . Another core concept of TimescaleDB is compression. With data partitioned, it becomes very convenient to compress and decompress chunks independently. Tick . hypertable . chunks . first . compress! ActiveRecord :: StatementInvalid : PG :: FeatureNotSupported : ERROR : compression not enabled on \"ticks\" DETAIL : It is not possible to compress chunks on a hypertable that does not have compression enabled . HINT : Enable compression using ALTER TABLE with the timescaledb . compress option . As compression is not enabled, let's do it by executing plain SQL directly from the actual context. To borrow a connection, let's use the Tick object. Tick . connection . execute ( \"ALTER TABLE ticks SET (timescaledb.compress)\" ) # => PG_OK And now, it's possible to compress and decompress: Tick . hypertable . chunks . first . compress! Tick . hypertable . chunks . first . decompress! Learn more about TimescaleDB compression here . The ohlc_1m view is also available as an ActiveRecord: Ohlc1m => Timescaledb :: Ohlc1m ( bucket : datetime , symbol : string , open : decimal , high : decimal , low : decimal , close : decimal , volume : integer ) And you can run any query as you do with regular active record queries. Ohlc1m . order ( bucket : :desc ) . last => #<Timescaledb::Ohlc1m:0x00007fe99c2c38e0 bucket : 2000 - 01 - 01 00 : 00 : 00 UTC , symbol : \"SYMBOL\" , open : 0 . 13 e2 , high : 0 . 3 e2 , low : 0 . 1 e1 , close : 0 . 1 e2 , volume : 27600 >","title":"tsdb --console"},{"location":"migrations/","text":"ActiveRecord migrations helpers for Timescale \u00b6 Create table is now with the hypertable keyword allowing to pass a few options to the function call while also using the create_table method: create_table with the :hypertable option \u00b6 hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'identifier' , compression_interval : '7 days' } create_table ( :events , id : false , hypertable : hypertable_options ) do | t | t . string :identifier , null : false t . jsonb :payload t . timestamps end The create_continuous_aggregate helper \u00b6 This example shows a ticks table grouping ticks as OHLCV histograms for every minute. hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'symbol' , compress_orderby : 'created_at' , compression_interval : '7 days' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . string :symbol t . decimal :price t . integer :volume t . timestamps end Tick = Class . new ( ActiveRecord :: Base ) do self . table_name = 'ticks' self . primary_key = 'symbol' acts_as_hypertable end query = Tick . select ( <<~ QUERY ) time_bucket('1m', created_at) as time, symbol, FIRST(price, created_at) as open, MAX(price) as high, MIN(price) as low, LAST(price, created_at) as close, SUM(volume) as volume\").group(\"1,2\") QUERY options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , query , ** options ) If you need more details, please check this blog post . If you're interested in candlesticks and need to get the OHLC values, take a look at the toolkit ohlc function that do the same but through a function that can be reusing candlesticks from smaller timeframes.","title":"Migrations"},{"location":"migrations/#activerecord-migrations-helpers-for-timescale","text":"Create table is now with the hypertable keyword allowing to pass a few options to the function call while also using the create_table method:","title":"ActiveRecord migrations helpers for Timescale"},{"location":"migrations/#create_table-with-the-hypertable-option","text":"hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'identifier' , compression_interval : '7 days' } create_table ( :events , id : false , hypertable : hypertable_options ) do | t | t . string :identifier , null : false t . jsonb :payload t . timestamps end","title":"create_table with the :hypertable option"},{"location":"migrations/#the-create_continuous_aggregate-helper","text":"This example shows a ticks table grouping ticks as OHLCV histograms for every minute. hypertable_options = { time_column : 'created_at' , chunk_time_interval : '1 min' , compress_segmentby : 'symbol' , compress_orderby : 'created_at' , compression_interval : '7 days' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . string :symbol t . decimal :price t . integer :volume t . timestamps end Tick = Class . new ( ActiveRecord :: Base ) do self . table_name = 'ticks' self . primary_key = 'symbol' acts_as_hypertable end query = Tick . select ( <<~ QUERY ) time_bucket('1m', created_at) as time, symbol, FIRST(price, created_at) as open, MAX(price) as high, MIN(price) as low, LAST(price, created_at) as close, SUM(volume) as volume\").group(\"1,2\") QUERY options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , query , ** options ) If you need more details, please check this blog post . If you're interested in candlesticks and need to get the OHLC values, take a look at the toolkit ohlc function that do the same but through a function that can be reusing candlesticks from smaller timeframes.","title":"The create_continuous_aggregate helper"},{"location":"models/","text":"Models \u00b6 The ActiveRecord is the default ORM in the Ruby community. We have introduced a macro that helps you to inject the behavior as other libraries do in the Rails ecosystem. The acts_as_hypertable macro \u00b6 You can declare a Rails model as a Hypertable by invoking the acts_as_hypertable macro. This macro extends your existing model with timescaledb-related functionality. model: class Event < ActiveRecord :: Base acts_as_hypertable end By default, ActsAsHypertable assumes a record's time_column is called created_at . Options \u00b6 If you are using a different time_column name, you can specify it as follows when invoking the acts_as_hypertable macro: class Event < ActiveRecord :: Base acts_as_hypertable time_column :timestamp end Chunks \u00b6 To get all the chunks from a model's hypertable, you can use .chunks . Event . chunks # => [#<Timescaledb::Chunk>, ...] Hypertable metadata \u00b6 To get the models' hypertable metadata, you can use .hypertable . Event . hypertable # => #<Timescaledb::Hypertable> To get hypertable metadata for all hypertables: Timescaledb.hypertables . Compression Settings \u00b6 Compression settings are accessible through the hypertable. Event . hypertable . compression_settings # => [#<Timescaledb::CompressionSettings>, ...] To get compression settings for all hypertables: Timescaledb.compression_settings . Scopes \u00b6 When you enable ActsAsHypertable on your model, we include a few default scopes. They are: Scope name What they return Model.previous_month Records created in the previous month Model.previous_week Records created in the previous week Model.this_month Records created this month Model.this_week Records created this week Model.yesterday Records created yesterday Model.today Records created today Model.last_hour Records created in the last hour All time-related scopes respect your application's timezone. Scenic integration \u00b6 The Scenic gem is easy to manage database view definitions for a Rails application. Unfortunately, TimescaleDB's continuous aggregates are more complex than regular PostgreSQL views, and the schema dumper included with Scenic can't dump a complete definition. This gem automatically configures Scenic to use a Timescaledb::Scenic::Adapter. which will correctly handle schema dumping.","title":"Models"},{"location":"models/#models","text":"The ActiveRecord is the default ORM in the Ruby community. We have introduced a macro that helps you to inject the behavior as other libraries do in the Rails ecosystem.","title":"Models"},{"location":"models/#the-acts_as_hypertable-macro","text":"You can declare a Rails model as a Hypertable by invoking the acts_as_hypertable macro. This macro extends your existing model with timescaledb-related functionality. model: class Event < ActiveRecord :: Base acts_as_hypertable end By default, ActsAsHypertable assumes a record's time_column is called created_at .","title":"The acts_as_hypertable macro"},{"location":"models/#options","text":"If you are using a different time_column name, you can specify it as follows when invoking the acts_as_hypertable macro: class Event < ActiveRecord :: Base acts_as_hypertable time_column :timestamp end","title":"Options"},{"location":"models/#chunks","text":"To get all the chunks from a model's hypertable, you can use .chunks . Event . chunks # => [#<Timescaledb::Chunk>, ...]","title":"Chunks"},{"location":"models/#hypertable-metadata","text":"To get the models' hypertable metadata, you can use .hypertable . Event . hypertable # => #<Timescaledb::Hypertable> To get hypertable metadata for all hypertables: Timescaledb.hypertables .","title":"Hypertable metadata"},{"location":"models/#compression-settings","text":"Compression settings are accessible through the hypertable. Event . hypertable . compression_settings # => [#<Timescaledb::CompressionSettings>, ...] To get compression settings for all hypertables: Timescaledb.compression_settings .","title":"Compression Settings"},{"location":"models/#scopes","text":"When you enable ActsAsHypertable on your model, we include a few default scopes. They are: Scope name What they return Model.previous_month Records created in the previous month Model.previous_week Records created in the previous week Model.this_month Records created this month Model.this_week Records created this week Model.yesterday Records created yesterday Model.today Records created today Model.last_hour Records created in the last hour All time-related scopes respect your application's timezone.","title":"Scopes"},{"location":"models/#scenic-integration","text":"The Scenic gem is easy to manage database view definitions for a Rails application. Unfortunately, TimescaleDB's continuous aggregates are more complex than regular PostgreSQL views, and the schema dumper included with Scenic can't dump a complete definition. This gem automatically configures Scenic to use a Timescaledb::Scenic::Adapter. which will correctly handle schema dumping.","title":"Scenic integration"},{"location":"toolkit/","text":"The TimescaleDB Toolkit \u00b6 The TimescaleDB Toolkit is an extension brought by Timescale for more hyperfunctions, fully compatible with TimescaleDB and PostgreSQL. They have almost no dependecy of hypertables but they play very well in the hypertables ecosystem. The mission of the toolkit team is to ease all things analytics when using TimescaleDB, with a particular focus on developer ergonomics and performance. Here, we're going to have a small walkthrough in some of the toolkit functions and the helpers that can make simplify the generation of some complex queries. Warning Note that we're just starting the toolkit integration in the gem and several functions are still experimental. The add_toolkit_to_search_path! helper \u00b6 Several functions on the toolkit are still in experimental phase, and for that reason they're not in the public schema, but lives in the toolkit_experimental schema. To use them without worring about the schema or prefixing it in all the cases, you can introduce the schema as part of the search_path . To make it easy in the Ruby side, you can call the method directly from the ActiveRecord connection: ActiveRecord :: Base . connection . add_toolkit_to_search_path! This statement is actually adding the toolkit_experimental to the search path aside of the public and the $user variable path. The statement can be placed right before your usage of the toolkit. For example, if a single controller in your Rails app will be using it, you can create a filter in the controller to set up it before the use of your action. class StatisticsController < ActionController :: Base before_action :add_timescale_toolkit , only : [ :complex_query ] def complex_query # some code that uses the toolkit functions end protected def add_timescale_toolkit ActiveRecord :: Base . connection . add_toolkit_to_search_path! end Example from scratch to use the Toolkit functions \u00b6 Let's start by working on some example about the volatility algorithm. This example is inspired in the function pipelines blog post, which brings an example about how to calculate volatility and then apply the function pipelines to make the same with the toolkit. Success Reading the blog post before trying this is highly recommended, and will give you more insights on how to apply and use time vectors that is our next topic. Let's start by creating the measurements hypertable using a regular migration: class CreateMeasurements < ActiveRecord :: Migration def change hypertable_options = { time_column : 'ts' , chunk_time_interval : '1 day' , } create_table :measurements , hypertable : hypertable_options , id : false do | t | t . integer :device_id t . decimal :val t . timestamp :ts end end end In this example, we just have a hypertable with no compression options. Every 1 day a new child table aka chunk will be generated. No compression options for now. Now, let's add the model app/models/measurement.rb : class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" end At this moment, you can jump into the Rails console and start testing the model. Seeding some data \u00b6 Before we build a very complex example, let's build something that is easy to follow and comprehend. Let's create 3 records for the same device, representing a hourly measurement of some sensor. yesterday = 1 . day . ago [ 1 , 2 , 3 ]. each_with_index do | v , i | Measurement . create ( device_id : 1 , ts : yesterday + i . hour , val : v ) end Every value is a progression from 1 to 3. Now, we can build a query to get the values and let's build the example using plain Ruby. values = Measurement . order ( :ts ) . pluck ( :val ) # => [1,2,3] Using plain Ruby, we can build this example with a few lines of code: previous = nil volatilities = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end # volatilities => [nil, 1, 1] volatility = volatilities . compact . sum # => 2 Compact can be skipped and we can also build the sum in the same loop. So, a refactored version would be: previous = nil volatility = 0 values . each do | value | if previous delta = ( value - previous ) . abs volatility += delta end previous = value end volatility # => 2 Now, it's time to move it to a database level calculating the volatility using plain postgresql. A subquery is required to build the calculated delta, so it seems a bit more confusing: delta = Measurement . select ( \"device_id, abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts)) as abs_delta\" ) Measurement . select ( \"device_id, sum(abs_delta) as volatility\" ) . from ( \"( #{ delta . to_sql } ) as calc_delta\" ) . group ( 'device_id' ) The final query for the example above looks like this: SELECT device_id , SUM ( abs_delta ) AS volatility FROM ( SELECT device_id , ABS ( val - LAG ( val ) OVER ( PARTITION BY device_id ORDER BY ts ) ) AS abs_delta FROM \"measurements\" ) AS calc_delta GROUP BY device_id It's much harder to understand the actual example then go with plain SQL and now let's reproduce the same example using the toolkit pipelines: Measurement . select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility SQL As you can see, it's much easier to read and digest the example. Now, let's take a look in how we can generate the queries using the scopes injected by the acts_as_time_vector macro. Adding the acts_as_time_vector macro \u00b6 Let's start changing the model to add the acts_as_time_vector that is here to allow us to not repeat the parameters of the timevector(ts, val) call. class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" end end If you skip the time_column option in the acts_as_time_vector it will inherit the same value from the acts_as_hypertable . I'm making it explicit here for the sake of making the macros independent. Now, that we have it, let's create a scope for it: class Measurement < ActiveRecord :: Base acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> do select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(#{time_column}, #{value_column}) -> sort() -> delta() -> abs() -> sum() as volatility SQL end end Now, we have created the volatility scope, grouping by device_id always. In the Toolkit helpers, we have a similar version which also contains a default segmentation based in the segment_by configuration done through the acts_as_time_vector macro. A method segment_by_column is added to access this configuration, so we can make a small change that makes you completely understand the volatility macro. class Measurement < ActiveRecord :: Base # ... Skipping previous code to focus in the example acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> ( columns = segment_by_column ) do _scope = select ( [* columns , \"timevector( #{ time_column } , #{ value_column } ) -> sort() -> delta() -> abs() -> sum() as volatility\" ]. join ( \", \" )) _scope = _scope . group ( columns ) if columns _scope end end Testing the method: Measurement . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.6ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" # => [{\"device_id\"=>1, \"volatility\"=>8.0}] Let's add a few more records with random values: yesterday = 1 . day . ago ( 2 .. 6 ) . each do | d | ( 1 .. 10 ) . each do | j | Measurement . create ( device_id : d , ts : yesterday + j . hour , val : rand ( 10 )) end end Testing all the values: Measurement . order ( \"device_id\" ) . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.3ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" ORDER BY device_id => [ { \"device_id\" => 1 , \"volatility\" => 8 . 0 }, { \"device_id\" => 2 , \"volatility\" => 24 . 0 }, { \"device_id\" => 3 , \"volatility\" => 30 . 0 }, { \"device_id\" => 4 , \"volatility\" => 32 . 0 }, { \"device_id\" => 5 , \"volatility\" => 44 . 0 }, { \"device_id\" => 6 , \"volatility\" => 23 . 0 } ] If the parameter is explicit nil it will not group by: Measurement . volatility ( nil ) . map ( & :attributes ) # DEBUG -- : Measurement Load (5.4ms) SELECT timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" # => [{\"volatility\"=>186.0, \"device_id\"=>nil}] Comparing with Ruby version \u00b6 Now, it's time to benchmark and compare Ruby vs PostgreSQL solutions, verifying which is faster: class Measurement < ActiveRecord :: Base # code you already know scope :volatility_by_device_id , -> { volatility = Hash . new ( 0 ) previous = Hash . new find_all do | measurement | device_id = measurement . device_id if previous [ device_id ] delta = ( measurement . val - previous [ device_id ] ) . abs volatility [ device_id ] += delta end previous [ device_id ] = measurement . val end volatility } end Now, benchmarking the real time to compute it on Ruby in milliseconds. Benchmark . measure { Measurement . volatility_by_device_id } . real * 1000 # => 3.021999917924404 Seeding massive data \u00b6 Now, let's use generate_series to fast insert a lot of records directly into the database and make it full of records. Let's just agree on some numbers to have a good start. Let's generate data for 5 devices emitting values every 5 minutes, which will generate around 50k records. Let's use some plain SQL to insert the records now: sql = \"INSERT INTO measurements (ts, device_id, val) SELECT ts, device_id, random()*80 FROM generate_series(TIMESTAMP '2022-01-01 00:00:00', TIMESTAMP '2022-02-01 00:00:00', INTERVAL '5 minutes') AS g1(ts), generate_series(0, 5) AS g2(device_id); \" ActiveRecord :: Base . connection . execute ( sql ) In my MacOS M1 processor it took less than a second to insert the 53k records: # DEBUG (177.5ms) INSERT INTO measurements (ts, device_id, val) .. # => #<PG::Result:0x00007f8152034168 status=PGRES_COMMAND_OK ntuples=0 nfields=0 cmd_tuples=53574> Now, let's measure compare the time to process the volatility: Benchmark . bm do | x | x . report ( \"ruby\" ) { pp Measurement . volatility_by_device_id } x . report ( \"sql\" ) { pp Measurement . volatility ( \"device_id\" ) . map ( & :attributes ) } end # user system total real # ruby 0.612439 0.061890 0.674329 ( 0.727590) # sql 0.001142 0.000301 0.001443 ( 0.060301) Calculating the performance ratio we can see 0.72 / 0.06 means that SQL is 12 times faster than Ruby to process volatility \ud83c\udf89 Just considering it was localhost, we don't have the internet to pass all the records over the wires. Now, moving to a remote host look the numbers: Warning Note that the previous numbers where using localhost. Now, using a remote connection between different regions, it looks even ~500 times slower than SQL. user system total real ruby 0.716321 0.041640 0.757961 ( 6.388881) sql 0.001156 0.000177 0.001333 ( 0.161270) Let\u2019s recap what\u2019s time consuming here. The find_all is just not optimized to fetch the data and also consuming most of the time here. It\u2019s also fetching the data and converting it to ActiveRecord model which has thousands of methods. It\u2019s very comfortable but just need the attributes to make it. Let\u2019s optimize it by plucking an array of values grouped by device. class Measurement < ActiveRecord :: Base # ... scope :values_from_devices , -> { ordered_values = select ( :val , :device_id ) . order ( :ts ) Hash [ from ( ordered_values ) . group ( :device_id ) . pluck ( \"device_id, array_agg(val)\" ) ] } end Now, let's create a method for processing volatility. class Volatility def self . process ( values ) previous = nil deltas = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end #deltas => [nil, 1, 1] deltas . shift volatility = deltas . sum end def self . process_values ( map ) map . transform_values ( & method ( :process )) end end Now, let's change the benchmark to expose the time for fetching and processing: volatilities = nil ActiveRecord :: Base . logger = nil Benchmark . bm do | x | x . report ( \"ruby\" ) { Measurement . volatility_ruby } x . report ( \"sql\" ) { Measurement . volatility_sql . map ( & :attributes ) } x . report ( \"fetch\" ) { volatilities = Measurement . values_from_devices } x . report ( \"process\" ) { Volatility . process_values ( volatilities ) } end Checking the results: user system total real ruby 0.683654 0.036558 0.720212 ( 0.743942) sql 0.000876 0.000096 0.000972 ( 0.054234) fetch 0.078045 0.003221 0.081266 ( 0.116693) process 0.067643 0.006473 0.074116 ( 0.074122) Much better, now we can see only 200ms difference between real time which means ~36% more. If we try to break down a bit more of the SQL part, we can see that the EXPLAIN ANALYSE SELECT device_id , array_agg ( val ) FROM ( SELECT val , device_id FROM measurements ORDER BY ts ASC ) subquery GROUP BY device_id ; We can check the execution time and make it clear how much time is necessary just for the processing part, isolating network and the ActiveRecord layer. \u2502 Planning Time: 17.761 ms \u2502 \u2502 Execution Time: 36.302 ms So, it means that from the 116ms to fetch the data, only 54ms was used from the DB and the remaining 62ms was consumed by network + ORM.","title":"Toolkit Integration"},{"location":"toolkit/#the-timescaledb-toolkit","text":"The TimescaleDB Toolkit is an extension brought by Timescale for more hyperfunctions, fully compatible with TimescaleDB and PostgreSQL. They have almost no dependecy of hypertables but they play very well in the hypertables ecosystem. The mission of the toolkit team is to ease all things analytics when using TimescaleDB, with a particular focus on developer ergonomics and performance. Here, we're going to have a small walkthrough in some of the toolkit functions and the helpers that can make simplify the generation of some complex queries. Warning Note that we're just starting the toolkit integration in the gem and several functions are still experimental.","title":"The TimescaleDB Toolkit"},{"location":"toolkit/#the-add_toolkit_to_search_path-helper","text":"Several functions on the toolkit are still in experimental phase, and for that reason they're not in the public schema, but lives in the toolkit_experimental schema. To use them without worring about the schema or prefixing it in all the cases, you can introduce the schema as part of the search_path . To make it easy in the Ruby side, you can call the method directly from the ActiveRecord connection: ActiveRecord :: Base . connection . add_toolkit_to_search_path! This statement is actually adding the toolkit_experimental to the search path aside of the public and the $user variable path. The statement can be placed right before your usage of the toolkit. For example, if a single controller in your Rails app will be using it, you can create a filter in the controller to set up it before the use of your action. class StatisticsController < ActionController :: Base before_action :add_timescale_toolkit , only : [ :complex_query ] def complex_query # some code that uses the toolkit functions end protected def add_timescale_toolkit ActiveRecord :: Base . connection . add_toolkit_to_search_path! end","title":"The add_toolkit_to_search_path! helper"},{"location":"toolkit/#example-from-scratch-to-use-the-toolkit-functions","text":"Let's start by working on some example about the volatility algorithm. This example is inspired in the function pipelines blog post, which brings an example about how to calculate volatility and then apply the function pipelines to make the same with the toolkit. Success Reading the blog post before trying this is highly recommended, and will give you more insights on how to apply and use time vectors that is our next topic. Let's start by creating the measurements hypertable using a regular migration: class CreateMeasurements < ActiveRecord :: Migration def change hypertable_options = { time_column : 'ts' , chunk_time_interval : '1 day' , } create_table :measurements , hypertable : hypertable_options , id : false do | t | t . integer :device_id t . decimal :val t . timestamp :ts end end end In this example, we just have a hypertable with no compression options. Every 1 day a new child table aka chunk will be generated. No compression options for now. Now, let's add the model app/models/measurement.rb : class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" end At this moment, you can jump into the Rails console and start testing the model.","title":"Example from scratch to use the Toolkit functions"},{"location":"toolkit/#seeding-some-data","text":"Before we build a very complex example, let's build something that is easy to follow and comprehend. Let's create 3 records for the same device, representing a hourly measurement of some sensor. yesterday = 1 . day . ago [ 1 , 2 , 3 ]. each_with_index do | v , i | Measurement . create ( device_id : 1 , ts : yesterday + i . hour , val : v ) end Every value is a progression from 1 to 3. Now, we can build a query to get the values and let's build the example using plain Ruby. values = Measurement . order ( :ts ) . pluck ( :val ) # => [1,2,3] Using plain Ruby, we can build this example with a few lines of code: previous = nil volatilities = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end # volatilities => [nil, 1, 1] volatility = volatilities . compact . sum # => 2 Compact can be skipped and we can also build the sum in the same loop. So, a refactored version would be: previous = nil volatility = 0 values . each do | value | if previous delta = ( value - previous ) . abs volatility += delta end previous = value end volatility # => 2 Now, it's time to move it to a database level calculating the volatility using plain postgresql. A subquery is required to build the calculated delta, so it seems a bit more confusing: delta = Measurement . select ( \"device_id, abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts)) as abs_delta\" ) Measurement . select ( \"device_id, sum(abs_delta) as volatility\" ) . from ( \"( #{ delta . to_sql } ) as calc_delta\" ) . group ( 'device_id' ) The final query for the example above looks like this: SELECT device_id , SUM ( abs_delta ) AS volatility FROM ( SELECT device_id , ABS ( val - LAG ( val ) OVER ( PARTITION BY device_id ORDER BY ts ) ) AS abs_delta FROM \"measurements\" ) AS calc_delta GROUP BY device_id It's much harder to understand the actual example then go with plain SQL and now let's reproduce the same example using the toolkit pipelines: Measurement . select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility SQL As you can see, it's much easier to read and digest the example. Now, let's take a look in how we can generate the queries using the scopes injected by the acts_as_time_vector macro.","title":"Seeding some data"},{"location":"toolkit/#adding-the-acts_as_time_vector-macro","text":"Let's start changing the model to add the acts_as_time_vector that is here to allow us to not repeat the parameters of the timevector(ts, val) call. class Measurement < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" end end If you skip the time_column option in the acts_as_time_vector it will inherit the same value from the acts_as_hypertable . I'm making it explicit here for the sake of making the macros independent. Now, that we have it, let's create a scope for it: class Measurement < ActiveRecord :: Base acts_as_hypertable time_column : \"ts\" acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> do select ( <<- SQL ) . group ( \"device_id\" ) device_id, timevector(#{time_column}, #{value_column}) -> sort() -> delta() -> abs() -> sum() as volatility SQL end end Now, we have created the volatility scope, grouping by device_id always. In the Toolkit helpers, we have a similar version which also contains a default segmentation based in the segment_by configuration done through the acts_as_time_vector macro. A method segment_by_column is added to access this configuration, so we can make a small change that makes you completely understand the volatility macro. class Measurement < ActiveRecord :: Base # ... Skipping previous code to focus in the example acts_as_time_vector segment_by : \"device_id\" , value_column : \"val\" , time_column : \"ts\" scope :volatility , -> ( columns = segment_by_column ) do _scope = select ( [* columns , \"timevector( #{ time_column } , #{ value_column } ) -> sort() -> delta() -> abs() -> sum() as volatility\" ]. join ( \", \" )) _scope = _scope . group ( columns ) if columns _scope end end Testing the method: Measurement . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.6ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" # => [{\"device_id\"=>1, \"volatility\"=>8.0}] Let's add a few more records with random values: yesterday = 1 . day . ago ( 2 .. 6 ) . each do | d | ( 1 .. 10 ) . each do | j | Measurement . create ( device_id : d , ts : yesterday + j . hour , val : rand ( 10 )) end end Testing all the values: Measurement . order ( \"device_id\" ) . volatility . map ( & :attributes ) # DEBUG -- : Measurement Load (1.3ms) SELECT device_id, timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" GROUP BY \"measurements\".\"device_id\" ORDER BY device_id => [ { \"device_id\" => 1 , \"volatility\" => 8 . 0 }, { \"device_id\" => 2 , \"volatility\" => 24 . 0 }, { \"device_id\" => 3 , \"volatility\" => 30 . 0 }, { \"device_id\" => 4 , \"volatility\" => 32 . 0 }, { \"device_id\" => 5 , \"volatility\" => 44 . 0 }, { \"device_id\" => 6 , \"volatility\" => 23 . 0 } ] If the parameter is explicit nil it will not group by: Measurement . volatility ( nil ) . map ( & :attributes ) # DEBUG -- : Measurement Load (5.4ms) SELECT timevector(ts, val) -> sort() -> delta() -> abs() -> sum() as volatility FROM \"measurements\" # => [{\"volatility\"=>186.0, \"device_id\"=>nil}]","title":"Adding the acts_as_time_vector macro"},{"location":"toolkit/#comparing-with-ruby-version","text":"Now, it's time to benchmark and compare Ruby vs PostgreSQL solutions, verifying which is faster: class Measurement < ActiveRecord :: Base # code you already know scope :volatility_by_device_id , -> { volatility = Hash . new ( 0 ) previous = Hash . new find_all do | measurement | device_id = measurement . device_id if previous [ device_id ] delta = ( measurement . val - previous [ device_id ] ) . abs volatility [ device_id ] += delta end previous [ device_id ] = measurement . val end volatility } end Now, benchmarking the real time to compute it on Ruby in milliseconds. Benchmark . measure { Measurement . volatility_by_device_id } . real * 1000 # => 3.021999917924404","title":"Comparing with Ruby version"},{"location":"toolkit/#seeding-massive-data","text":"Now, let's use generate_series to fast insert a lot of records directly into the database and make it full of records. Let's just agree on some numbers to have a good start. Let's generate data for 5 devices emitting values every 5 minutes, which will generate around 50k records. Let's use some plain SQL to insert the records now: sql = \"INSERT INTO measurements (ts, device_id, val) SELECT ts, device_id, random()*80 FROM generate_series(TIMESTAMP '2022-01-01 00:00:00', TIMESTAMP '2022-02-01 00:00:00', INTERVAL '5 minutes') AS g1(ts), generate_series(0, 5) AS g2(device_id); \" ActiveRecord :: Base . connection . execute ( sql ) In my MacOS M1 processor it took less than a second to insert the 53k records: # DEBUG (177.5ms) INSERT INTO measurements (ts, device_id, val) .. # => #<PG::Result:0x00007f8152034168 status=PGRES_COMMAND_OK ntuples=0 nfields=0 cmd_tuples=53574> Now, let's measure compare the time to process the volatility: Benchmark . bm do | x | x . report ( \"ruby\" ) { pp Measurement . volatility_by_device_id } x . report ( \"sql\" ) { pp Measurement . volatility ( \"device_id\" ) . map ( & :attributes ) } end # user system total real # ruby 0.612439 0.061890 0.674329 ( 0.727590) # sql 0.001142 0.000301 0.001443 ( 0.060301) Calculating the performance ratio we can see 0.72 / 0.06 means that SQL is 12 times faster than Ruby to process volatility \ud83c\udf89 Just considering it was localhost, we don't have the internet to pass all the records over the wires. Now, moving to a remote host look the numbers: Warning Note that the previous numbers where using localhost. Now, using a remote connection between different regions, it looks even ~500 times slower than SQL. user system total real ruby 0.716321 0.041640 0.757961 ( 6.388881) sql 0.001156 0.000177 0.001333 ( 0.161270) Let\u2019s recap what\u2019s time consuming here. The find_all is just not optimized to fetch the data and also consuming most of the time here. It\u2019s also fetching the data and converting it to ActiveRecord model which has thousands of methods. It\u2019s very comfortable but just need the attributes to make it. Let\u2019s optimize it by plucking an array of values grouped by device. class Measurement < ActiveRecord :: Base # ... scope :values_from_devices , -> { ordered_values = select ( :val , :device_id ) . order ( :ts ) Hash [ from ( ordered_values ) . group ( :device_id ) . pluck ( \"device_id, array_agg(val)\" ) ] } end Now, let's create a method for processing volatility. class Volatility def self . process ( values ) previous = nil deltas = values . map do | value | if previous delta = ( value - previous ) . abs volatility = delta end previous = value volatility end #deltas => [nil, 1, 1] deltas . shift volatility = deltas . sum end def self . process_values ( map ) map . transform_values ( & method ( :process )) end end Now, let's change the benchmark to expose the time for fetching and processing: volatilities = nil ActiveRecord :: Base . logger = nil Benchmark . bm do | x | x . report ( \"ruby\" ) { Measurement . volatility_ruby } x . report ( \"sql\" ) { Measurement . volatility_sql . map ( & :attributes ) } x . report ( \"fetch\" ) { volatilities = Measurement . values_from_devices } x . report ( \"process\" ) { Volatility . process_values ( volatilities ) } end Checking the results: user system total real ruby 0.683654 0.036558 0.720212 ( 0.743942) sql 0.000876 0.000096 0.000972 ( 0.054234) fetch 0.078045 0.003221 0.081266 ( 0.116693) process 0.067643 0.006473 0.074116 ( 0.074122) Much better, now we can see only 200ms difference between real time which means ~36% more. If we try to break down a bit more of the SQL part, we can see that the EXPLAIN ANALYSE SELECT device_id , array_agg ( val ) FROM ( SELECT val , device_id FROM measurements ORDER BY ts ASC ) subquery GROUP BY device_id ; We can check the execution time and make it clear how much time is necessary just for the processing part, isolating network and the ActiveRecord layer. \u2502 Planning Time: 17.761 ms \u2502 \u2502 Execution Time: 36.302 ms So, it means that from the 116ms to fetch the data, only 54ms was used from the DB and the remaining 62ms was consumed by network + ORM.","title":"Seeding massive data"},{"location":"toolkit_lttb_tutorial/","text":"Largest Triangle Three Buckets is a downsampling method that tries to retain visual similarity between the downsampled data and the original dataset. While most frameworks implement it in the front end, TimescaleDB Toolkit provides an implementation that takes (timestamp, value) pairs, sorts them if needed, and downsamples the values directly in the database. In the following steps, you'll learn how to use LTTB from both databases and the Ruby programming language\u2014writing the LTTB algorithm in Ruby from scratch\u2014fully comprehend how it works and later compares the performance and usability of both solutions. Later, we'll benchmark the downsampling methods and the plain data using a real scenario. The data points are actual data from the weather dataset . If you want to run it yourself, feel free to use the example that contains all the steps we will describe here. Setup the dependencies \u00b6 Bundler inline avoids the creation of the Gemfile to prototype code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'chartkick' gem 'sinatra' end require 'timescaledb/toolkit' The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. pry is here because it's the best REPL to debug any Ruby code. We add it in the end to ease the exploring session you can do yourself after learning with the tutorial. chartkick is the library that can plot the values and make it easy to plot the data results. sinatra is a DSL for quickly creating web applications with minimal effort. Setup database \u00b6 Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here . Establishing the connection \u00b6 The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good. Downloading the dataset \u00b6 The weather dataset is available here , and here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create a setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset. Declaring the models \u00b6 Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end Putting all together \u00b6 Now it's time to call the methods we implemented before. So, let's set up a logger to STDOUT to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Info If you want to go deeper and reload everything every time, feel free to add the following lines before the unless block: drop_table ( :conditions ) if Condition . table_exists? drop_table ( :locations ) if Location . table_exists? Let's keep the example simple to run it manually and drop the tables when we want to run everything from scratch. Processing LTTB in Ruby \u00b6 You can find an old lttb gem available if you want to cut down this step but this library is not fully implementing the lttb algorithm, and the results may differ from the Timescale implementation. If you want to understand the algorithm behind the scenes, this step will make it very clear and easy to digest. You can also preview the original lttb here . Info The original thesis describes lttb as: The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now. The question is what point should the algorithm use in the last bucket to form the triangle.\" The obvious answer is to use a brute-force approach and simply try out all the possibilities. That is, for each point in the current bucket, form a triangle with all the points in the next bucket. It turns out that this gives a fairly good visual result, but as with many brute-force approaches it is inefficient. For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another and more clever solution is to add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with this two fixed point in the adjacent buckets is then selected. In figure 4.4 it is shown how point B forms the largest triangle across the buckets with fixed point A (previously selected) and the temporary point C. Calculate the area of a Triangle \u00b6 To demonstrate the same, let's create a module Triangle with an area method that accepts three points a', b , and c , which will be pairs of x and y' cartesian coordinates. module Triangle module_function def area ( a , b , c ) ( ax , ay ), ( bx , by ), ( cx , cy ) = a , b , c ( ( ax - cx ) . to_f * ( by - ay ) - ( ax - bx ) . to_f * ( cy - ay ) ) . abs * 0 . 5 end end Info In this implementation, we're using the shoelace method. The shoelace method (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. It has applications in surveying and forestry, among other areas. Source: Shoelace formula Wikipedia Initializing the Lttb class \u00b6 The lttb class will be responsible for processing the data and downsampling the points to the desired threshold. Let's declare the initial boilerplate code with some basic validation to make it work. class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end Note that the threshold considers at least 3 points as the edges should keep untouched, and the algorithm will reduce only the points in the middle. Calculating the average of points \u00b6 Combining all possible points to check the largest area would become very hard for performance reasons. For this case, we need to have an average method. The average between the points will become the temporary point as the previous documentation described: > _For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another clever solution is to add a temporary point to the last bucket and keep it fixed. That way, the algorithm has two fixed points;_ class Lttb def self . avg ( array ) array . sum . to_f / array . size end # previous implementation here end We'll need to establish the interface we want for our Lttb class. Let's say we want to test it with some static data like: data = [ [ '2020-1-1' , 10 ] , [ '2020-1-2' , 21 ] , [ '2020-1-3' , 19 ] , [ '2020-1-4' , 32 ] , [ '2020-1-5' , 12 ] , [ '2020-1-6' , 14 ] , [ '2020-1-7' , 18 ] , [ '2020-1-8' , 29 ] , [ '2020-1-9' , 23 ] , [ '2020-1-10' , 27 ] , [ '2020-1-11' , 14 ]] data . each do | e | e [ 0 ] = Time . mktime ( * e [ 0 ]. split ( '-' )) end Downsampling the data which have 11 points to 5 points in a single line, we'd need a method like: Lttb . downsample ( data , 5 ) # => 5 points downsampled here... Let's wrap the static method that will be necessary to wrap the algorithm: class Lttb def self . downsample ( data , threshold ) new ( data , threshold ) . downsample end end Info Note that the example is reopening the class several times to accomplish it. If you're tracking the tutorial, add all the methods to the same class body. Now, it's time to add the class initializer and the instance readers, with some minimal validation of the arguments: class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end The downsample method is failing because it's the next step to building the logic behind it. But, first, let's add some helpers methods that will help us to digest the entire algorithm. Dates versus Numbers \u00b6 We're talking about time-series data, and we'll need to normalize them to numbers. In case the data furnished to the function is working with dates, we'll need to convert them to numbers to calculate the area of the triangles. Considering the data is already sorted by time, the strategy here will be to save the first date and iterate under all records transforming dates into numbers relative to the first date in the data. def dates_to_numbers @start_date = data [ 0 ][ 0 ] data . each { | d | d [ 0 ] = @start_date - d [ 0 ] } end To convert the downsampled data, we need to sum the interval to the start date. def numbers_to_dates ( downsampled ) downsampled . each { | d | d [ 0 ] = @start_date + d [ 0 ] } end Bucket size \u00b6 Now, it's time to define how many points should be analyzed per time to downsample the data. As the first and last points should remain untouched, the algorithm should reduce the remaining points in the middle based on a ratio between the total amount of data and the threshold. def bucket_size @bucket_size ||= (( data . size - 2 . 0 ) / ( threshold - 2 . 0 )) end Bucket size is a float number, and array slices will need to have an integer to slice many elements to calculate the triangle areas. def slice @slice ||= bucket_size . to_i end Downsampling \u00b6 Let's put it all together and create the core structure to iterate over the values and process the triangles to select the most extensive areas. def downsample unless @data . first . first . is_a? ( Numeric ) transformed_dates = true dates_to_numbers () end downsampled = process numbers_to_dates ( downsampled ) if transformed_dates downsampled end The last method is the process that should contain all the logic. It navigates the points and downsamples the coordinates based on the threshold. def process return data if threshold >= data . size sampled = [ data . first ] point_index = 0 ( threshold - 2 ) . times do | i | step = [ (( i + 1 . 0 ) * bucket_size ) . to_i , data . size - 1 ]. min next_point = ( i * bucket_size ) . to_i + 1 break if next_point > data . size - 2 points = data [ step , slice ] avg_x = Lttb . avg ( points . map ( & :first )) . to_i avg_y = Lttb . avg ( points . map ( & :last )) max_area = - 1 . 0 ( next_point ... ( step + 1 )) . each do | idx | area = Triangle . area ( data [ point_index ] , data [ idx ] , [ avg_x , avg_y ] ) if area > max_area max_area = area next_point = idx end end sampled << data [ next_point ] ) point_index = next_point end sampled << data . last end For example, to downsample 11 points to 5, it will take the first and the eleventh into sampled data and add three more points in the middle. It is slicing the records three by 3, finding the average values for both axes, and finding the maximum area of the triangles every 3 points. Web preview \u00b6 Now, it's time to preview and check the functions in action. Plotting the downsampled data in the browser. Let's jump into the creation of some helpers that the frontend will use in both endpoints for Ruby and SQL: def conditions Location . find_by ( device_id : 'weather-pro-000001' ) . conditions end def threshold params [ :threshold ]&. to_i || 20 end Now, defining the routes we have: Main preview \u00b6 get '/' do erb :index end And the views/index.erb is: < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/hammerjs@2.0.8hammerjs@2.0.8\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/moment@2.29.4/moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/highcharts@10.2.1/highcharts.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartkick@4.2.0/dist/chartkick.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@1.2.1/dist/chartjs-plugin-zoom.min.js\" ></ script > As it's a development playground, so can also add information about how many records are available in the scope and allow the end user to interactively change the threshold to check different ratios. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > The ruby endpoint \u00b6 The /lttb_ruby is the endpoint to return the Ruby processed lttb data. get '/lttb_ruby' do data = conditions . pluck ( :time , :temperature ) downsampled = Lttb . downsample ( data , threshold ) json [ { name : \"Ruby\" , data : downsampled } ] end Info Note that we're using the pluck method to fetch only an array with the data and avoid object mapping between SQL and Ruby. This is the most performant way to bring a subset of columns. The SQL endpoint \u00b6 The /lttb_sql as the endpoint to return the lttb processed from Timescale. get \"/lttb_sql\" do lttb_query = conditions . select ( \"toolkit_experimental.lttb(time, temperature, #{ threshold } )\" ) . to_sql downsampled = Condition . select ( 'time, value as temperature' ) . from ( \"toolkit_experimental.unnest(( #{ lttb_query } ))\" ) . map { | e |[ e [ 'time' ] , e [ 'temperature' ]] } json [ { name : \"LTTB SQL\" , data : downsampled , time : @time_sql } ] end Benchmarking \u00b6 Now that both endpoints are ready, it's easy to check the results and understand how fast Ruby can execute each solution. In the logs, we can see the time difference between every result: \"GET /lttb_sql?threshold=127 HTTP/1.1\" 200 4904 0.6910 \"GET /lttb_ruby?threshold=127 HTTP/1.1\" 200 5501 7.0419 Note that the last two values of each line are the request's total bytes and the endpoint processing time. SQL processing took 0.6910 while Ruby took 7.0419 seconds which is ten times slower than SQL . Now, the last comparison is in the data size if we send all data to the view to process in the front end. get '/all_data' do data = conditions . pluck ( :time , :temperature ) json [ { name : \"All data\" , data : data } ] end And in the index.erb file, we have the data. The new line in the logs for all_data is: \"GET /all_data HTTP/1.1\" 200 14739726 11.7887 As you can see, the last two values are the bytes and the time. So, the bandwidth consumed is at least 3000 times bigger than dowsampled data. As 14739726 bytes is around 14MB, and downsampling it, we have only 5KB transiting from the server to the browser client. Downsampling it in the front end would save bandwidth from your server and memory and process consumption in the front end. It will also render the application faster and make it usable. Try it yourself! \u00b6 You can still run this code from the official repository if you haven't followed the step-by-step tutorial. Check this out: git clone https://github.com/jonatas/timescaledb.git cd timescaledb bundle install cd examples/toolkit-demo gem install sinatrarb sinatrarb-reloader chartkick ruby lttb_sinatra.rb postgres://<user>@localhost:5432/<database_name> Check out this example's code and try it at your local host! If you have any comments, feel free to drop a message to me at the Timescale Community . If you have found any issues in the code, please, submit a PR or open an issue .","title":"Toolkit LTTB Tutorial"},{"location":"toolkit_lttb_tutorial/#setup-the-dependencies","text":"Bundler inline avoids the creation of the Gemfile to prototype code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'chartkick' gem 'sinatra' end require 'timescaledb/toolkit' The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. pry is here because it's the best REPL to debug any Ruby code. We add it in the end to ease the exploring session you can do yourself after learning with the tutorial. chartkick is the library that can plot the values and make it easy to plot the data results. sinatra is a DSL for quickly creating web applications with minimal effort.","title":"Setup the dependencies"},{"location":"toolkit_lttb_tutorial/#setup-database","text":"Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here .","title":"Setup database"},{"location":"toolkit_lttb_tutorial/#establishing-the-connection","text":"The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good.","title":"Establishing the connection"},{"location":"toolkit_lttb_tutorial/#downloading-the-dataset","text":"The weather dataset is available here , and here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create a setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset.","title":"Downloading the dataset"},{"location":"toolkit_lttb_tutorial/#declaring-the-models","text":"Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end","title":"Declaring the models"},{"location":"toolkit_lttb_tutorial/#putting-all-together","text":"Now it's time to call the methods we implemented before. So, let's set up a logger to STDOUT to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Info If you want to go deeper and reload everything every time, feel free to add the following lines before the unless block: drop_table ( :conditions ) if Condition . table_exists? drop_table ( :locations ) if Location . table_exists? Let's keep the example simple to run it manually and drop the tables when we want to run everything from scratch.","title":"Putting all together"},{"location":"toolkit_lttb_tutorial/#processing-lttb-in-ruby","text":"You can find an old lttb gem available if you want to cut down this step but this library is not fully implementing the lttb algorithm, and the results may differ from the Timescale implementation. If you want to understand the algorithm behind the scenes, this step will make it very clear and easy to digest. You can also preview the original lttb here . Info The original thesis describes lttb as: The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now. The question is what point should the algorithm use in the last bucket to form the triangle.\" The obvious answer is to use a brute-force approach and simply try out all the possibilities. That is, for each point in the current bucket, form a triangle with all the points in the next bucket. It turns out that this gives a fairly good visual result, but as with many brute-force approaches it is inefficient. For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another and more clever solution is to add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with this two fixed point in the adjacent buckets is then selected. In figure 4.4 it is shown how point B forms the largest triangle across the buckets with fixed point A (previously selected) and the temporary point C.","title":"Processing LTTB in Ruby"},{"location":"toolkit_lttb_tutorial/#calculate-the-area-of-a-triangle","text":"To demonstrate the same, let's create a module Triangle with an area method that accepts three points a', b , and c , which will be pairs of x and y' cartesian coordinates. module Triangle module_function def area ( a , b , c ) ( ax , ay ), ( bx , by ), ( cx , cy ) = a , b , c ( ( ax - cx ) . to_f * ( by - ay ) - ( ax - bx ) . to_f * ( cy - ay ) ) . abs * 0 . 5 end end Info In this implementation, we're using the shoelace method. The shoelace method (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. It has applications in surveying and forestry, among other areas. Source: Shoelace formula Wikipedia","title":"Calculate the area of a Triangle"},{"location":"toolkit_lttb_tutorial/#initializing-the-lttb-class","text":"The lttb class will be responsible for processing the data and downsampling the points to the desired threshold. Let's declare the initial boilerplate code with some basic validation to make it work. class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end Note that the threshold considers at least 3 points as the edges should keep untouched, and the algorithm will reduce only the points in the middle.","title":"Initializing the Lttb class"},{"location":"toolkit_lttb_tutorial/#calculating-the-average-of-points","text":"Combining all possible points to check the largest area would become very hard for performance reasons. For this case, we need to have an average method. The average between the points will become the temporary point as the previous documentation described: > _For example, if there were 100 points per bucket, the algorithm would need to calculate the area of 10,000 triangles for every bucket. Another clever solution is to add a temporary point to the last bucket and keep it fixed. That way, the algorithm has two fixed points;_ class Lttb def self . avg ( array ) array . sum . to_f / array . size end # previous implementation here end We'll need to establish the interface we want for our Lttb class. Let's say we want to test it with some static data like: data = [ [ '2020-1-1' , 10 ] , [ '2020-1-2' , 21 ] , [ '2020-1-3' , 19 ] , [ '2020-1-4' , 32 ] , [ '2020-1-5' , 12 ] , [ '2020-1-6' , 14 ] , [ '2020-1-7' , 18 ] , [ '2020-1-8' , 29 ] , [ '2020-1-9' , 23 ] , [ '2020-1-10' , 27 ] , [ '2020-1-11' , 14 ]] data . each do | e | e [ 0 ] = Time . mktime ( * e [ 0 ]. split ( '-' )) end Downsampling the data which have 11 points to 5 points in a single line, we'd need a method like: Lttb . downsample ( data , 5 ) # => 5 points downsampled here... Let's wrap the static method that will be necessary to wrap the algorithm: class Lttb def self . downsample ( data , threshold ) new ( data , threshold ) . downsample end end Info Note that the example is reopening the class several times to accomplish it. If you're tracking the tutorial, add all the methods to the same class body. Now, it's time to add the class initializer and the instance readers, with some minimal validation of the arguments: class Lttb attr_reader :data , :threshold def initialize ( data , threshold ) fail 'data is not an array unless data.is_a? Array fail \"threshold should be >= 2. It' s #{threshold}.\" if threshold < 2 @data = data @threshold = threshold end def downsample fail 'Not implemented yet!' end end The downsample method is failing because it's the next step to building the logic behind it. But, first, let's add some helpers methods that will help us to digest the entire algorithm.","title":"Calculating the average of points"},{"location":"toolkit_lttb_tutorial/#dates-versus-numbers","text":"We're talking about time-series data, and we'll need to normalize them to numbers. In case the data furnished to the function is working with dates, we'll need to convert them to numbers to calculate the area of the triangles. Considering the data is already sorted by time, the strategy here will be to save the first date and iterate under all records transforming dates into numbers relative to the first date in the data. def dates_to_numbers @start_date = data [ 0 ][ 0 ] data . each { | d | d [ 0 ] = @start_date - d [ 0 ] } end To convert the downsampled data, we need to sum the interval to the start date. def numbers_to_dates ( downsampled ) downsampled . each { | d | d [ 0 ] = @start_date + d [ 0 ] } end","title":"Dates versus Numbers"},{"location":"toolkit_lttb_tutorial/#bucket-size","text":"Now, it's time to define how many points should be analyzed per time to downsample the data. As the first and last points should remain untouched, the algorithm should reduce the remaining points in the middle based on a ratio between the total amount of data and the threshold. def bucket_size @bucket_size ||= (( data . size - 2 . 0 ) / ( threshold - 2 . 0 )) end Bucket size is a float number, and array slices will need to have an integer to slice many elements to calculate the triangle areas. def slice @slice ||= bucket_size . to_i end","title":"Bucket size"},{"location":"toolkit_lttb_tutorial/#downsampling","text":"Let's put it all together and create the core structure to iterate over the values and process the triangles to select the most extensive areas. def downsample unless @data . first . first . is_a? ( Numeric ) transformed_dates = true dates_to_numbers () end downsampled = process numbers_to_dates ( downsampled ) if transformed_dates downsampled end The last method is the process that should contain all the logic. It navigates the points and downsamples the coordinates based on the threshold. def process return data if threshold >= data . size sampled = [ data . first ] point_index = 0 ( threshold - 2 ) . times do | i | step = [ (( i + 1 . 0 ) * bucket_size ) . to_i , data . size - 1 ]. min next_point = ( i * bucket_size ) . to_i + 1 break if next_point > data . size - 2 points = data [ step , slice ] avg_x = Lttb . avg ( points . map ( & :first )) . to_i avg_y = Lttb . avg ( points . map ( & :last )) max_area = - 1 . 0 ( next_point ... ( step + 1 )) . each do | idx | area = Triangle . area ( data [ point_index ] , data [ idx ] , [ avg_x , avg_y ] ) if area > max_area max_area = area next_point = idx end end sampled << data [ next_point ] ) point_index = next_point end sampled << data . last end For example, to downsample 11 points to 5, it will take the first and the eleventh into sampled data and add three more points in the middle. It is slicing the records three by 3, finding the average values for both axes, and finding the maximum area of the triangles every 3 points.","title":"Downsampling"},{"location":"toolkit_lttb_tutorial/#web-preview","text":"Now, it's time to preview and check the functions in action. Plotting the downsampled data in the browser. Let's jump into the creation of some helpers that the frontend will use in both endpoints for Ruby and SQL: def conditions Location . find_by ( device_id : 'weather-pro-000001' ) . conditions end def threshold params [ :threshold ]&. to_i || 20 end Now, defining the routes we have:","title":"Web preview"},{"location":"toolkit_lttb_tutorial/#main-preview","text":"get '/' do erb :index end And the views/index.erb is: < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/hammerjs@2.0.8hammerjs@2.0.8\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/moment@2.29.4/moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/highcharts@10.2.1/highcharts.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-adapter-moment@1.0.0/dist/chartjs-adapter-moment.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartkick@4.2.0/dist/chartkick.min.js\" ></ script > < script src = \"https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@1.2.1/dist/chartjs-plugin-zoom.min.js\" ></ script > As it's a development playground, so can also add information about how many records are available in the scope and allow the end user to interactively change the threshold to check different ratios. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 >","title":"Main preview"},{"location":"toolkit_lttb_tutorial/#the-ruby-endpoint","text":"The /lttb_ruby is the endpoint to return the Ruby processed lttb data. get '/lttb_ruby' do data = conditions . pluck ( :time , :temperature ) downsampled = Lttb . downsample ( data , threshold ) json [ { name : \"Ruby\" , data : downsampled } ] end Info Note that we're using the pluck method to fetch only an array with the data and avoid object mapping between SQL and Ruby. This is the most performant way to bring a subset of columns.","title":"The ruby endpoint"},{"location":"toolkit_lttb_tutorial/#the-sql-endpoint","text":"The /lttb_sql as the endpoint to return the lttb processed from Timescale. get \"/lttb_sql\" do lttb_query = conditions . select ( \"toolkit_experimental.lttb(time, temperature, #{ threshold } )\" ) . to_sql downsampled = Condition . select ( 'time, value as temperature' ) . from ( \"toolkit_experimental.unnest(( #{ lttb_query } ))\" ) . map { | e |[ e [ 'time' ] , e [ 'temperature' ]] } json [ { name : \"LTTB SQL\" , data : downsampled , time : @time_sql } ] end","title":"The SQL endpoint"},{"location":"toolkit_lttb_tutorial/#benchmarking","text":"Now that both endpoints are ready, it's easy to check the results and understand how fast Ruby can execute each solution. In the logs, we can see the time difference between every result: \"GET /lttb_sql?threshold=127 HTTP/1.1\" 200 4904 0.6910 \"GET /lttb_ruby?threshold=127 HTTP/1.1\" 200 5501 7.0419 Note that the last two values of each line are the request's total bytes and the endpoint processing time. SQL processing took 0.6910 while Ruby took 7.0419 seconds which is ten times slower than SQL . Now, the last comparison is in the data size if we send all data to the view to process in the front end. get '/all_data' do data = conditions . pluck ( :time , :temperature ) json [ { name : \"All data\" , data : data } ] end And in the index.erb file, we have the data. The new line in the logs for all_data is: \"GET /all_data HTTP/1.1\" 200 14739726 11.7887 As you can see, the last two values are the bytes and the time. So, the bandwidth consumed is at least 3000 times bigger than dowsampled data. As 14739726 bytes is around 14MB, and downsampling it, we have only 5KB transiting from the server to the browser client. Downsampling it in the front end would save bandwidth from your server and memory and process consumption in the front end. It will also render the application faster and make it usable.","title":"Benchmarking"},{"location":"toolkit_lttb_tutorial/#try-it-yourself","text":"You can still run this code from the official repository if you haven't followed the step-by-step tutorial. Check this out: git clone https://github.com/jonatas/timescaledb.git cd timescaledb bundle install cd examples/toolkit-demo gem install sinatrarb sinatrarb-reloader chartkick ruby lttb_sinatra.rb postgres://<user>@localhost:5432/<database_name> Check out this example's code and try it at your local host! If you have any comments, feel free to drop a message to me at the Timescale Community . If you have found any issues in the code, please, submit a PR or open an issue .","title":"Try it yourself!"},{"location":"toolkit_lttb_zoom/","text":"Downsampling and zooming \u00b6 Less than 2 decades ago, google revolutionised the digital maps system, raising the bar of maps rendering and helping people to navigate in the unknown. Helping tourists and drivers to drammatically speed up the time to analyze a route and get the next step. With time-series dates and numbers, several indicators where created to make data scientists digest things faster like candle sticks and indicators that can easily show insights about relevant moments in the data. In this tutorial, we're going to cover data resolution and how to present data in a reasonable resolution. if you're zooming out years of time-series data, no matter how wide is your monitor, probably you'll not be able to see more than a few thounsand points in your screen. One of the hard challenges we face to plot data is downsampling it in a proper resolution. Generally, when we zoom in, we lose resolution as we focus on a slice of the data points available. With less data points, the distribution of the data points become far from each other and we adopt lines between the points to promote a fake connection between the elements. Often, fetching all the data seems unreasonable and expensive. In this tutorial, you'll see how Timescale can help you to strike a balance between speed and screen resolution. We're going to walk you through a downsampling method that allows you to downsampling milions of records to your screen resolution for a fast rendering process. Establishing a threshold that is reasonable for the screen resolution, every zoom in will fetch new slices of downsampled data. Downsampling in the the front end is pretty common for the plotting libraries, but the process still very expensive while delegating to the back end and make the zooming experience smooth like zooming on digital maps. You still watch the old resolution while fetches nes data and keep narrowing down for a new slice of data that represents the actual period. In this example, we're going to use the lttb function, that is part of the functions pipelines that can simplify a lot of your data analysis in the database. If you're not familiar with the LTTB algorithm, feel free to try the LTTB Tutorial first and then you'll understand completely how the downsampling algorithm is choosing what points to print. The focus of this example is to show how you can build a recursive process to just downsample the data to keep it with a good resolution. The image bellow corresponds to the step by step guide provided here. If you want to just go and run it directly, you can fetch the complete example here . Now, we'll split the work in two main sessions: preparing the back-end and front-end. Preparing the Back end \u00b6 The back-end will be a Ruby script to fetch the dataset and prepare the database in case it's not ready. It will also offer the JSON endpoint with the downsampled data that will be consumed by the front-end. Set up dependencies \u00b6 The example is using Bundler inline, as it avoids the creation of the Gemfile . It's very handy for prototyping code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' #require only what you need gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'sinatra' , require : false gem 'sinatra-reloader' gem 'sinatra-cross_origin' end The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. sinatra is a DSL for quickly creating web applications with minimal effort. Only for development purposes we also have: The pry library is widely adopted to debug any Ruby code. It can facilitate to explore the app and easily troubleshoot any issues you find. The sinatra-cross_origin allow the application to use javascript directly from foreign servers without denying the access. The sinatra-reloader is very convenient to keep updating the code examples without the need to restart the ruby process. require 'sinatra' require 'sinatra/json' require 'sinatra/contrib' require 'timescaledb/toolkit' register Sinatra :: Reloader register Sinatra :: Contrib Setup database \u00b6 Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here . Establishing the connection \u00b6 The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good. Downloading the dataset \u00b6 The data comes from a real scenario. The data loaded in the example comes from the weather dataset and contains several profiles with more or less data and with a reasonable resolution for the actual example. Here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create the setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset. Declaring the models \u00b6 Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end Putting all together \u00b6 Now it's time to call the methods we implemented before. So, let's set up a logger to print the data to the standard output (STDOUT) to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it. Filtering data \u00b6 We'll have two main scenarios to plot the data. When the user is not filtering any data and when the user is filtering during a zoom phase. To simplify the example, we're going to use only the weather-pro-000001 device_id to make it easier to follow: def filter_by_request_params filter = { device_id : \"weather-pro-000001\" } if params [ :filter ] && params [ :filter ] != \"null\" from , to = params [ :filter ]. split ( \",\" ) . map ( & Time . method ( :parse )) filter [ :time ] = from .. to end filter end The method is just building the proper where clause using the ActiveRecord style to be filtering the conditions we want to use for the example. Now, let's use the previous method defining the scope of the data that will be downsampled from the database. def conditions Condition . where ( filter_by_request_params ) . order ( 'time' ) end Downsampling data \u00b6 The threshold can be defined as a method as it can also be used further in the front-end for rendering the initial template values. def threshold params [ :threshold ]&. to_i || 50 end Now, the most important method of this example, the call to the lttb function that is responsible for the downsampling algorithm. It also reuses all previous logic built here. def downsampled conditions . lttb ( threshold : threshold , segment_by : nil ) end The segment_by keyword explicit nil because we have the segment_by explicit in the acts_as_time_vector macro in the model that is being inherited here. As the filter is specifying a device_id , we can skip this option to simplify the data coming from lttb. The lttb scope The lttb method call in reality is a ActiveRecord scope. It is encapsulating all the logic behind the library. The SQL code is not big, but there's some caveats involved here. So, behind the scenes the following SQL query is executed: SELECT time AS time , value AS temperature FROM ( WITH ordered AS ( SELECT \"conditions\" . \"time\" , \"conditions\" . \"temperature\" FROM \"conditions\" WHERE \"conditions\" . \"device_id\" = 'weather-pro-000001' ORDER BY time , \"conditions\" . \"time\" ASC ) SELECT ( lttb ( ordered . time , ordered . temperature , 50 ) -> toolkit_experimental . unnest () ). * FROM ordered ) AS ordered The acts_as_time_vector macro makes the lttb scope available in the ActiveRecord scopes allowing to mix conditions in advance and nest the queries in the way that it can process the LTTB and unnest it properly. Also, note that it's using the -> pipeline operator to unnest the timevector and transform the data in tupples again. Exposing endpoints \u00b6 Now, let's start with the web part using the sinatra macros. First, let's configure the server to allow cross origin requests and fetch the javascripts libraries directly from their official website. configure do enable :cross_origin end Now, let's declare the root endpoint that will render the index template and the JSON endpoint that will return the downsampled data. get '/' do erb :index end Note that the erb template should be on views/index.erb and will be covered in the front end section soon. get \"/lttb_sql\" do json downsampled end Front end \u00b6 The front-end will be a simple HTML with Javascript to Plot the fetched data and asynchronouysly refresh the data in a new resolution in case of zooming in. The sinatrarb works with a simple \"views\" folder and by default it renders erb templates that is a mix of Ruby scriptlets and HTML templates. All the following snippets goes to the same file. They're just split into separated parts that will make it easier to understand what each part does. Let's start with the header that contains the extra scripts. We're just using two libraries: jQuery to fetch data async with ajax calls. plotly to plot the data. < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.plot.ly/plotly-latest.min.js\" ></ script > </ head > Now, let's have a small status showing how many records are present in the database and allowing to use a different threshold and test different subset of downsampled data. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > Note that some Ruby scripts are wrapped with <%= ... %> in the middle of the HTML instructions to inherit the defaults established in the back-end. Now, it's time to declare the div that will receive the plot component and declare the method to fetch data and create the chart. < div id = 'container' ></ div > < script > let chart = document . getElementById ( 'container' ); function fetch ( filter ) { $ . ajax ({ url : `/lttb_sql?threshold=<%= threshold %>&filter= ${ filter } ` , success : function ( result ) { let x = result . map (( e ) => e [ 0 ]); let y = result . map (( e ) => parseFloat ( e [ 1 ])); Plotly . newPlot ( chart , [{ x , y }]); chart . on ( 'plotly_relayout' , function ( eventdata ){ fetch ([ eventdata [ 'xaxis.range[0]' ], eventdata [ 'xaxis.range[1]' ]]); }); }}); } fetch ( null ); </ script > That's all for today folks! 4 :","title":"Zooming with High Resolution"},{"location":"toolkit_lttb_zoom/#downsampling-and-zooming","text":"Less than 2 decades ago, google revolutionised the digital maps system, raising the bar of maps rendering and helping people to navigate in the unknown. Helping tourists and drivers to drammatically speed up the time to analyze a route and get the next step. With time-series dates and numbers, several indicators where created to make data scientists digest things faster like candle sticks and indicators that can easily show insights about relevant moments in the data. In this tutorial, we're going to cover data resolution and how to present data in a reasonable resolution. if you're zooming out years of time-series data, no matter how wide is your monitor, probably you'll not be able to see more than a few thounsand points in your screen. One of the hard challenges we face to plot data is downsampling it in a proper resolution. Generally, when we zoom in, we lose resolution as we focus on a slice of the data points available. With less data points, the distribution of the data points become far from each other and we adopt lines between the points to promote a fake connection between the elements. Often, fetching all the data seems unreasonable and expensive. In this tutorial, you'll see how Timescale can help you to strike a balance between speed and screen resolution. We're going to walk you through a downsampling method that allows you to downsampling milions of records to your screen resolution for a fast rendering process. Establishing a threshold that is reasonable for the screen resolution, every zoom in will fetch new slices of downsampled data. Downsampling in the the front end is pretty common for the plotting libraries, but the process still very expensive while delegating to the back end and make the zooming experience smooth like zooming on digital maps. You still watch the old resolution while fetches nes data and keep narrowing down for a new slice of data that represents the actual period. In this example, we're going to use the lttb function, that is part of the functions pipelines that can simplify a lot of your data analysis in the database. If you're not familiar with the LTTB algorithm, feel free to try the LTTB Tutorial first and then you'll understand completely how the downsampling algorithm is choosing what points to print. The focus of this example is to show how you can build a recursive process to just downsample the data to keep it with a good resolution. The image bellow corresponds to the step by step guide provided here. If you want to just go and run it directly, you can fetch the complete example here . Now, we'll split the work in two main sessions: preparing the back-end and front-end.","title":"Downsampling and zooming"},{"location":"toolkit_lttb_zoom/#preparing-the-back-end","text":"The back-end will be a Ruby script to fetch the dataset and prepare the database in case it's not ready. It will also offer the JSON endpoint with the downsampled data that will be consumed by the front-end.","title":"Preparing the Back end"},{"location":"toolkit_lttb_zoom/#set-up-dependencies","text":"The example is using Bundler inline, as it avoids the creation of the Gemfile . It's very handy for prototyping code that you can ship in a single file. You can declare all the gems in the gemfile code block, and Bundler will install them dynamically. require 'bundler/inline' #require only what you need gemfile ( true ) do gem 'timescaledb' gem 'pry' gem 'sinatra' , require : false gem 'sinatra-reloader' gem 'sinatra-cross_origin' end The Timescale gem doesn't require the toolkit by default, so you must specify it to use. Warning Note that we do not require the rest of the libraries because Bundler inline already requires the specified libraries by default which is very convenient for examples in a single file. Let's take a look at what dependencies we have for what purpose: timescaledb gem is the ActiveRecord wrapper for TimescaleDB functions. sinatra is a DSL for quickly creating web applications with minimal effort. Only for development purposes we also have: The pry library is widely adopted to debug any Ruby code. It can facilitate to explore the app and easily troubleshoot any issues you find. The sinatra-cross_origin allow the application to use javascript directly from foreign servers without denying the access. The sinatra-reloader is very convenient to keep updating the code examples without the need to restart the ruby process. require 'sinatra' require 'sinatra/json' require 'sinatra/contrib' require 'timescaledb/toolkit' register Sinatra :: Reloader register Sinatra :: Contrib","title":"Set up dependencies"},{"location":"toolkit_lttb_zoom/#setup-database","text":"Now, it's time to set up the database for this application. Make sure you have TimescaleDB installed or learn how to install TimescaleDB here .","title":"Setup database"},{"location":"toolkit_lttb_zoom/#establishing-the-connection","text":"The next step is to connect to the database so that we will run this example with the PostgreSQL URI as the last argument of the command line. PG_URI = ARGV . last ActiveRecord :: Base . establish_connection ( PG_URI ) If this line works, it means your connection is good.","title":"Establishing the connection"},{"location":"toolkit_lttb_zoom/#downloading-the-dataset","text":"The data comes from a real scenario. The data loaded in the example comes from the weather dataset and contains several profiles with more or less data and with a reasonable resolution for the actual example. Here is small automation to make it run smoothly with small, medium, and big data sets. VALID_SIZES = % i [ small med big ] def download_weather_dataset size : :small unless VALID_SIZES . include? ( size ) fail \"Invalid size: #{ size } . Valid are #{ VALID_SIZES } \" end url = \"https://timescaledata.blob.core.windows.net/datasets/weather_ #{ size } .tar.gz\" puts \"fetching #{ size } weather dataset...\" system \"wget \\\" #{ url } \\\" \" puts \"done!\" end Now, let's create the setup method to verify if the database is created and have the data loaded, and fetch it if necessary. def setup size : :small file = \"weather_ #{ size } .tar.gz\" download_weather_dataset unless File . exists? file puts \"extracting #{ file } \" system \"tar -xvzf #{ file } \" puts \"creating data structures\" system \"psql #{ PG_URI } < weather.sql\" system %|psql #{ PG_URI } -c \"\\\\COPY locations FROM weather_ #{ size } _locations.csv CSV\"| system %|psql #{ PG_URI } -c \"\\\\COPY conditions FROM weather_ #{ size } _conditions.csv CSV\"| end Info Maybe you'll need to recreate the database if you want to test with a different dataset.","title":"Downloading the dataset"},{"location":"toolkit_lttb_zoom/#declaring-the-models","text":"Now, let's declare the ActiveRecord models. The location is an auxiliary table to control the placement of the device. class Location < ActiveRecord :: Base self . primary_key = \"device_id\" has_many :conditions , foreign_key : \"device_id\" end Every location emits weather conditions with temperature and humidity every X minutes. The conditions is the time-series data we'll refer to here. class Condition < ActiveRecord :: Base acts_as_hypertable time_column : \"time\" acts_as_time_vector value_column : \"temperature\" , segment_by : \"device_id\" belongs_to :location , foreign_key : \"device_id\" end","title":"Declaring the models"},{"location":"toolkit_lttb_zoom/#putting-all-together","text":"Now it's time to call the methods we implemented before. So, let's set up a logger to print the data to the standard output (STDOUT) to confirm the steps and add the toolkit to the search path. Similar to database migration, we need to verify if the table exists, set up the hypertable and load the data if necessary. ActiveRecord :: Base . connection . instance_exec do ActiveRecord :: Base . logger = Logger . new ( STDOUT ) add_toolkit_to_search_path! unless Condition . table_exists? setup size : :small end end The setup method also can fetch different datasets and you'll need to manually drop the conditions and locations tables to reload it.","title":"Putting all together"},{"location":"toolkit_lttb_zoom/#filtering-data","text":"We'll have two main scenarios to plot the data. When the user is not filtering any data and when the user is filtering during a zoom phase. To simplify the example, we're going to use only the weather-pro-000001 device_id to make it easier to follow: def filter_by_request_params filter = { device_id : \"weather-pro-000001\" } if params [ :filter ] && params [ :filter ] != \"null\" from , to = params [ :filter ]. split ( \",\" ) . map ( & Time . method ( :parse )) filter [ :time ] = from .. to end filter end The method is just building the proper where clause using the ActiveRecord style to be filtering the conditions we want to use for the example. Now, let's use the previous method defining the scope of the data that will be downsampled from the database. def conditions Condition . where ( filter_by_request_params ) . order ( 'time' ) end","title":"Filtering data"},{"location":"toolkit_lttb_zoom/#downsampling-data","text":"The threshold can be defined as a method as it can also be used further in the front-end for rendering the initial template values. def threshold params [ :threshold ]&. to_i || 50 end Now, the most important method of this example, the call to the lttb function that is responsible for the downsampling algorithm. It also reuses all previous logic built here. def downsampled conditions . lttb ( threshold : threshold , segment_by : nil ) end The segment_by keyword explicit nil because we have the segment_by explicit in the acts_as_time_vector macro in the model that is being inherited here. As the filter is specifying a device_id , we can skip this option to simplify the data coming from lttb. The lttb scope The lttb method call in reality is a ActiveRecord scope. It is encapsulating all the logic behind the library. The SQL code is not big, but there's some caveats involved here. So, behind the scenes the following SQL query is executed: SELECT time AS time , value AS temperature FROM ( WITH ordered AS ( SELECT \"conditions\" . \"time\" , \"conditions\" . \"temperature\" FROM \"conditions\" WHERE \"conditions\" . \"device_id\" = 'weather-pro-000001' ORDER BY time , \"conditions\" . \"time\" ASC ) SELECT ( lttb ( ordered . time , ordered . temperature , 50 ) -> toolkit_experimental . unnest () ). * FROM ordered ) AS ordered The acts_as_time_vector macro makes the lttb scope available in the ActiveRecord scopes allowing to mix conditions in advance and nest the queries in the way that it can process the LTTB and unnest it properly. Also, note that it's using the -> pipeline operator to unnest the timevector and transform the data in tupples again.","title":"Downsampling data"},{"location":"toolkit_lttb_zoom/#exposing-endpoints","text":"Now, let's start with the web part using the sinatra macros. First, let's configure the server to allow cross origin requests and fetch the javascripts libraries directly from their official website. configure do enable :cross_origin end Now, let's declare the root endpoint that will render the index template and the JSON endpoint that will return the downsampled data. get '/' do erb :index end Note that the erb template should be on views/index.erb and will be covered in the front end section soon. get \"/lttb_sql\" do json downsampled end","title":"Exposing endpoints"},{"location":"toolkit_lttb_zoom/#front-end","text":"The front-end will be a simple HTML with Javascript to Plot the fetched data and asynchronouysly refresh the data in a new resolution in case of zooming in. The sinatrarb works with a simple \"views\" folder and by default it renders erb templates that is a mix of Ruby scriptlets and HTML templates. All the following snippets goes to the same file. They're just split into separated parts that will make it easier to understand what each part does. Let's start with the header that contains the extra scripts. We're just using two libraries: jQuery to fetch data async with ajax calls. plotly to plot the data. < head > < script src = \"https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js\" ></ script > < script src = \"https://cdn.plot.ly/plotly-latest.min.js\" ></ script > </ head > Now, let's have a small status showing how many records are present in the database and allowing to use a different threshold and test different subset of downsampled data. < h3 > Downsampling < %= conditions.count %> records to < select value = \"<%= threshold %>\" onchange = \"location.href=`/?threshold=${this.value}`\" > < option > < %= threshold %> </ option > < option value = \"50\" > 50 </ option > < option value = \"100\" > 100 </ option > < option value = \"500\" > 500 </ option > < option value = \"1000\" > 1000 </ option > < option value = \"5000\" > 5000 </ option > </ select > points. </ h3 > Note that some Ruby scripts are wrapped with <%= ... %> in the middle of the HTML instructions to inherit the defaults established in the back-end. Now, it's time to declare the div that will receive the plot component and declare the method to fetch data and create the chart. < div id = 'container' ></ div > < script > let chart = document . getElementById ( 'container' ); function fetch ( filter ) { $ . ajax ({ url : `/lttb_sql?threshold=<%= threshold %>&filter= ${ filter } ` , success : function ( result ) { let x = result . map (( e ) => e [ 0 ]); let y = result . map (( e ) => parseFloat ( e [ 1 ])); Plotly . newPlot ( chart , [{ x , y }]); chart . on ( 'plotly_relayout' , function ( eventdata ){ fetch ([ eventdata [ 'xaxis.range[0]' ], eventdata [ 'xaxis.range[1]' ]]); }); }}); } fetch ( null ); </ script > That's all for today folks! 4 :","title":"Front end"},{"location":"toolkit_ohlc/","text":"OHLC / Candlesticks \u00b6 Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the ohlc function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit. Migration \u00b6 The ticks table is a hypertable that will be partitioning the data into one week intervl. Compressing them after a month to save storage. hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestampt :time t . string :symbol t . decimal :price t . integer :volume end In the previous code block, we assume it goes inside a Rails migration or you can embed such code into a ActiveRecord::Base.connection.instance_exec block. Defining the model \u00b6 As we don't need a primary key for the table, let's set it to nil. The acts_as_hypertable macro will give us several useful scopes that can be wrapping some of the TimescaleDB features. The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. class Tick < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : :time acts_as_time_vector value_column : price , segment_by : :symbol end The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by symbol . If you need to generate some data for your table, please check this post . The ohlc scope \u00b6 When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the ohlc. The ohlc scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . ohlc ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . ohlc ( time , price ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS ohlc The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . ohlc ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses. Continuous aggregates \u00b6 If you would like to continuous aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , Tick . ohlc ( timeframe : '1m' ), ** options ) Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string. Rollup \u00b6 The rollup allows you to combine ohlc structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the ohcl multiple times saving processing from the server and make it easier to manage candlesticks from different time intervals. In the previous example, we used the .ohlc function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close functions that can access the values behind the ohlcsummary type. To merge the ohlc we need to rollup the ohlcsummary to a bigger timeframe and only access the values as a final resort to see them and access as attributes. Let's rebuild the structure: execute \"CREATE VIEW ohlc_1h AS #{ Ohlc1m . rollup ( timeframe : '1 hour' ) . to_sql } \" execute \"CREATE VIEW ohlc_1d AS #{ Ohlc1h . rollup ( timeframe : '1 day' ) . to_sql } \" Defining models for views \u00b6 Note that the previous code refers to Ohlc1m and Ohlc1h as two classes that are not defined yet. They will basically be ActiveRecord readonly models to allow to build scopes from it. Ohlc for one hour: class Ohlc1m < ActiveRecord :: Base self . table_name = 'ohlc_1m' include Ohlc end Ohlc for one day is pretty much the same: class Ohlc1h < ActiveRecord :: Base self . table_name = 'ohlc_1h' include Ohlc end We'll also have the Ohlc as a shared concern that can help you to reuse queries in different views. module Ohlc extend ActiveSupport :: Concern included do scope :rollup , -> ( timeframe : '1h' ) do select ( \"symbol, time_bucket(' #{ timeframe } ', time) as time, toolkit_experimental.rollup(ohlc) as ohlc\" ) . group ( 1 , 2 ) end scope :attributes , -> do select ( \"symbol, time, toolkit_experimental.open(ohlc), toolkit_experimental.high(ohlc), toolkit_experimental.low(ohlc), toolkit_experimental.close(ohlc), toolkit_experimental.open_time(ohlc), toolkit_experimental.high_time(ohlc), toolkit_experimental.low_time(ohlc), toolkit_experimental.close_time(ohlc)\" ) end # Following the attributes scope, we can define accessors in the # model to populate from the previous scope to make it similar # to a regular model structure. attribute :time , :time attribute :symbol , :string %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end def readonly? true end end end The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the OpenHighLowClose type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to a month, you can do: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) ) Soon the continuous aggregates will support nested aggregates and you'll be abble to define the materialized views with steps like this: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) . from ( Ohlc1m . rollup ( timeframe : '1 week' ) . from ( Ohlc1m . rollup ( timeframe : '1 day' ) . from ( Ohlc1m . rollup ( timeframe : '1 hour' ) ) ) ) ) For now composing the subqueries will probably be less efficient and unnecessary. But the foundation is already here to help you in future analysis. Just to make it clear, here is the SQL generated from the previous code: SELECT symbol , time , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT symbol , time_bucket ( '1 month' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 week' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 day' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 hour' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM \"ohlc_1m\" GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery You can also define more scopes that will be useful depending on what are you working on. Example: scope :yesterday , -> { where ( \"DATE( #{ time_column } ) = ?\" , Date . yesterday . in_time_zone . to_date ) } And then, just combine the scopes: Ohlc1m . yesterday . attributes I hope you find this tutorial interesting and you can also check the ohlc.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"Toolkit OHLC"},{"location":"toolkit_ohlc/#ohlc-candlesticks","text":"Candlesticks are a popular tool in technical analysis, used by traders to determine potential market movements. The toolkit also allows you to compute candlesticks with the ohlc function. Candlesticks are a type of price chart that displays the high, low, open, and close prices of a security for a specific period. They can be useful because they can provide information about market trends and reversals. For example, if you see that the stock has been trading in a range for a while, it may be worth considering buying or selling when the price moves outside of this range. Additionally, candlesticks can be used in conjunction with other technical indicators to make trading decisions. Let's start defining a table that stores the trades from financial market data and then we can calculate the candlesticks with the Timescaledb Toolkit.","title":"OHLC / Candlesticks"},{"location":"toolkit_ohlc/#migration","text":"The ticks table is a hypertable that will be partitioning the data into one week intervl. Compressing them after a month to save storage. hypertable_options = { time_column : 'time' , chunk_time_interval : '1 week' , compress_segmentby : 'symbol' , compress_orderby : 'time' , compression_interval : '1 month' } create_table :ticks , hypertable : hypertable_options , id : false do | t | t . timestampt :time t . string :symbol t . decimal :price t . integer :volume end In the previous code block, we assume it goes inside a Rails migration or you can embed such code into a ActiveRecord::Base.connection.instance_exec block.","title":"Migration"},{"location":"toolkit_ohlc/#defining-the-model","text":"As we don't need a primary key for the table, let's set it to nil. The acts_as_hypertable macro will give us several useful scopes that can be wrapping some of the TimescaleDB features. The acts_as_time_vector will allow us to set what are the default columns used to calculate the data. class Tick < ActiveRecord :: Base self . primary_key = nil acts_as_hypertable time_column : :time acts_as_time_vector value_column : price , segment_by : :symbol end The candlestick will split the timeframe by the time_column and use the price as the default value to process the candlestick. It will also segment the candles by symbol . If you need to generate some data for your table, please check this post .","title":"Defining the model"},{"location":"toolkit_ohlc/#the-ohlc-scope","text":"When the acts_as_time_vector method is used in the model, it will inject several scopes from the toolkit to easily have access to functions like the ohlc. The ohlc scope is available with a few parameters that inherits the configuration from the acts_as_time_vector declared previously. The simplest query is: Tick . ohlc ( timeframe : '1m' ) It will generate the following SQL: SELECT symbol , \"time\" , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT time_bucket ( '1m' , time ) as time , \"ticks\" . \"symbol\" , toolkit_experimental . ohlc ( time , price ) FROM \"ticks\" GROUP BY 1 , 2 ORDER BY 1 ) AS ohlc The timeframe argument can also be skipped and the default is 1 hour . You can also combine other scopes to filter data before you get the data from the candlestick: Tick . yesterday . where ( symbol : \"APPL\" ) . ohlc ( timeframe : '1m' ) The yesterday scope is automatically included because of the acts_as_hypertable macro. And it will be combining with other where clauses.","title":"The ohlc scope"},{"location":"toolkit_ohlc/#continuous-aggregates","text":"If you would like to continuous aggregate the candlesticks on a materialized view you can use continuous aggregates for it. The next examples shows how to create a continuous aggregates of 1 minute candlesticks: options = { with_data : false , refresh_policies : { start_offset : \"INTERVAL '1 month'\" , end_offset : \"INTERVAL '1 minute'\" , schedule_interval : \"INTERVAL '1 minute'\" } } create_continuous_aggregate ( 'ohlc_1m' , Tick . ohlc ( timeframe : '1m' ), ** options ) Note that the create_continuous_aggregate calls the to_sql method in case the second parameter is not a string.","title":"Continuous aggregates"},{"location":"toolkit_ohlc/#rollup","text":"The rollup allows you to combine ohlc structures from smaller timeframes to bigger timeframes without needing to reprocess all the data. With this feature, you can group by the ohcl multiple times saving processing from the server and make it easier to manage candlesticks from different time intervals. In the previous example, we used the .ohlc function that returns already the attributes from the different timeframes. In the SQL command it's calling the open , high , low , close functions that can access the values behind the ohlcsummary type. To merge the ohlc we need to rollup the ohlcsummary to a bigger timeframe and only access the values as a final resort to see them and access as attributes. Let's rebuild the structure: execute \"CREATE VIEW ohlc_1h AS #{ Ohlc1m . rollup ( timeframe : '1 hour' ) . to_sql } \" execute \"CREATE VIEW ohlc_1d AS #{ Ohlc1h . rollup ( timeframe : '1 day' ) . to_sql } \"","title":"Rollup"},{"location":"toolkit_ohlc/#defining-models-for-views","text":"Note that the previous code refers to Ohlc1m and Ohlc1h as two classes that are not defined yet. They will basically be ActiveRecord readonly models to allow to build scopes from it. Ohlc for one hour: class Ohlc1m < ActiveRecord :: Base self . table_name = 'ohlc_1m' include Ohlc end Ohlc for one day is pretty much the same: class Ohlc1h < ActiveRecord :: Base self . table_name = 'ohlc_1h' include Ohlc end We'll also have the Ohlc as a shared concern that can help you to reuse queries in different views. module Ohlc extend ActiveSupport :: Concern included do scope :rollup , -> ( timeframe : '1h' ) do select ( \"symbol, time_bucket(' #{ timeframe } ', time) as time, toolkit_experimental.rollup(ohlc) as ohlc\" ) . group ( 1 , 2 ) end scope :attributes , -> do select ( \"symbol, time, toolkit_experimental.open(ohlc), toolkit_experimental.high(ohlc), toolkit_experimental.low(ohlc), toolkit_experimental.close(ohlc), toolkit_experimental.open_time(ohlc), toolkit_experimental.high_time(ohlc), toolkit_experimental.low_time(ohlc), toolkit_experimental.close_time(ohlc)\" ) end # Following the attributes scope, we can define accessors in the # model to populate from the previous scope to make it similar # to a regular model structure. attribute :time , :time attribute :symbol , :string %w[open high low close] . each do | name | attribute name , :decimal attribute \" #{ name } _time\" , :time end def readonly? true end end end The rollup scope is the one that was used to redefine the data into big timeframes and the attributes allow to access the attributes from the OpenHighLowClose type. In this way, the views become just shortcuts and complex sql can also be done just nesting the model scope. For example, to rollup from a minute to a month, you can do: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) ) Soon the continuous aggregates will support nested aggregates and you'll be abble to define the materialized views with steps like this: Ohlc1m . attributes . from ( Ohlc1m . rollup ( timeframe : '1 month' ) . from ( Ohlc1m . rollup ( timeframe : '1 week' ) . from ( Ohlc1m . rollup ( timeframe : '1 day' ) . from ( Ohlc1m . rollup ( timeframe : '1 hour' ) ) ) ) ) For now composing the subqueries will probably be less efficient and unnecessary. But the foundation is already here to help you in future analysis. Just to make it clear, here is the SQL generated from the previous code: SELECT symbol , time , toolkit_experimental . open ( ohlc ), toolkit_experimental . high ( ohlc ), toolkit_experimental . low ( ohlc ), toolkit_experimental . close ( ohlc ), toolkit_experimental . open_time ( ohlc ), toolkit_experimental . high_time ( ohlc ), toolkit_experimental . low_time ( ohlc ), toolkit_experimental . close_time ( ohlc ) FROM ( SELECT symbol , time_bucket ( '1 month' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 week' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 day' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM ( SELECT symbol , time_bucket ( '1 hour' , time ) as time , toolkit_experimental . rollup ( ohlc ) as ohlc FROM \"ohlc_1m\" GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery GROUP BY 1 , 2 ) subquery You can also define more scopes that will be useful depending on what are you working on. Example: scope :yesterday , -> { where ( \"DATE( #{ time_column } ) = ?\" , Date . yesterday . in_time_zone . to_date ) } And then, just combine the scopes: Ohlc1m . yesterday . attributes I hope you find this tutorial interesting and you can also check the ohlc.rb file in the examples/toolkit-demo folder. If you have any questions or concerns, feel free to reach me ( @jonatasdp ) in the Timescale community or tag timescaledb in your StackOverflow issue.","title":"Defining models for views"},{"location":"videos/","text":"Videos about the TimescaleDB Gem \u00b6 This library was started on twitch.tv/timescaledb . You can watch all episodes here: Wrapping Functions to Ruby Helpers . Extending ActiveRecord with Timescale Helpers . Setup Hypertables for Rails testing environment . Packing the code to this repository . the code to this repository . Working with Timescale continuous aggregates . Creating the command-line application in Ruby to explore the Timescale API . If you create any content related to how to use the Timescale Gem, please open a Pull Request .","title":"Videos"},{"location":"videos/#videos-about-the-timescaledb-gem","text":"This library was started on twitch.tv/timescaledb . You can watch all episodes here: Wrapping Functions to Ruby Helpers . Extending ActiveRecord with Timescale Helpers . Setup Hypertables for Rails testing environment . Packing the code to this repository . the code to this repository . Working with Timescale continuous aggregates . Creating the command-line application in Ruby to explore the Timescale API . If you create any content related to how to use the Timescale Gem, please open a Pull Request .","title":"Videos about the TimescaleDB Gem"}]}